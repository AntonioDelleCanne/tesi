{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/dsvm_server_admin/notebooks/fastai/tesi/DeepLearning_Financial\n"
     ]
    }
   ],
   "source": [
    "%cd DeepLearning_Financial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXTERNAL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import yfinance\n",
    "from pandas import Series\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, PredefinedSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random \n",
    "from sklearn.datasets import make_regression\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetRegressor\n",
    "from torch.nn.modules.loss import MSELoss\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from skorch.dataset import CVSplit\n",
    "from skorch import callbacks\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "import skorch\n",
    "import pywt\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "\n",
    "\n",
    "##INTERNAL\n",
    "from models import Autoencoder, waveletSmooth, SequenceDouble, SequenceDoubleAtt, SequenceAtt\n",
    "from utils import prepare_data_lstm, ExampleDataset, save_checkpoint, evaluate_lstm, backtest\n",
    "from testing import *\n",
    "from sa_tools import *\n",
    "from test_models import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione dei dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = ['^GSPC']\n",
    "feature_sets = ['open', 'ohlcv', 'ext']\n",
    "start_date=\"2000-01-01\"\n",
    "end_date=\"2018-12-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fill_dates(df):\n",
    "#     dates = pd.date_range(start=df.index.min(), end=df.index.max())\n",
    "#     for date in dates:\n",
    "#         if(date not in df.index):\n",
    "#             df.loc[date] = None\n",
    "#     df = df.sort_index()\n",
    "#     return df.fillna(method='ffill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = get_ext_feats(fill_dates(get_index()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'^GSPC': {'original':                Open     High      Low    Close        Volume\n",
      "Date                                                        \n",
      "2000-01-03  1469.25  1478.00  1438.36  1455.22  9.318000e+08\n",
      "2000-01-04  1455.22  1455.22  1397.43  1399.42  1.009000e+09\n",
      "2000-01-05  1399.42  1413.27  1377.68  1402.11  1.085500e+09\n",
      "2000-01-06  1402.11  1411.90  1392.10  1403.45  1.092300e+09\n",
      "2000-01-07  1403.45  1441.47  1400.73  1441.47  1.225200e+09\n",
      "...             ...      ...      ...      ...           ...\n",
      "2018-12-24  2400.56  2410.34  2351.10  2351.10  2.613930e+09\n",
      "2018-12-25  2400.56  2410.34  2351.10  2351.10  2.613930e+09\n",
      "2018-12-26  2363.12  2467.76  2346.58  2467.70  4.233990e+09\n",
      "2018-12-27  2442.50  2489.10  2397.94  2488.83  4.096610e+09\n",
      "2018-12-28  2498.77  2520.27  2472.89  2485.74  3.702620e+09\n",
      "\n",
      "[6935 rows x 5 columns], 'features': {}, 'target': Date\n",
      "2000-01-03    1455.22\n",
      "2000-01-04    1399.42\n",
      "2000-01-05    1402.11\n",
      "2000-01-06    1403.45\n",
      "2000-01-07    1441.47\n",
      "               ...   \n",
      "2018-12-24    2351.10\n",
      "2018-12-25    2351.10\n",
      "2018-12-26    2467.70\n",
      "2018-12-27    2488.83\n",
      "2018-12-28    2485.74\n",
      "Name: Close, Length: 6935, dtype: float64}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/pywt/_multilevel.py:45: UserWarning: Level value of 3 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n",
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/pywt/_thresholding.py:23: RuntimeWarning: invalid value encountered in true_divide\n",
      "  thresholded = (1 - value/magnitude)\n"
     ]
    }
   ],
   "source": [
    "# start_data = None\n",
    "# end_data = None\n",
    "# datasets = {}\n",
    "# for index in indices:\n",
    "#     datasets[index] = {}\n",
    "#     datasets[index][\"original\"] = get_index(index=index, start_date=start_date, end_date=end_date)\n",
    "#     datasets[index][\"original\"] = fill_dates(datasets[index][\"original\"].copy())\n",
    "#     datasets[index][\"features\"] = {}\n",
    "#     datasets[index][\"target\"] = datasets[index][\"original\"][\"Close\"]\n",
    "#     print(datasets)\n",
    "#     for feature_set in feature_sets:\n",
    "#         data = get_dataset_by_name(datasets[index][\"original\"], name=feature_set)\n",
    "#         data = data.dropna()\n",
    "#         if(start_data is None):\n",
    "#             start_data = data.index.min()\n",
    "#         else:\n",
    "#             start_data = max(data.index.min(), start_data)\n",
    "#         if(end_data is None):\n",
    "#             end_data = data.index.max()\n",
    "#         else:\n",
    "#             end_data = min(data.index.max(), end_data)\n",
    "#         datasets[index][\"features\"][feature_set] =  data.copy()\n",
    "\n",
    "# #allineamento delle date\n",
    "# for index in datasets.keys():      \n",
    "#     datasets[index][\"target\"] = datasets[index][\"target\"].loc[start_data:end_data].copy()\n",
    "#     for feature_set in datasets[index][\"features\"].keys():  \n",
    "#         datasets[index][\"features\"][feature_set] = datasets[index][\"features\"][feature_set].loc[start_data:end_data].copy()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'^GSPC': {'original':                Open     High      Low    Close        Volume\n",
      "Date                                                        \n",
      "2000-01-03  1469.25  1478.00  1438.36  1455.22  9.318000e+08\n",
      "2000-01-04  1455.22  1455.22  1397.43  1399.42  1.009000e+09\n",
      "2000-01-05  1399.42  1413.27  1377.68  1402.11  1.085500e+09\n",
      "2000-01-06  1402.11  1411.90  1392.10  1403.45  1.092300e+09\n",
      "2000-01-07  1403.45  1441.47  1400.73  1441.47  1.225200e+09\n",
      "...             ...      ...      ...      ...           ...\n",
      "2018-12-24  2400.56  2410.34  2351.10  2351.10  2.613930e+09\n",
      "2018-12-25  2400.56  2410.34  2351.10  2351.10  2.613930e+09\n",
      "2018-12-26  2363.12  2467.76  2346.58  2467.70  4.233990e+09\n",
      "2018-12-27  2442.50  2489.10  2397.94  2488.83  4.096610e+09\n",
      "2018-12-28  2498.77  2520.27  2472.89  2485.74  3.702620e+09\n",
      "\n",
      "[6935 rows x 5 columns], 'features': {}, 'target': Date\n",
      "2000-01-03    1455.22\n",
      "2000-01-04    1399.42\n",
      "2000-01-05    1402.11\n",
      "2000-01-06    1403.45\n",
      "2000-01-07    1441.47\n",
      "               ...   \n",
      "2018-12-24    2351.10\n",
      "2018-12-25    2351.10\n",
      "2018-12-26    2467.70\n",
      "2018-12-27    2488.83\n",
      "2018-12-28    2485.74\n",
      "Name: Close, Length: 6935, dtype: float64}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/pywt/_multilevel.py:45: UserWarning: Level value of 3 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n",
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/pywt/_thresholding.py:23: RuntimeWarning: invalid value encountered in true_divide\n",
      "  thresholded = (1 - value/magnitude)\n"
     ]
    }
   ],
   "source": [
    "datasets=get_datasets(indices = indices, feature_sets = feature_sets, end_date=end_date, start_date=start_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione di training e validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size=0.3\n",
    "\n",
    "serie = datasets['^GSPC'][\"target\"].to_numpy()\n",
    "\n",
    "splitted = np.split(serie, [np.ceil(len(serie)*valid_size).astype(int)])\n",
    "l1, l2 = len(splitted[1]), len(splitted[0])\n",
    "perc = PredefinedSplit(np.concatenate((np.ones(l1)*-1,np.ones(l2))))\n",
    "perc_split =  CVSplit(cv=perc, stratified=False, random_state=None)\n",
    "train_dates, val_dates = list(perc.split(serie))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_set=\"ext\"\n",
    "x, y, x_train, x_val, x_scaler, y_train, y_val, y_scaler = get_dataset_train(datasets=datasets, feature_set=feature_set, train_dates=train_dates, val_dates=val_dates, index='^GSPC', sa=None, load=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparaizione dei dati per il training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def roi TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_days=5\n",
    "\n",
    "x_batch = days_group(x, n_days=n_days)\n",
    "y_batch = y[n_days:]\n",
    "serie = x_batch\n",
    "\n",
    "splitted = np.split(serie, [np.ceil(len(serie)*valid_size).astype(int)])\n",
    "l1, l2 = len(splitted[1]), len(splitted[0])\n",
    "train_s = PredefinedSplit(np.concatenate((np.ones(l1)*-1,np.ones(l2))))\n",
    "train_split =  CVSplit(cv=train_s, stratified=False, random_state=None)\n",
    "train_dates, val_dates = list(perc.split(serie))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "lstm_att = NeuralNetRegressor(\n",
    "    module=SequenceAtt,\n",
    "    optimizer=optim.Adam,\n",
    "    batch_size=batch_size,\n",
    "    max_epochs=5000, # trovato empiricamente\n",
    "    train_split=train_split,\n",
    "    callbacks=[\n",
    "        callbacks.EpochScoring('neg_mean_absolute_error', lower_is_better=False),\n",
    "        callbacks.EpochScoring('r2', lower_is_better=False),\n",
    "#         callbacks.Checkpoint(monitor='valid_loss_best', f_pickle='lstm_sa_best')        \n",
    "    ],\n",
    "    \n",
    "    module__nb_features=x.shape[1],\n",
    "    module__hidden_size=100,\n",
    "    module__nb_layers= 5,\n",
    "    optimizer__lr=0.0001, #TODO prova con questa lr\n",
    "#     optimizer__weight_decay=0,\n",
    "#     optimizer__momentum=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    neg_mean_absolute_error       r2    train_loss    valid_loss     dur\n",
      "-------  -------------------------  -------  ------------  ------------  ------\n",
      "      1                    \u001b[36m-4.2200\u001b[0m  \u001b[32m-6.2281\u001b[0m        \u001b[35m0.7384\u001b[0m       \u001b[31m20.6650\u001b[0m  3.5365\n",
      "      2                    \u001b[36m-4.0583\u001b[0m  \u001b[32m-5.7593\u001b[0m        \u001b[35m0.4164\u001b[0m       \u001b[31m19.3247\u001b[0m  3.9825\n",
      "      3                    \u001b[36m-3.8086\u001b[0m  \u001b[32m-5.0727\u001b[0m        \u001b[35m0.2807\u001b[0m       \u001b[31m17.3617\u001b[0m  2.9646\n",
      "      4                    \u001b[36m-3.6439\u001b[0m  \u001b[32m-4.6436\u001b[0m        \u001b[35m0.2294\u001b[0m       \u001b[31m16.1348\u001b[0m  2.9033\n",
      "      5                    \u001b[36m-3.4472\u001b[0m  \u001b[32m-4.1555\u001b[0m        \u001b[35m0.1310\u001b[0m       \u001b[31m14.7394\u001b[0m  4.1377\n",
      "      6                    \u001b[36m-3.2503\u001b[0m  \u001b[32m-3.6924\u001b[0m        \u001b[35m0.0947\u001b[0m       \u001b[31m13.4155\u001b[0m  4.8206\n",
      "      7                    \u001b[36m-3.1132\u001b[0m  \u001b[32m-3.3849\u001b[0m        \u001b[35m0.0576\u001b[0m       \u001b[31m12.5363\u001b[0m  4.9229\n",
      "      8                    \u001b[36m-3.0571\u001b[0m  \u001b[32m-3.2628\u001b[0m        0.0676       \u001b[31m12.1871\u001b[0m  2.9343\n",
      "      9                    \u001b[36m-2.9952\u001b[0m  \u001b[32m-3.1298\u001b[0m        0.0581       \u001b[31m11.8071\u001b[0m  2.5553\n",
      "     10                    \u001b[36m-2.9532\u001b[0m  \u001b[32m-3.0415\u001b[0m        \u001b[35m0.0566\u001b[0m       \u001b[31m11.5546\u001b[0m  2.6020\n",
      "     11                    \u001b[36m-2.9198\u001b[0m  \u001b[32m-2.9721\u001b[0m        0.0573       \u001b[31m11.3561\u001b[0m  2.6990\n",
      "     12                    \u001b[36m-2.8819\u001b[0m  \u001b[32m-2.8926\u001b[0m        \u001b[35m0.0485\u001b[0m       \u001b[31m11.1287\u001b[0m  3.7522\n",
      "     13                    \u001b[36m-2.8394\u001b[0m  \u001b[32m-2.8061\u001b[0m        0.0501       \u001b[31m10.8815\u001b[0m  3.7777\n",
      "     14                    \u001b[36m-2.8220\u001b[0m  \u001b[32m-2.7704\u001b[0m        \u001b[35m0.0474\u001b[0m       \u001b[31m10.7794\u001b[0m  4.7931\n",
      "     15                    \u001b[36m-2.7874\u001b[0m  \u001b[32m-2.7022\u001b[0m        \u001b[35m0.0454\u001b[0m       \u001b[31m10.5845\u001b[0m  3.0281\n",
      "     16                    \u001b[36m-2.7726\u001b[0m  \u001b[32m-2.6726\u001b[0m        \u001b[35m0.0437\u001b[0m       \u001b[31m10.4998\u001b[0m  3.2324\n",
      "     17                    \u001b[36m-2.7528\u001b[0m  \u001b[32m-2.6330\u001b[0m        \u001b[35m0.0413\u001b[0m       \u001b[31m10.3866\u001b[0m  3.5120\n",
      "     18                    \u001b[36m-2.7344\u001b[0m  \u001b[32m-2.5954\u001b[0m        \u001b[35m0.0403\u001b[0m       \u001b[31m10.2790\u001b[0m  3.1380\n",
      "     19                    \u001b[36m-2.7177\u001b[0m  \u001b[32m-2.5632\u001b[0m        \u001b[35m0.0378\u001b[0m       \u001b[31m10.1872\u001b[0m  3.2958\n",
      "     20                    \u001b[36m-2.7096\u001b[0m  \u001b[32m-2.5466\u001b[0m        0.0381       \u001b[31m10.1396\u001b[0m  4.2553\n",
      "     21                    \u001b[36m-2.6991\u001b[0m  \u001b[32m-2.5261\u001b[0m        \u001b[35m0.0374\u001b[0m       \u001b[31m10.0811\u001b[0m  3.1626\n",
      "     22                    \u001b[36m-2.6899\u001b[0m  \u001b[32m-2.5080\u001b[0m        \u001b[35m0.0372\u001b[0m       \u001b[31m10.0292\u001b[0m  4.3831\n",
      "     23                    \u001b[36m-2.6798\u001b[0m  \u001b[32m-2.4881\u001b[0m        \u001b[35m0.0361\u001b[0m        \u001b[31m9.9724\u001b[0m  3.6640\n",
      "     24                    \u001b[36m-2.6708\u001b[0m  \u001b[32m-2.4708\u001b[0m        \u001b[35m0.0353\u001b[0m        \u001b[31m9.9229\u001b[0m  3.1762\n",
      "     25                    \u001b[36m-2.6647\u001b[0m  \u001b[32m-2.4590\u001b[0m        \u001b[35m0.0350\u001b[0m        \u001b[31m9.8892\u001b[0m  3.6311\n",
      "     26                    \u001b[36m-2.6602\u001b[0m  \u001b[32m-2.4506\u001b[0m        \u001b[35m0.0348\u001b[0m        \u001b[31m9.8651\u001b[0m  3.1506\n",
      "     27                    \u001b[36m-2.6572\u001b[0m  \u001b[32m-2.4451\u001b[0m        \u001b[35m0.0348\u001b[0m        \u001b[31m9.8495\u001b[0m  3.9402\n",
      "     28                    \u001b[36m-2.6562\u001b[0m  \u001b[32m-2.4434\u001b[0m        \u001b[35m0.0347\u001b[0m        \u001b[31m9.8445\u001b[0m  2.9589\n",
      "     29                    -2.6563  -2.4434        \u001b[35m0.0344\u001b[0m        9.8447  3.0029\n",
      "     30                    \u001b[36m-2.6531\u001b[0m  \u001b[32m-2.4379\u001b[0m        \u001b[35m0.0339\u001b[0m        \u001b[31m9.8288\u001b[0m  3.1815\n",
      "     31                    -2.6565  -2.4444        0.0342        9.8474  2.7737\n",
      "     32                    -2.6558  -2.4432        \u001b[35m0.0335\u001b[0m        9.8439  2.5173\n",
      "     33                    -2.6562  -2.4439        \u001b[35m0.0332\u001b[0m        9.8461  2.9277\n",
      "     34                    -2.6570  -2.4457        \u001b[35m0.0329\u001b[0m        9.8513  2.6345\n",
      "     35                    -2.6543  -2.4405        \u001b[35m0.0327\u001b[0m        9.8364  2.5022\n",
      "     36                    -2.6553  -2.4430        \u001b[35m0.0323\u001b[0m        9.8433  4.9252\n",
      "     37                    -2.6600  -2.4520        0.0325        9.8692  3.2474\n",
      "     38                    -2.6648  -2.4619        \u001b[35m0.0321\u001b[0m        9.8975  4.9058\n",
      "     39                    -2.6638  -2.4599        \u001b[35m0.0318\u001b[0m        9.8917  2.9612\n",
      "     40                    -2.6564  -2.4460        \u001b[35m0.0309\u001b[0m        9.8521  3.7136\n",
      "     41                    -2.6639  -2.4608        0.0317        9.8944  2.7267\n",
      "     42                    -2.6584  -2.4498        \u001b[35m0.0303\u001b[0m        9.8628  3.1836\n",
      "     43                    -2.6693  -2.4720        0.0315        9.9263  2.9500\n",
      "     44                    -2.6603  -2.4532        \u001b[35m0.0294\u001b[0m        9.8726  2.5217\n",
      "     45                    -2.6665  -2.4672        0.0322        9.9125  2.5291\n",
      "     46                    -2.6685  -2.4696        \u001b[35m0.0293\u001b[0m        9.9196  3.0339\n",
      "     47                    -2.6601  -2.4553        0.0298        9.8787  2.9439\n",
      "     48                    -2.6790  -2.4907        0.0303        9.9799  3.5062\n",
      "     49                    -2.6686  -2.4707        \u001b[35m0.0282\u001b[0m        9.9227  2.9375\n",
      "     50                    -2.6628  -2.4602        0.0290        9.8925  2.6368\n",
      "     51                    -2.6785  -2.4901        0.0298        9.9781  2.3797\n",
      "     52                    -2.6562  -2.4475        0.0286        9.8563  2.7090\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
       "  module_=SequenceAtt(\n",
       "    (lstm1): LSTM(5, 100, num_layers=5, batch_first=True)\n",
       "    (lin): Linear(in_features=100, out_features=1, bias=True)\n",
       "    (lin_out): Linear(in_features=100, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (tanh): Tanh()\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= lstm_att\n",
    "model.fit(x_batch,y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "save(model, 'lstm_att')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
