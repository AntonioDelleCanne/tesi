{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4SoRjGQmhDkg"
   },
   "source": [
    "# Import delle librerie necessarie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c6wa9QZChDkh"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "iAvkL66NMopR",
    "outputId": "4a7b7edd-d1dd-47e5-8bd8-956c07eade63"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/AntonioDelleCanne/tesi.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "cFpIIEUVM9p9",
    "outputId": "c94eb57a-b2ac-467d-e84f-cbb68bf1af4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/dsvm_server_admin/notebooks/fastai/tesi/DeepLearning_Financial\n"
     ]
    }
   ],
   "source": [
    "%cd DeepLearning_Financial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "colab_type": "code",
    "id": "7OGJqse3hDkk",
    "outputId": "9e37161b-5ec6-4deb-be4d-e33d86c09d79"
   },
   "outputs": [],
   "source": [
    "## EXTERNAL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import yfinance\n",
    "from pandas import Series\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, PredefinedSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random \n",
    "from sklearn.datasets import make_regression\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetRegressor\n",
    "from torch.nn.modules.loss import MSELoss\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from skorch.dataset import CVSplit\n",
    "from skorch import callbacks\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "import skorch\n",
    "import pywt\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "\n",
    "\n",
    "##INTERNAL\n",
    "from models import Autoencoder, waveletSmooth, SequenceDouble, SequenceDoubleAtt, SequenceAtt\n",
    "from utils import prepare_data_lstm, ExampleDataset, save_checkpoint, evaluate_lstm, backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "        \n",
    "def load(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns open high low close volume\n",
    "def get_index(index=\"^DJI\", start_date=\"2000-01-01\", end_date=\"2018-12-31\"):\n",
    "    security = yfinance.Ticker(index)# TODO trova mercato asiatico e indiano\n",
    "    security_data = security.history(start=start_date, end=end_date, actions=False)\n",
    "    return security_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_index(security_data):\n",
    "    return security_data[\"Open\"], security_data[\"High\"], security_data[\"Low\"], security_data[\"Close\"], security_data[\"Volume\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N3mVLVkyhDlI"
   },
   "source": [
    "# Allenamento modello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gw2hmMQHhDlI"
   },
   "source": [
    "## Metriche\n",
    "Come metrica principale per valutare la bonta' dele predizioni utilizzeremo il Return of Investment (ROI).\n",
    "\n",
    "Con questa metrica assumiamo di utilizzare l'algoritmo di trading precedentemente descritto, e calcoliamo il guadango che avremmo ottenuto se lo avessimo utilizzato sul dataset che stiamo valutando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3gpU8l6hDlI"
   },
   "outputs": [],
   "source": [
    "def gain(C, C_pred, opn):\n",
    "    O = opn.reindex_like(C)\n",
    "    CO_diff = C - O\n",
    "    growth = C_pred > O\n",
    "    decline = C_pred < O\n",
    "    return CO_diff[growth].sum() - CO_diff[decline].sum()\n",
    "def roi(C, C_pred, opn):\n",
    "    mean_opn = opn.reindex_like(C).mean()\n",
    "    return gain(C, C_pred, opn) / mean_opn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BpEzlQ-ghDlK"
   },
   "source": [
    "## Preparazione dei dati\n",
    "Come spiegato in precedenza, visto che alcune feature non sono disponibili all'inizio della giornata, per poterle utilizzare nel nostro modello, utilizzeremo i dati dei giori passati, servendoci della funzione shift.\n",
    "\n",
    "Visto che l'utilizzo di questa funzione fara' si che in alcune riche vi saranno dei vaolri NaN, dobbiamo assicurarci di eliminare queste osservazioni sia nelle serie relative ale features che in quella della variabile da predire.\n",
    "Questo compito e' svolto dalla funzione prepare_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K7Q_75EthDlL"
   },
   "outputs": [],
   "source": [
    "def prepare_data(features, target):\n",
    "    X = pd.DataFrame(features)\n",
    "    X.dropna(inplace=True)\n",
    "    Y = target.reindex_like(X)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJjrwfynhDlN"
   },
   "source": [
    "Con questa funzione dividiamo i dati in training set e validation set come è stato fatto nel paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_split_before_2010_06_30(X):\n",
    "    is_train = X.index.date < datetime.date(2010,6,30)\n",
    "    X_train = X.loc[is_train]\n",
    "    X_val = X.loc[~is_train]\n",
    "    return X_train, X_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OReH-EXzhDlO"
   },
   "outputs": [],
   "source": [
    "def split_before_2010_06_30(X, y):\n",
    "    is_train = X.index.date < datetime.date(2010,6,30)\n",
    "    X_train = X.loc[is_train]\n",
    "    y_train = y.loc[is_train]\n",
    "    X_val = X.loc[~is_train]\n",
    "    y_val = y.loc[~is_train]\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def days_group(data, n_days=10):\n",
    "    res = np.zeros([data.shape[0]-n_days, n_days, data.shape[1]], dtype=np.float32)\n",
    "    for i, el in enumerate(data):\n",
    "        if(i >= n_days):\n",
    "            res[i-n_days] = data[i-n_days:i]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oW0tPxrAhDlP"
   },
   "source": [
    "# Preparazione del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ext_feats(ohlcv):\n",
    "    res = ohlcv.copy()\n",
    "    \n",
    "    opn = res[\"Open\"]\n",
    "    close = res[\"Close\"]\n",
    "    high = res[\"High\"]\n",
    "    low = res[\"Low\"]\n",
    "    volume = res[\"Volume\"]\n",
    "    \n",
    "    #calucate derived indicators\n",
    "    TP = ((high + low + close) / 3 ).shift(1)\n",
    "    trs = pd.DataFrame(index=high.index)\n",
    "    trs['tr0'] = abs(high - close)\n",
    "    trs['tr1'] = abs(high - close.shift(1))\n",
    "    trs['tr2'] = abs(low - close.shift(1))\n",
    "    TR = trs[['tr0', 'tr1', 'tr2']].max(axis=1).shift(1)\n",
    "    ema20 = opn.ewm(span=20).mean()\n",
    "    ma10 = opn.rolling(window=10).mean()\n",
    "    ma5 = opn.rolling(window=5).mean()\n",
    "    macd = opn.ewm(span=26).mean() - opn.ewm(span=12).mean()\n",
    "    cci_ndays=20\n",
    "    cci = (TP - TP.rolling(cci_ndays).mean()) / (0.015 * TP.rolling(cci_ndays).std())\n",
    "    atr = TR.ewm(span = 10).mean()\n",
    "    ma20 = opn.rolling(window=20).mean()\n",
    "    std20 = opn.rolling(window=20).std()\n",
    "    k=2\n",
    "    boll_up =  ma20 + (k*std20)\n",
    "    boll_down = ma20 - (k*std20)\n",
    "    roc = (opn - opn.shift(9))/opn.shift(9)\n",
    "    mtm6 = (opn - opn.shift(127))\n",
    "    mtm12 = (opn - opn.shift(253)) #length of a trading year is on average 253 days\n",
    "    wvad = (((close - low) - (high - close)) * volume/(high - low)).shift(1)\n",
    "    smi = (close - (high - low)/2).shift(1)\n",
    "    \n",
    "    res[\"CloseL1\"] = close.shift(1)\n",
    "    res[\"HighL1\"] = high.shift(1)\n",
    "    res[\"LowL1\"] = low.shift(1)\n",
    "    res[\"VolumeL1\"] = volume.shift(1)\n",
    "    res[\"EMA20\"] = ema20\n",
    "    res[\"MA5\"] = ma5\n",
    "    res[\"MA10\"] = ma10\n",
    "    res[\"MA20\"] = ma20\n",
    "    res[\"MACD\"] = macd\n",
    "    res[\"CCI\"] = cci\n",
    "    res[\"ATR\"] = atr\n",
    "    res[\"BollUp\"] = boll_up\n",
    "    res[\"BollDown\"] = boll_down\n",
    "    res[\"WVAD\"] = wvad\n",
    "    res[\"MTM6\"] = mtm6\n",
    "    res[\"MTM12\"] = mtm12\n",
    "    res[\"SMI\"] = smi\n",
    "    res[\"ROC\"] = roc\n",
    "    \n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes as input ohlcv dataframe\n",
    "def get_data_set(ohlcv, ext_feats=True, usd_index='DX-Y.NYB', wavelet=True):\n",
    "    feats = ohlcv.copy()\n",
    "    usd_open = get_index(usd_index, start_date=ohlcv[\"Open\"].index.min(), end_date=ohlcv[\"Open\"].index.max())[\"Open\"]\n",
    "    \n",
    "    if(wavelet):\n",
    "    #apply transforms\n",
    "        for f_name in ('Open', 'Close', 'High', 'Low'):\n",
    "            feats[f_name] = apply_wavelet_transform(feats[f_name]) \n",
    "        usd_open = apply_wavelet_transform(usd_open)\n",
    "    \n",
    "    if(ext_feats):\n",
    "        feats = get_ext_feats(feats)\n",
    "        feats[\"USDOpen\"]  = usd_open\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_by_name(ohlcv, name):\n",
    "    if(name is \"open\"):\n",
    "        return get_data_set(ohlcv, ext_feats=False)[[\"Open\", \"Close\"]]\n",
    "    elif(name is \"ohlcv\"):\n",
    "        return get_data_set(ohlcv, ext_feats=False)\n",
    "    elif(name is \"ext\"):\n",
    "        return get_data_set(ohlcv)\n",
    "    raise Exception('Nome del feature-set non valido')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regolarizzazione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_wavelet_transform(data, consider_future=False, wavelet='haar'):\n",
    "    res = data.copy()\n",
    "    if(consider_future):\n",
    "        res, _ = pywt.dwt(data.copy(), wavelet=wavelet)\n",
    "    else:\n",
    "        for i in range(res.shape[0]):\n",
    "            if(i > 0):\n",
    "                cA =  waveletSmooth(data.iloc[:i+1].copy(), wavelet=wavelet, level=4, DecLvl=3)\n",
    "                res.iloc[i] = cA[-1]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_wavelet_transform(data, consider_future=False, wavelet='haar'):\n",
    "#     res = data.copy()\n",
    "#     if(consider_future):\n",
    "#         res, _ = pywt.dwt(data.copy(), wavelet=wavelet)\n",
    "#     else:\n",
    "#         for i in range(res.shape[0]):\n",
    "#             if(i > 0):\n",
    "#                 cA, cD = pywt.dwt(data.iloc[:i+1].copy(), wavelet=wavelet)\n",
    "#                 res.iloc[i] = cA[-1]\n",
    "#     return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# opn =get_index()[\"Open\"]\n",
    "# haar = apply_wavelet_transform(opn, consider_future=False, wavelet='haar')\n",
    "# coif3 = apply_wavelet_transform(opn, consider_future=False, wavelet='coif3')\n",
    "# fig, ax = plt.subplots(3,1, sharex=False, figsize=(20,15))\n",
    "# ax[0].plot(opn, label='open')\n",
    "# ax[1].plot(haar, label='haar wavelet')\n",
    "# ax[2].plot(coif3, label ='coif3 wavelet')\n",
    "# ax[0].legend()\n",
    "# ax[1].legend()\n",
    "# ax[2].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizzazione \n",
    "Le seguenti funzioni sono utilizzate per normalizzare i dati, pianifichiamo di utilizzare queste funzioni anche sulla variabile da predire dovremo implementarne anche l'inversa, di modo da poter denormalizzare le predizioni fatte dal modello.\n",
    "Questo passo sarà importante nella valutazione del modello, ad esempio per calcolare il ROI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder(X, val=None, sa_hidden_size=10):\n",
    "    X_train_f = X.astype(np.float32)\n",
    "    if(val is not None):\n",
    "        X_val_f = val.astype(np.float32)\n",
    "    #Initialize the autoencoder\n",
    "    sa_hidden_size= np.ceil(X.shape[1] / 2).astype(int) # Con tutte le features 10\n",
    "\n",
    "    num_hidden_1 = sa_hidden_size\n",
    "    num_hidden_2 = sa_hidden_size\n",
    "    num_hidden_3 = sa_hidden_size\n",
    "    num_hidden_4 = sa_hidden_size\n",
    "\n",
    "    n_epoch1=15000 #10000\n",
    "    n_epoch2 = 2000\n",
    "    n_epoch3 = 600\n",
    "    n_epoch4 = 500\n",
    "    batch_size=20\n",
    "\n",
    "    # ---- train using training data\n",
    "\n",
    "    # The n==0 statement is done because we only want to initialize the network once and then keep training\n",
    "    # as we move through time \n",
    "\n",
    "    auto1 = Autoencoder(X_train_f.shape[1], num_hidden_1)\n",
    "    auto2 = Autoencoder(num_hidden_1, num_hidden_2)\n",
    "    auto3 = Autoencoder(num_hidden_2, num_hidden_3)\n",
    "    auto4 = Autoencoder(num_hidden_3, num_hidden_4)\n",
    "    \n",
    "    # Train the autoencoder \n",
    "    # switch to training mode\n",
    "    auto1.train()      \n",
    "    auto2.train()\n",
    "    auto3.train()\n",
    "    auto4.train()\n",
    "\n",
    "    inputs = torch.from_numpy(X_train_f)\n",
    "    val_in = torch.from_numpy(X_val_f)\n",
    "    auto1.fit(X_train_f, X_val_f, n_epoch=n_epoch1, batch_size=batch_size)\n",
    "\n",
    "    auto1_out = auto1.encoder(inputs).data.numpy()\n",
    "    val1_out = auto1.encoder(val_in).data.numpy()\n",
    "    auto2.fit(auto1_out, val1_out, n_epoch=n_epoch2, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    auto1_out = torch.from_numpy(auto1_out.astype(np.float32))\n",
    "    auto2_out = auto2.encoder(auto1_out).data.numpy()\n",
    "    val1_out = torch.from_numpy(val1_out.astype(np.float32))\n",
    "    val2_out = auto2.encoder(val1_out).data.numpy()\n",
    "    auto3.fit(auto2_out, val2_out, n_epoch=n_epoch3, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    auto2_out = torch.from_numpy(auto2_out.astype(np.float32))\n",
    "    auto3_out = auto3.encoder(auto2_out).data.numpy()\n",
    "    val2_out = torch.from_numpy(val2_out.astype(np.float32))\n",
    "    val3_out = auto3.encoder(val2_out).data.numpy()\n",
    "    auto4.fit(auto3_out, val3_out, n_epoch=n_epoch4, batch_size=batch_size)\n",
    "\n",
    "    # Change to evaluation mode, in this mode the network behaves differently, e.g. dropout is switched off and so on\n",
    "    auto1.eval()        \n",
    "    auto2.eval()\n",
    "    auto3.eval()\n",
    "    auto4.eval()\n",
    "    return [auto1, auto2, auto3, auto4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(feat_matrix, encoder):\n",
    "    encoded = torch.from_numpy(feat_matrix)\n",
    "    for auto in encoder:\n",
    "        encoded = auto.encoder(encoded)\n",
    "    return encoded.data.numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ext_train = ext_scaler.transform(s_split_before_2010_06_30(ext_dataset)[0])\n",
    "# ext_eval = ext_scaler.transform(s_split_before_2010_06_30(ext_dataset)[1])\n",
    "# ohlcv_train = ohlcv_scaler.transform(s_split_before_2010_06_30(ohlcv_dataset)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ext_encoder = get_encoder(ext_train, ext_eval, sa_hidden_size=10)\n",
    "# save(ext_encoder, 'ext_encoder_better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #test the sa\n",
    "# with torch.no_grad():\n",
    "#     res = torch.from_numpy(ext_eval.astype(np.float32))\n",
    "#     for encoder in ext_encoder:\n",
    "#         res, loss = ext_encoder[0].train()(res)\n",
    "#         print(abs(res.numpy() - ext_eval.astype(np.float32)).mean())\n",
    "#         print(loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creazione dei dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #parte utile per visualizzare i diversi indici da selezionare\n",
    "# start_date = \"2000-01-01\"\n",
    "# end_date = \"2018-12-31\"\n",
    "# security = yfinance.Ticker('ASHR')# TODO trova mercato asiatico e indiano\n",
    "# security_data = security.history(start=start_date, end=end_date, actions=False)\n",
    "# # security_data = pd.DataFrame(security_data.values, index=security_data.index[::-1], columns=security_data.columns) # inv option\n",
    "# opn.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consideriamo i seguenti indici:\n",
    "-mercati sviluppati: S&P500(^GSPC), Dow Jones Industrial Average(^DJI)\n",
    "-mercati nel mezzo: Hang Seng index in Hong Kong(^HSI), Nikkei 225 index in Tokyo(^N225)\n",
    "-mercati in via di sviluppo: CSI 300 in mainland China (ASHR), Nifty 50 in India(^NSEI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = ['^GSPC', '^DJI', '^HSI', '^N225', 'ASHR','^NSEI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = ['^GSPC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_sets = ['open', 'ohlcv', 'ext','sa_ohlcv', 'sa_ext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = ['open', 'ohlcv', 'ext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date=\"2000-01-01\"\n",
    "end_date=\"2018-12-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/pywt/_multilevel.py:45: UserWarning: Level value of 3 is too high: all coefficients will experience boundary effects.\n",
      "  \"boundary effects.\").format(level))\n",
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/pywt/_thresholding.py:23: RuntimeWarning: invalid value encountered in true_divide\n",
      "  thresholded = (1 - value/magnitude)\n"
     ]
    }
   ],
   "source": [
    "# i dataset sono organizzati nel seguente modo\n",
    "# -indice\n",
    "#  -dati originali : dataframe pandas\n",
    "#  -features: dizionario di featureset\n",
    "start_date=\"2000-01-01\"\n",
    "end_date=\"2018-12-31\"\n",
    "start_data = None\n",
    "end_data = None\n",
    "datasets = {}\n",
    "for index in indices:\n",
    "    datasets[index] = {}\n",
    "    datasets[index][\"original\"] = get_index(index=index, start_date=start_date, end_date=end_date)\n",
    "    datasets[index][\"features\"] = {}\n",
    "    datasets[index][\"target\"] = None\n",
    "    for feature_set in feature_sets:\n",
    "        data = get_dataset_by_name(datasets[index][\"original\"], name=feature_set)\n",
    "        data.dropna(inplace=True)\n",
    "        if(start_data is None):\n",
    "            start_data = data.index.min()\n",
    "        else:\n",
    "            start_data = max(data.index.min(), start_data)\n",
    "        if(end_data is None):\n",
    "            end_data = data.index.max()\n",
    "        else:\n",
    "            end_data = min(data.index.max(), end_data)\n",
    "        datasets[index][\"features\"][feature_set] = data.drop(\"Close\", axis=1)\n",
    "        if(datasets[index][\"target\"] is None):\n",
    "            datasets[index][\"target\"] = data[\"Close\"]\n",
    "            \n",
    "for index in datasets.keys():      \n",
    "    datasets[index][\"target\"] = datasets[index][\"target\"].loc[start_data:end_data].copy()\n",
    "    for feature_set in datasets[index][\"features\"].keys():  \n",
    "        datasets[index][\"features\"][feature_set] = datasets[index][\"features\"][feature_set].loc[start_data:end_data].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training dei modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #dataset è della forma (X,Y)\n",
    "# def set_dataset(data_set):\n",
    "#     global X\n",
    "#     global Y\n",
    "#     global X_train\n",
    "#     global Y_train\n",
    "#     global X_val\n",
    "#     global Y_val\n",
    "#     global X_train_f\n",
    "#     global Y_train_f\n",
    "#     global X_val_f\n",
    "#     global Y_val_f\n",
    "#     global X_f\n",
    "#     global Y_f\n",
    "#     global half\n",
    "#     global half_split\n",
    "#     X= data_set[0]\n",
    "#     Y= data_set[1]\n",
    "#     X_train, X_val, Y_train, Y_val = split_before_2010_06_30(X, Y)\n",
    "#     X_f = X.to_numpy().astype(np.float32)\n",
    "#     Y_f = Y.to_numpy().astype(np.float32)[...,None]\n",
    "#     X_train_f = X_train.to_numpy().astype(np.float32)\n",
    "#     Y_train_f = Y_train.to_numpy().astype(np.float32)[...,None]\n",
    "#     X_val_f = X_val.to_numpy().astype(np.float32)\n",
    "#     Y_val_f = Y_val.to_numpy().astype(np.float32)[...,None] \n",
    "#     l1 = len(np.split(X_f, [len(X_f)//2])[0])\n",
    "#     l2 = len(np.split(X_f, [len(X_f)//2])[1])\n",
    "#     half = PredefinedSplit(np.concatenate((np.ones(l1)*-1,np.ones(l2))))\n",
    "#     half_split =  CVSplit(cv=half, stratified=False, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss = TimeSeriesSplit(3)#max_train_size\n",
    "tss_split = CVSplit(cv=tss, stratified=False, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modello del paper di Moro\n",
    "#TODO prova\n",
    "n_days = 5\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "\n",
    "lstm_moro = NeuralNetRegressor(\n",
    "    module=SequenceDouble,\n",
    "    optimizer=optim.SGD,\n",
    "    batch_size=batch_size,\n",
    "    max_epochs=200, # usato nel paper\n",
    "    train_split=None,\n",
    "    \n",
    "    module__nb_features=X_f.shape[1],\n",
    "    module__hidden_size=256,\n",
    "    optimizer__lr=0.01,\n",
    "    optimizer__weight_decay=0,\n",
    "    optimizer__momentum=0.9\n",
    ")\n",
    "models['lstm_moro'] = (lstm_moro, 'open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modello del paper che usa attention mechanism\n",
    "n_days = 5\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "lstm_att_ohlcv = NeuralNetRegressor(\n",
    "    module=SequenceDoubleAtt,\n",
    "    optimizer=optim.Adam,\n",
    "    batch_size=batch_size,\n",
    "    max_epochs=250, # trovato empiricamente\n",
    "    train_split=None,\n",
    "    callbacks=[\n",
    "        callbacks.EpochScoring('neg_mean_absolute_error', lower_is_better=False),\n",
    "        callbacks.EpochScoring('r2', lower_is_better=False),\n",
    "        callbacks.Checkpoint(monitor='valid_loss_best', f_pickle='lstm_sa_best')        \n",
    "    ],\n",
    "    \n",
    "    module__nb_features=ohlcv_train.shape[1],\n",
    "    module__hidden_size=256,\n",
    "    optimizer__lr=0.0001,\n",
    ")\n",
    "\n",
    "models['lstm_att_ohlcv'] = (lstm_att_ohlcv, 'ohlcv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modello del paper che usa stacked autoencoders usando LSTM paper Moro\n",
    "n_days = 5\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "lstm_sa = NeuralNetRegressor(\n",
    "    module=SequenceDouble,\n",
    "    optimizer=optim.Adam,\n",
    "    batch_size=batch_size,\n",
    "    max_epochs=350, # trovato empiricamente\n",
    "    train_split=tss_split,\n",
    "    callbacks=[\n",
    "        callbacks.EpochScoring('neg_mean_absolute_error', lower_is_better=False),\n",
    "        callbacks.Checkpoint(monitor='valid_loss_best', f_pickle='lstm_sa_best'),\n",
    "        \n",
    "    ],\n",
    "    \n",
    "    module__nb_features=ext_sa_train.shape[1],\n",
    "    module__hidden_size=256,\n",
    "    optimizer__lr=0.0001,\n",
    "#     optimizer__weight_decay=0,\n",
    "#     optimizer__momentum=0.9,\n",
    "    iterator_train__shuffle = True,\n",
    ")\n",
    "\n",
    "models['lstm_sa'] = (lstm_sa, 'ext_sa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modello del paper che usa attention mechanism\n",
    "n_days = 5\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "lstm_sa_att = NeuralNetRegressor(\n",
    "    module=SequenceDoubleAtt,\n",
    "    optimizer=optim.Adam,\n",
    "    batch_size=batch_size,\n",
    "    max_epochs=5000, # trovato 280\n",
    "    train_split=half_split,\n",
    "    callbacks=[\n",
    "        callbacks.EpochScoring('neg_mean_absolute_error', lower_is_better=False),\n",
    "        callbacks.EpochScoring('r2', lower_is_better=False),\n",
    "        callbacks.Checkpoint(monitor='valid_loss_best', f_pickle='lstm_sa_best')        \n",
    "    ],\n",
    "    \n",
    "    module__nb_features=ext_sa_train.shape[1],\n",
    "    module__hidden_size=256,\n",
    "#     module__nb_layers= 5,\n",
    "    optimizer__lr=0.001,\n",
    "#     optimizer__weight_decay=0,\n",
    "#     optimizer__momentum=0.9\n",
    ")\n",
    "\n",
    "models['lstm_sa_att'] = (lstm_sa_att, 'ext_sa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = {'lstm_moro':(lstm_moro, 'open'), 'lstm_sa':(lstm_sa_d_1000,'sa_ext'), 'lstm_att_ohlcv':(lstm_att_d_600,'ohlcv'), 'lstm_att_ext':(lstm_att_d_600,'ext'), 'lstm_sa_att,':(lstm_sa_att_1500, 'sa_ext')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO assicurarsi che tutti i feature set abbiano la stessa luinghezza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_score(C, C_pred, opn):\n",
    "    O = opn\n",
    "    CO_diff = C - O\n",
    "    growth = C_pred > O\n",
    "    decline = C_pred < O\n",
    "    return CO_diff[growth].sum() - CO_diff[decline].sum()\n",
    "def roi_score(C, C_pred, opn):\n",
    "    mean_opn = opn.mean()\n",
    "    return gain(C, C_pred, opn) / mean_opn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test dei modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame()#TODO define structure salva risultati ad ogni iterazione in csv\n",
    "for index_data in indexes_data:\n",
    "    for i, (train, val) in enumerate(tss.split(index_data[\"target\"]), start=1):\n",
    "        \n",
    "        feature_sets = {}\n",
    "        \n",
    "#         train_dates = index_data[\"original\"].index[train]\n",
    "#         val_dates = index_data[\"original\"].index[val]\n",
    "        \n",
    "        y = index_data[\"target\"]\n",
    "        opn_dataset = index_data[\"features\"]['open']\n",
    "        ohlcv_dataset = index_data[\"features\"]['ohlcv']\n",
    "        ext_dataset = index_data[\"features\"]['ext']\n",
    "\n",
    "        #data split\n",
    "        \n",
    "        opn_train, opn_val = opn_dataset.iloc[train_dates].to_numpy(np.float32), opn_dataset.iloc[val_dates].to_numpy(np.float32)\n",
    "        ohlcv_train, ohlcv_val = ohlcv_dataset.iloc[train_dates].to_numpy(np.float32), ohlcv_dataset.iloc[val_dates].to_numpy(np.float32)\n",
    "        ext_train, ext_val = ext_dataset.iloc[train_dates].to_numpy(np.float32), ext_dataset.iloc[val_dates].to_numpy(np.float32)\n",
    "        y_train, y_val = y.iloc[train_dates].to_numpy(np.float32)[...,None], y.iloc[val_dates].to_numpy(np.float32)[...,None]\n",
    "        \n",
    "        #data scale\n",
    "        opn_scaler = StandardScaler()\n",
    "        ohlcv_scaler = StandardScaler()\n",
    "        ext_scaler = StandardScaler()\n",
    "        y_scaler = StandardScaler()\n",
    "\n",
    "        opn_train = ohlcv_scaler.fit_transform(opn_train)\n",
    "        ohlcv_train = ohlcv_scaler.fit_transform(ohlcv_train)\n",
    "        ext_train = ext_scaler.fit_transform(ext_train)\n",
    "        y_train = y_scaler.fit_transform(y_train)\n",
    "        opn_val = ohlcv_scaler.transform(opn_val)\n",
    "        ohlcv_val = ohlcv_scaler.ransform(ohlcv_val)\n",
    "        ext_val = ext_scaler.transform(ext_val)\n",
    "        y_val = y_scaler.transform(y_val)\n",
    "        \n",
    "        ext_encoder = get_encoder(ext_train, ext_val, sa_hidden_size=10)\n",
    "        ext_sa_train, ext_sa_val = encode(ext_train, ext_encoder), encode(ext_val, ext_encoder)\n",
    "        \n",
    "        feature_sets['open'] = (opn_train, opn_val, opn_scaler)\n",
    "        feature_sets['ohlcv'] = (ohlcv_train, ohlcv_val, ohlcv_scaler)\n",
    "        feature_sets['ext'] = (ext_train, ext_val, ext_scaler)\n",
    "        feature_sets['ext_sa'] = (ext_sa_train, ext_sa_val, ext_scaler)\n",
    "        \n",
    "        n_days = 5\n",
    "        opn_test = index_data[\"original\"][\"Open\"].reindex_like(opn_dataset.iloc[train_dates]).to_numpy(np.float32)\n",
    "        close_test =  index_data[\"original\"][\"Close\"].reindex_like(opn_dataset.iloc[train_dates]).to_numpy(np.float32)\n",
    "        #training\n",
    "        for key, info in models:\n",
    "            \n",
    "            model = info[0]\n",
    "            x_train, x_val, scaler = feature_sets[info[1]]\n",
    "            \n",
    "            xd_train = days_group(x_train, n_days=n_days)\n",
    "            yd_train = y_train[n_days:]\n",
    "            xd_val = days_group(x_val, n_days=n_days)\n",
    "            yd_val = y_val[n_days:]\n",
    "                        \n",
    "            model.fit(xd_train, yd_train)\n",
    "            \n",
    "            pred = model.predict(xd_val)\n",
    "            pred_unsc = scaler.inverse_transform(pred)\n",
    "#             y_unsc = y_scaler.inverse_transform(y[i])\n",
    "            \n",
    "\n",
    "            mape = (abs(close_test - pred_unsc)/close_test).mean()\n",
    "            mspe = (((close_test - pred_unsc)/close_test)**2).mean()\n",
    "            acc = 1 - mape\n",
    "            roi = roi_score(close_test, pred_unsc, opn_test)\n",
    "        \n",
    "#         print(\"FOLD {}\".format(i))\n",
    "#         train_dates = X.index[train]\n",
    "#         val_dates = X.index[val]\n",
    "#         print(\"Training set da {} a {}\".format(train_dates.min(), train_dates.max()))\n",
    "#         print(\"Validation set da {} a {}\".format(val_dates.min(), val_dates.max()))\n",
    "\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metriche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "c9nIWKRwYXRn",
    "outputId": "c39d95e8-f3fe-4108-bca3-9e64d9651cf1"
   },
   "outputs": [],
   "source": [
    "# ## Calcola la loss solo sull'utlimo elemento del batch\n",
    "# class RNNMSELoss(MSELoss):\n",
    "#     def __call__(self, input, target):\n",
    "#         return super().__call__(input, target[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NJIcMifAhDln"
   },
   "source": [
    "## Modelli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dates, val_dates = list(tss.split(datasets['^GSPC'][\"target\"]))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = datasets['^GSPC'][\"target\"]\n",
    "ext_dataset = datasets['^GSPC'][\"features\"]['open']\n",
    "ext_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "# sa = load('ext_encoder_better')\n",
    "ext_train, ext_val = ext_scaler.fit_transform(ext_dataset.iloc[train_dates].to_numpy(np.float32)), ext_scaler.transform(ext_dataset.iloc[val_dates].to_numpy(np.float32))\n",
    "y_train, y_val = y_scaler.fit_transform(y.iloc[train_dates].to_numpy(np.float32)[...,None]), y_scaler.transform(y.iloc[val_dates].to_numpy(np.float32)[...,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext = np.concatenate((ext_train, ext_val))\n",
    "y_data = np.concatenate((y_train, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = days_group(ext, n_days=n_days)\n",
    "y = y_data[n_days:]\n",
    "l1 = len(np.split(x, [(len(ext)*2)//3])[0])\n",
    "l2 = len(np.split(x, [(len(ext)*2)//3])[1])\n",
    "half = PredefinedSplit(np.concatenate((np.ones(l1)*-1,np.ones(l2))))\n",
    "half_split =  CVSplit(cv=half, stratified=False, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modello del paper di Moro\n",
    "#TODO prova\n",
    "n_days = 5\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "\n",
    "lstm_moro = NeuralNetRegressor(\n",
    "    module=SequenceDouble,\n",
    "    optimizer=optim.Adam,\n",
    "    batch_size=batch_size,\n",
    "    max_epochs = 400 #trovato empiricamente\n",
    "    train_split=half_split,\n",
    "    \n",
    "    module__nb_features=ext_train.shape[1],\n",
    "    module__hidden_size=256,\n",
    "    optimizer__lr=0.0001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_loss     dur\n",
      "-------  ------------  ------------  ------\n",
      "      1        \u001b[36m1.1838\u001b[0m        \u001b[32m2.4005\u001b[0m  3.1708\n",
      "      2        \u001b[36m0.3314\u001b[0m        \u001b[32m0.0768\u001b[0m  1.4612\n",
      "      3        \u001b[36m0.0299\u001b[0m        \u001b[32m0.0757\u001b[0m  2.3017\n",
      "      4        \u001b[36m0.0284\u001b[0m        \u001b[32m0.0748\u001b[0m  2.0754\n",
      "      5        0.0288        0.0775  1.9196\n",
      "      6        0.0307        0.0792  2.0209\n",
      "      7        0.0325        0.0813  2.6854\n",
      "      8        0.0345        0.0830  2.6317\n",
      "      9        0.0362        0.0846  2.3061\n",
      "     10        0.0377        0.0864  2.5968\n",
      "     11        0.0390        0.0886  2.9478\n",
      "     12        0.0401        0.0911  1.9306\n",
      "     13        0.0410        0.0936  1.5429\n",
      "     14        0.0416        0.0956  1.5579\n",
      "     15        0.0418        0.0968  1.5035\n",
      "     16        0.0416        0.0968  1.5557\n",
      "     17        0.0410        0.0956  1.9927\n",
      "     18        0.0402        0.0936  2.3326\n",
      "     19        0.0392        0.0911  2.2288\n",
      "     20        0.0381        0.0887  2.3320\n",
      "     21        0.0372        0.0866  2.0999\n",
      "     22        0.0363        0.0847  1.6990\n",
      "     23        0.0356        0.0831  1.6365\n",
      "     24        0.0349        0.0819  1.8959\n",
      "     25        0.0344        0.0807  2.0741\n",
      "     26        0.0339        0.0798  1.7671\n",
      "     27        0.0334        0.0789  2.8215\n",
      "     28        0.0330        0.0781  1.6685\n",
      "     29        0.0327        0.0773  2.8938\n",
      "     30        0.0323        0.0766  2.4232\n",
      "     31        0.0320        0.0760  1.8778\n",
      "     32        0.0317        0.0754  1.8233\n",
      "     33        0.0315        0.0748  2.0906\n",
      "     34        0.0312        \u001b[32m0.0742\u001b[0m  2.1265\n",
      "     35        0.0310        \u001b[32m0.0736\u001b[0m  1.7329\n",
      "     36        0.0307        \u001b[32m0.0731\u001b[0m  1.6939\n",
      "     37        0.0305        \u001b[32m0.0726\u001b[0m  1.7036\n",
      "     38        0.0303        \u001b[32m0.0721\u001b[0m  1.7274\n",
      "     39        0.0301        \u001b[32m0.0716\u001b[0m  1.5922\n",
      "     40        0.0299        \u001b[32m0.0711\u001b[0m  1.5528\n",
      "     41        0.0297        \u001b[32m0.0706\u001b[0m  2.3599\n",
      "     42        0.0295        \u001b[32m0.0701\u001b[0m  1.9250\n",
      "     43        0.0293        \u001b[32m0.0696\u001b[0m  1.5906\n",
      "     44        0.0291        \u001b[32m0.0691\u001b[0m  1.7868\n",
      "     45        0.0289        \u001b[32m0.0687\u001b[0m  1.8441\n",
      "     46        0.0287        \u001b[32m0.0682\u001b[0m  2.1024\n",
      "     47        0.0285        \u001b[32m0.0677\u001b[0m  2.1293\n",
      "     48        \u001b[36m0.0284\u001b[0m        \u001b[32m0.0673\u001b[0m  2.8732\n",
      "     49        \u001b[36m0.0282\u001b[0m        \u001b[32m0.0668\u001b[0m  2.2352\n",
      "     50        \u001b[36m0.0280\u001b[0m        \u001b[32m0.0664\u001b[0m  2.3368\n",
      "     51        \u001b[36m0.0278\u001b[0m        \u001b[32m0.0659\u001b[0m  2.2060\n",
      "     52        \u001b[36m0.0276\u001b[0m        \u001b[32m0.0655\u001b[0m  2.7553\n",
      "     53        \u001b[36m0.0275\u001b[0m        \u001b[32m0.0650\u001b[0m  1.7996\n",
      "     54        \u001b[36m0.0273\u001b[0m        \u001b[32m0.0645\u001b[0m  2.6240\n",
      "     55        \u001b[36m0.0271\u001b[0m        \u001b[32m0.0641\u001b[0m  1.8674\n",
      "     56        \u001b[36m0.0269\u001b[0m        \u001b[32m0.0636\u001b[0m  3.7110\n",
      "     57        \u001b[36m0.0268\u001b[0m        \u001b[32m0.0632\u001b[0m  2.4322\n",
      "     58        \u001b[36m0.0266\u001b[0m        \u001b[32m0.0627\u001b[0m  2.2354\n",
      "     59        \u001b[36m0.0265\u001b[0m        \u001b[32m0.0623\u001b[0m  2.2796\n",
      "     60        \u001b[36m0.0263\u001b[0m        \u001b[32m0.0618\u001b[0m  2.7112\n",
      "     61        \u001b[36m0.0261\u001b[0m        \u001b[32m0.0614\u001b[0m  2.5256\n",
      "     62        \u001b[36m0.0260\u001b[0m        \u001b[32m0.0609\u001b[0m  2.8949\n",
      "     63        \u001b[36m0.0258\u001b[0m        \u001b[32m0.0605\u001b[0m  2.8087\n",
      "     64        \u001b[36m0.0256\u001b[0m        \u001b[32m0.0601\u001b[0m  3.2092\n",
      "     65        \u001b[36m0.0255\u001b[0m        \u001b[32m0.0597\u001b[0m  1.5905\n",
      "     66        \u001b[36m0.0253\u001b[0m        \u001b[32m0.0592\u001b[0m  2.4027\n",
      "     67        \u001b[36m0.0252\u001b[0m        \u001b[32m0.0589\u001b[0m  1.5948\n",
      "     68        \u001b[36m0.0250\u001b[0m        \u001b[32m0.0584\u001b[0m  1.4427\n",
      "     69        \u001b[36m0.0249\u001b[0m        \u001b[32m0.0580\u001b[0m  1.4436\n",
      "     70        \u001b[36m0.0247\u001b[0m        \u001b[32m0.0575\u001b[0m  1.4405\n",
      "     71        \u001b[36m0.0246\u001b[0m        \u001b[32m0.0571\u001b[0m  1.4054\n",
      "     72        \u001b[36m0.0244\u001b[0m        \u001b[32m0.0567\u001b[0m  1.5773\n",
      "     73        \u001b[36m0.0243\u001b[0m        \u001b[32m0.0563\u001b[0m  1.4746\n",
      "     74        \u001b[36m0.0241\u001b[0m        \u001b[32m0.0559\u001b[0m  1.5933\n",
      "     75        \u001b[36m0.0240\u001b[0m        \u001b[32m0.0555\u001b[0m  2.0868\n",
      "     76        \u001b[36m0.0238\u001b[0m        \u001b[32m0.0551\u001b[0m  1.9045\n",
      "     77        \u001b[36m0.0237\u001b[0m        \u001b[32m0.0547\u001b[0m  1.6730\n",
      "     78        \u001b[36m0.0235\u001b[0m        \u001b[32m0.0543\u001b[0m  1.9149\n",
      "     79        \u001b[36m0.0234\u001b[0m        \u001b[32m0.0539\u001b[0m  1.6251\n",
      "     80        \u001b[36m0.0233\u001b[0m        \u001b[32m0.0535\u001b[0m  1.4283\n",
      "     81        \u001b[36m0.0231\u001b[0m        \u001b[32m0.0531\u001b[0m  1.8188\n",
      "     82        \u001b[36m0.0230\u001b[0m        \u001b[32m0.0528\u001b[0m  1.5490\n",
      "     83        \u001b[36m0.0228\u001b[0m        \u001b[32m0.0524\u001b[0m  1.4502\n",
      "     84        \u001b[36m0.0227\u001b[0m        \u001b[32m0.0520\u001b[0m  2.5139\n",
      "     85        \u001b[36m0.0226\u001b[0m        \u001b[32m0.0516\u001b[0m  1.4585\n",
      "     86        \u001b[36m0.0224\u001b[0m        \u001b[32m0.0512\u001b[0m  1.4861\n",
      "     87        \u001b[36m0.0223\u001b[0m        \u001b[32m0.0508\u001b[0m  2.1152\n",
      "     88        \u001b[36m0.0221\u001b[0m        \u001b[32m0.0504\u001b[0m  2.4839\n",
      "     89        \u001b[36m0.0220\u001b[0m        \u001b[32m0.0500\u001b[0m  1.8275\n",
      "     90        \u001b[36m0.0218\u001b[0m        \u001b[32m0.0496\u001b[0m  1.6511\n",
      "     91        \u001b[36m0.0217\u001b[0m        \u001b[32m0.0492\u001b[0m  1.6979\n",
      "     92        \u001b[36m0.0216\u001b[0m        \u001b[32m0.0488\u001b[0m  1.6711\n",
      "     93        \u001b[36m0.0214\u001b[0m        \u001b[32m0.0485\u001b[0m  1.7511\n",
      "     94        \u001b[36m0.0213\u001b[0m        \u001b[32m0.0481\u001b[0m  3.6281\n",
      "     95        \u001b[36m0.0212\u001b[0m        \u001b[32m0.0477\u001b[0m  1.9065\n",
      "     96        \u001b[36m0.0210\u001b[0m        \u001b[32m0.0474\u001b[0m  2.0281\n",
      "     97        \u001b[36m0.0209\u001b[0m        \u001b[32m0.0470\u001b[0m  2.4796\n",
      "     98        \u001b[36m0.0208\u001b[0m        \u001b[32m0.0466\u001b[0m  2.6689\n",
      "     99        \u001b[36m0.0206\u001b[0m        \u001b[32m0.0462\u001b[0m  1.9600\n",
      "    100        \u001b[36m0.0205\u001b[0m        \u001b[32m0.0458\u001b[0m  1.8610\n",
      "    101        \u001b[36m0.0204\u001b[0m        \u001b[32m0.0455\u001b[0m  2.7546\n",
      "    102        \u001b[36m0.0202\u001b[0m        \u001b[32m0.0451\u001b[0m  2.1636\n",
      "    103        \u001b[36m0.0201\u001b[0m        \u001b[32m0.0447\u001b[0m  2.1657\n",
      "    104        \u001b[36m0.0199\u001b[0m        \u001b[32m0.0443\u001b[0m  1.8971\n",
      "    105        \u001b[36m0.0198\u001b[0m        \u001b[32m0.0439\u001b[0m  1.9623\n",
      "    106        \u001b[36m0.0196\u001b[0m        \u001b[32m0.0435\u001b[0m  2.0095\n",
      "    107        \u001b[36m0.0195\u001b[0m        \u001b[32m0.0432\u001b[0m  1.9509\n",
      "    108        \u001b[36m0.0194\u001b[0m        \u001b[32m0.0428\u001b[0m  2.0377\n",
      "    109        \u001b[36m0.0193\u001b[0m        \u001b[32m0.0424\u001b[0m  2.0165\n",
      "    110        \u001b[36m0.0191\u001b[0m        \u001b[32m0.0420\u001b[0m  2.4184\n",
      "    111        \u001b[36m0.0190\u001b[0m        \u001b[32m0.0416\u001b[0m  2.9741\n",
      "    112        \u001b[36m0.0188\u001b[0m        \u001b[32m0.0412\u001b[0m  1.7449\n",
      "    113        \u001b[36m0.0187\u001b[0m        \u001b[32m0.0409\u001b[0m  1.7462\n",
      "    114        \u001b[36m0.0185\u001b[0m        \u001b[32m0.0404\u001b[0m  1.7608\n",
      "    115        \u001b[36m0.0183\u001b[0m        \u001b[32m0.0400\u001b[0m  1.7117\n",
      "    116        \u001b[36m0.0182\u001b[0m        \u001b[32m0.0397\u001b[0m  2.2138\n",
      "    117        \u001b[36m0.0181\u001b[0m        \u001b[32m0.0393\u001b[0m  1.8384\n",
      "    118        \u001b[36m0.0179\u001b[0m        \u001b[32m0.0389\u001b[0m  1.6652\n",
      "    119        \u001b[36m0.0178\u001b[0m        \u001b[32m0.0385\u001b[0m  2.1211\n",
      "    120        \u001b[36m0.0176\u001b[0m        \u001b[32m0.0381\u001b[0m  2.5968\n",
      "    121        \u001b[36m0.0174\u001b[0m        \u001b[32m0.0377\u001b[0m  4.0310\n",
      "    122        \u001b[36m0.0173\u001b[0m        \u001b[32m0.0373\u001b[0m  1.9263\n",
      "    123        \u001b[36m0.0171\u001b[0m        \u001b[32m0.0370\u001b[0m  1.8738\n",
      "    124        \u001b[36m0.0170\u001b[0m        \u001b[32m0.0366\u001b[0m  2.2617\n",
      "    125        \u001b[36m0.0168\u001b[0m        \u001b[32m0.0362\u001b[0m  1.9127\n",
      "    126        \u001b[36m0.0166\u001b[0m        \u001b[32m0.0359\u001b[0m  1.9365\n",
      "    127        \u001b[36m0.0164\u001b[0m        \u001b[32m0.0355\u001b[0m  2.3440\n",
      "    128        \u001b[36m0.0163\u001b[0m        \u001b[32m0.0351\u001b[0m  2.1735\n",
      "    129        \u001b[36m0.0161\u001b[0m        \u001b[32m0.0347\u001b[0m  2.1279\n",
      "    130        \u001b[36m0.0159\u001b[0m        \u001b[32m0.0344\u001b[0m  2.7236\n",
      "    131        \u001b[36m0.0158\u001b[0m        \u001b[32m0.0340\u001b[0m  2.6428\n",
      "    132        \u001b[36m0.0156\u001b[0m        \u001b[32m0.0337\u001b[0m  2.1083\n",
      "    133        \u001b[36m0.0155\u001b[0m        \u001b[32m0.0334\u001b[0m  2.1302\n",
      "    134        \u001b[36m0.0153\u001b[0m        \u001b[32m0.0331\u001b[0m  2.3779\n",
      "    135        \u001b[36m0.0151\u001b[0m        \u001b[32m0.0328\u001b[0m  2.4950\n",
      "    136        \u001b[36m0.0150\u001b[0m        \u001b[32m0.0325\u001b[0m  1.5815\n",
      "    137        \u001b[36m0.0148\u001b[0m        \u001b[32m0.0322\u001b[0m  1.6237\n",
      "    138        \u001b[36m0.0147\u001b[0m        \u001b[32m0.0319\u001b[0m  1.4411\n",
      "    139        \u001b[36m0.0145\u001b[0m        \u001b[32m0.0316\u001b[0m  2.7107\n",
      "    140        \u001b[36m0.0143\u001b[0m        \u001b[32m0.0314\u001b[0m  2.7044\n",
      "    141        \u001b[36m0.0142\u001b[0m        \u001b[32m0.0312\u001b[0m  4.1925\n",
      "    142        \u001b[36m0.0141\u001b[0m        \u001b[32m0.0310\u001b[0m  2.0255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    143        \u001b[36m0.0139\u001b[0m        \u001b[32m0.0308\u001b[0m  1.4514\n",
      "    144        \u001b[36m0.0138\u001b[0m        \u001b[32m0.0305\u001b[0m  1.4418\n",
      "    145        \u001b[36m0.0136\u001b[0m        \u001b[32m0.0303\u001b[0m  1.4827\n",
      "    146        \u001b[36m0.0135\u001b[0m        \u001b[32m0.0301\u001b[0m  1.7545\n",
      "    147        \u001b[36m0.0134\u001b[0m        \u001b[32m0.0299\u001b[0m  1.5382\n",
      "    148        \u001b[36m0.0133\u001b[0m        \u001b[32m0.0297\u001b[0m  1.9074\n",
      "    149        \u001b[36m0.0131\u001b[0m        \u001b[32m0.0296\u001b[0m  2.0913\n",
      "    150        \u001b[36m0.0130\u001b[0m        \u001b[32m0.0294\u001b[0m  1.7170\n",
      "    151        \u001b[36m0.0129\u001b[0m        \u001b[32m0.0294\u001b[0m  1.7078\n",
      "    152        \u001b[36m0.0128\u001b[0m        \u001b[32m0.0292\u001b[0m  1.6399\n",
      "    153        \u001b[36m0.0127\u001b[0m        \u001b[32m0.0291\u001b[0m  2.0171\n",
      "    154        \u001b[36m0.0126\u001b[0m        \u001b[32m0.0290\u001b[0m  1.8385\n",
      "    155        \u001b[36m0.0125\u001b[0m        \u001b[32m0.0290\u001b[0m  1.9224\n",
      "    156        \u001b[36m0.0124\u001b[0m        \u001b[32m0.0289\u001b[0m  5.6137\n",
      "    157        \u001b[36m0.0124\u001b[0m        \u001b[32m0.0288\u001b[0m  1.8560\n",
      "    158        \u001b[36m0.0123\u001b[0m        \u001b[32m0.0287\u001b[0m  1.6915\n",
      "    159        \u001b[36m0.0122\u001b[0m        0.0288  1.7247\n",
      "    160        \u001b[36m0.0122\u001b[0m        \u001b[32m0.0286\u001b[0m  1.7325\n",
      "    161        \u001b[36m0.0121\u001b[0m        0.0287  1.7686\n",
      "    162        \u001b[36m0.0120\u001b[0m        \u001b[32m0.0285\u001b[0m  2.1866\n",
      "    163        \u001b[36m0.0119\u001b[0m        0.0287  1.7638\n",
      "    164        \u001b[36m0.0119\u001b[0m        \u001b[32m0.0285\u001b[0m  1.7759\n",
      "    165        \u001b[36m0.0118\u001b[0m        0.0286  1.9163\n",
      "    166        \u001b[36m0.0118\u001b[0m        0.0285  1.7224\n",
      "    167        \u001b[36m0.0117\u001b[0m        0.0287  1.7913\n",
      "    168        \u001b[36m0.0117\u001b[0m        0.0286  2.0441\n",
      "    169        \u001b[36m0.0116\u001b[0m        0.0288  2.1023\n",
      "    170        \u001b[36m0.0116\u001b[0m        0.0287  1.7821\n",
      "    171        \u001b[36m0.0116\u001b[0m        0.0290  2.7131\n",
      "    172        \u001b[36m0.0116\u001b[0m        0.0290  2.3724\n",
      "    173        \u001b[36m0.0115\u001b[0m        0.0292  2.3289\n",
      "    174        \u001b[36m0.0115\u001b[0m        0.0293  2.9320\n",
      "    175        \u001b[36m0.0115\u001b[0m        0.0296  2.3443\n",
      "    176        0.0115        0.0295  4.4526\n",
      "    177        \u001b[36m0.0115\u001b[0m        0.0299  1.7560\n",
      "    178        0.0115        0.0298  1.4781\n",
      "    179        \u001b[36m0.0115\u001b[0m        0.0302  1.4687\n",
      "    180        0.0115        0.0303  2.3464\n",
      "    181        0.0115        0.0306  2.5492\n",
      "    182        0.0115        0.0306  2.2188\n",
      "    183        0.0115        0.0309  1.8922\n",
      "    184        0.0115        0.0309  3.6137\n",
      "    185        0.0115        0.0312  2.6776\n",
      "    186        0.0115        0.0312  2.6065\n",
      "    187        0.0115        0.0314  1.7433\n",
      "    188        0.0115        0.0314  1.9299\n",
      "    189        0.0115        0.0316  2.0359\n",
      "    190        0.0115        0.0314  1.8026\n",
      "    191        0.0115        0.0317  2.7436\n",
      "    192        0.0116        0.0314  3.1041\n",
      "    193        0.0115        0.0318  2.7483\n",
      "    194        0.0116        0.0314  2.3873\n",
      "    195        0.0115        0.0317  1.7375\n",
      "    196        0.0115        0.0312  2.8076\n",
      "    197        0.0115        0.0314  1.7007\n",
      "    198        0.0115        0.0309  1.6562\n",
      "    199        \u001b[36m0.0114\u001b[0m        0.0310  2.1574\n",
      "    200        0.0114        0.0306  1.7007\n",
      "    201        \u001b[36m0.0114\u001b[0m        0.0306  1.4234\n",
      "    202        0.0114        0.0302  1.4099\n",
      "    203        \u001b[36m0.0114\u001b[0m        0.0299  1.4556\n",
      "    204        \u001b[36m0.0114\u001b[0m        0.0297  1.6162\n",
      "    205        \u001b[36m0.0113\u001b[0m        0.0294  1.6031\n",
      "    206        \u001b[36m0.0113\u001b[0m        0.0292  1.6593\n",
      "    207        \u001b[36m0.0113\u001b[0m        0.0290  1.4400\n",
      "    208        \u001b[36m0.0113\u001b[0m        0.0286  1.4892\n",
      "    209        \u001b[36m0.0113\u001b[0m        0.0285  1.5891\n",
      "    210        \u001b[36m0.0112\u001b[0m        \u001b[32m0.0282\u001b[0m  1.7317\n",
      "    211        \u001b[36m0.0112\u001b[0m        \u001b[32m0.0281\u001b[0m  2.2200\n",
      "    212        \u001b[36m0.0112\u001b[0m        \u001b[32m0.0278\u001b[0m  2.4915\n",
      "    213        \u001b[36m0.0112\u001b[0m        \u001b[32m0.0276\u001b[0m  2.7667\n",
      "    214        \u001b[36m0.0112\u001b[0m        \u001b[32m0.0274\u001b[0m  2.3641\n",
      "    215        \u001b[36m0.0111\u001b[0m        \u001b[32m0.0273\u001b[0m  1.7979\n",
      "    216        \u001b[36m0.0111\u001b[0m        \u001b[32m0.0271\u001b[0m  4.0012\n",
      "    217        \u001b[36m0.0111\u001b[0m        \u001b[32m0.0270\u001b[0m  2.3137\n",
      "    218        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0268\u001b[0m  2.4763\n",
      "    219        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0268\u001b[0m  4.4185\n",
      "    220        \u001b[36m0.0110\u001b[0m        \u001b[32m0.0266\u001b[0m  2.5656\n",
      "    221        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0266\u001b[0m  2.8409\n",
      "    222        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0264\u001b[0m  1.9208\n",
      "    223        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0263\u001b[0m  1.4179\n",
      "    224        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0262\u001b[0m  2.2019\n",
      "    225        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0261\u001b[0m  3.7181\n",
      "    226        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0261\u001b[0m  2.6747\n",
      "    227        \u001b[36m0.0109\u001b[0m        \u001b[32m0.0260\u001b[0m  2.1443\n",
      "    228        0.0109        \u001b[32m0.0259\u001b[0m  1.7848\n",
      "    229        \u001b[36m0.0108\u001b[0m        \u001b[32m0.0258\u001b[0m  2.6393\n",
      "    230        \u001b[36m0.0108\u001b[0m        \u001b[32m0.0257\u001b[0m  2.4460\n",
      "    231        \u001b[36m0.0108\u001b[0m        \u001b[32m0.0257\u001b[0m  2.4165\n",
      "    232        \u001b[36m0.0108\u001b[0m        \u001b[32m0.0256\u001b[0m  1.5834\n",
      "    233        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0256\u001b[0m  1.5130\n",
      "    234        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0255\u001b[0m  1.5826\n",
      "    235        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0254\u001b[0m  1.5529\n",
      "    236        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0253\u001b[0m  2.0909\n",
      "    237        0.0107        \u001b[32m0.0253\u001b[0m  2.3724\n",
      "    238        0.0107        0.0253  2.1352\n",
      "    239        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0252\u001b[0m  1.8569\n",
      "    240        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0252\u001b[0m  1.9837\n",
      "    241        \u001b[36m0.0107\u001b[0m        \u001b[32m0.0251\u001b[0m  2.0234\n",
      "    242        \u001b[36m0.0106\u001b[0m        0.0251  1.4735\n",
      "    243        \u001b[36m0.0106\u001b[0m        \u001b[32m0.0250\u001b[0m  1.5087\n",
      "    244        0.0106        \u001b[32m0.0250\u001b[0m  1.5195\n",
      "    245        \u001b[36m0.0106\u001b[0m        \u001b[32m0.0249\u001b[0m  2.0283\n",
      "    246        \u001b[36m0.0106\u001b[0m        0.0249  1.6533\n",
      "    247        \u001b[36m0.0106\u001b[0m        \u001b[32m0.0249\u001b[0m  2.8277\n",
      "    248        \u001b[36m0.0106\u001b[0m        0.0249  2.5784\n",
      "    249        \u001b[36m0.0105\u001b[0m        \u001b[32m0.0248\u001b[0m  2.3708\n",
      "    250        \u001b[36m0.0105\u001b[0m        \u001b[32m0.0248\u001b[0m  1.4827\n",
      "    251        \u001b[36m0.0105\u001b[0m        \u001b[32m0.0247\u001b[0m  1.4876\n",
      "    252        \u001b[36m0.0105\u001b[0m        \u001b[32m0.0247\u001b[0m  1.5159\n",
      "    253        \u001b[36m0.0105\u001b[0m        \u001b[32m0.0246\u001b[0m  1.5964\n",
      "    254        \u001b[36m0.0105\u001b[0m        \u001b[32m0.0246\u001b[0m  1.5469\n",
      "    255        0.0105        \u001b[32m0.0245\u001b[0m  1.9477\n",
      "    256        0.0105        \u001b[32m0.0244\u001b[0m  2.1440\n",
      "    257        0.0105        0.0245  1.6969\n",
      "    258        \u001b[36m0.0105\u001b[0m        \u001b[32m0.0244\u001b[0m  1.9357\n",
      "    259        \u001b[36m0.0105\u001b[0m        \u001b[32m0.0244\u001b[0m  1.8947\n",
      "    260        \u001b[36m0.0105\u001b[0m        0.0244  2.6740\n",
      "    261        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0243\u001b[0m  2.7842\n",
      "    262        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0243\u001b[0m  2.4723\n",
      "    263        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0242\u001b[0m  2.4103\n",
      "    264        \u001b[36m0.0104\u001b[0m        0.0242  2.6102\n",
      "    265        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0242\u001b[0m  2.4858\n",
      "    266        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0242\u001b[0m  2.3650\n",
      "    267        \u001b[36m0.0104\u001b[0m        \u001b[32m0.0241\u001b[0m  2.4884\n",
      "    268        \u001b[36m0.0103\u001b[0m        \u001b[32m0.0241\u001b[0m  2.6485\n",
      "    269        0.0103        \u001b[32m0.0240\u001b[0m  2.7271\n",
      "    270        \u001b[36m0.0103\u001b[0m        0.0240  2.6237\n",
      "    271        0.0103        \u001b[32m0.0240\u001b[0m  4.6671\n",
      "    272        \u001b[36m0.0103\u001b[0m        0.0240  4.0740\n",
      "    273        \u001b[36m0.0103\u001b[0m        \u001b[32m0.0239\u001b[0m  2.0777\n",
      "    274        \u001b[36m0.0103\u001b[0m        \u001b[32m0.0239\u001b[0m  2.7130\n",
      "    275        0.0103        \u001b[32m0.0239\u001b[0m  2.8149\n",
      "    276        \u001b[36m0.0103\u001b[0m        0.0239  2.6431\n",
      "    277        \u001b[36m0.0103\u001b[0m        \u001b[32m0.0238\u001b[0m  2.1566\n",
      "    278        \u001b[36m0.0102\u001b[0m        \u001b[32m0.0238\u001b[0m  1.9474\n",
      "    279        \u001b[36m0.0102\u001b[0m        \u001b[32m0.0238\u001b[0m  2.5706\n",
      "    280        \u001b[36m0.0102\u001b[0m        0.0238  3.0203\n",
      "    281        0.0102        \u001b[32m0.0237\u001b[0m  1.6543\n",
      "    282        \u001b[36m0.0102\u001b[0m        0.0237  1.4413\n",
      "    283        0.0102        \u001b[32m0.0236\u001b[0m  1.4862\n",
      "    284        \u001b[36m0.0102\u001b[0m        0.0236  2.6163\n",
      "    285        0.0102        \u001b[32m0.0236\u001b[0m  2.8405\n",
      "    286        \u001b[36m0.0102\u001b[0m        \u001b[32m0.0236\u001b[0m  1.9852\n",
      "    287        0.0102        \u001b[32m0.0235\u001b[0m  2.1411\n",
      "    288        \u001b[36m0.0102\u001b[0m        \u001b[32m0.0235\u001b[0m  1.9170\n",
      "    289        \u001b[36m0.0102\u001b[0m        \u001b[32m0.0235\u001b[0m  1.8402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    290        \u001b[36m0.0101\u001b[0m        \u001b[32m0.0235\u001b[0m  1.6418\n",
      "    291        \u001b[36m0.0101\u001b[0m        \u001b[32m0.0235\u001b[0m  1.4736\n",
      "    292        \u001b[36m0.0101\u001b[0m        \u001b[32m0.0234\u001b[0m  1.7432\n",
      "    293        \u001b[36m0.0101\u001b[0m        \u001b[32m0.0234\u001b[0m  1.6126\n",
      "    294        \u001b[36m0.0101\u001b[0m        \u001b[32m0.0234\u001b[0m  4.0074\n",
      "    295        \u001b[36m0.0101\u001b[0m        \u001b[32m0.0234\u001b[0m  2.1259\n",
      "    296        \u001b[36m0.0101\u001b[0m        \u001b[32m0.0233\u001b[0m  1.9156\n",
      "    297        \u001b[36m0.0101\u001b[0m        0.0233  1.8668\n",
      "    298        0.0101        \u001b[32m0.0233\u001b[0m  2.2593\n",
      "    299        \u001b[36m0.0101\u001b[0m        0.0233  2.4057\n",
      "    300        0.0101        \u001b[32m0.0232\u001b[0m  2.3185\n",
      "    301        \u001b[36m0.0101\u001b[0m        0.0233  2.3027\n",
      "    302        0.0101        \u001b[32m0.0232\u001b[0m  2.4997\n",
      "    303        \u001b[36m0.0101\u001b[0m        \u001b[32m0.0232\u001b[0m  2.3009\n",
      "    304        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0232\u001b[0m  1.7721\n",
      "    305        \u001b[36m0.0100\u001b[0m        0.0232  1.7000\n",
      "    306        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0232\u001b[0m  1.6654\n",
      "    307        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0231\u001b[0m  1.7975\n",
      "    308        \u001b[36m0.0100\u001b[0m        0.0231  1.8772\n",
      "    309        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0231\u001b[0m  2.1128\n",
      "    310        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0231\u001b[0m  1.8554\n",
      "    311        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0231\u001b[0m  2.2813\n",
      "    312        \u001b[36m0.0100\u001b[0m        \u001b[32m0.0230\u001b[0m  2.1829\n",
      "    313        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0230\u001b[0m  2.0235\n",
      "    314        \u001b[36m0.0099\u001b[0m        0.0230  4.0134\n",
      "    315        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0230\u001b[0m  2.7514\n",
      "    316        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0230\u001b[0m  2.4448\n",
      "    317        \u001b[36m0.0099\u001b[0m        0.0230  1.8175\n",
      "    318        0.0099        \u001b[32m0.0230\u001b[0m  1.6922\n",
      "    319        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0229\u001b[0m  2.0621\n",
      "    320        \u001b[36m0.0099\u001b[0m        0.0229  6.2380\n",
      "    321        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0229\u001b[0m  2.4067\n",
      "    322        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0229\u001b[0m  2.5066\n",
      "    323        \u001b[36m0.0099\u001b[0m        0.0229  1.9653\n",
      "    324        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0229\u001b[0m  2.7981\n",
      "    325        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0228\u001b[0m  4.6701\n",
      "    326        0.0099        0.0228  1.6830\n",
      "    327        \u001b[36m0.0099\u001b[0m        0.0229  1.7302\n",
      "    328        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0228\u001b[0m  1.7343\n",
      "    329        0.0099        \u001b[32m0.0228\u001b[0m  2.6466\n",
      "    330        \u001b[36m0.0099\u001b[0m        0.0228  2.3964\n",
      "    331        \u001b[36m0.0099\u001b[0m        \u001b[32m0.0228\u001b[0m  2.3781\n",
      "    332        \u001b[36m0.0098\u001b[0m        0.0228  3.0783\n",
      "    333        \u001b[36m0.0098\u001b[0m        0.0228  2.5823\n",
      "    334        \u001b[36m0.0098\u001b[0m        \u001b[32m0.0228\u001b[0m  4.0080\n",
      "    335        0.0098        \u001b[32m0.0227\u001b[0m  1.8585\n",
      "    336        \u001b[36m0.0098\u001b[0m        0.0227  2.0656\n",
      "    337        0.0098        \u001b[32m0.0227\u001b[0m  2.3665\n",
      "    338        \u001b[36m0.0098\u001b[0m        0.0227  1.7694\n",
      "    339        \u001b[36m0.0098\u001b[0m        0.0227  2.4495\n",
      "    340        \u001b[36m0.0098\u001b[0m        \u001b[32m0.0227\u001b[0m  2.3616\n",
      "    341        0.0098        0.0227  2.4241\n",
      "    342        0.0098        0.0227  2.6926\n",
      "    343        \u001b[36m0.0098\u001b[0m        \u001b[32m0.0226\u001b[0m  2.1862\n",
      "    344        0.0098        0.0226  2.4009\n",
      "    345        \u001b[36m0.0097\u001b[0m        0.0227  2.3928\n",
      "    346        0.0098        \u001b[32m0.0226\u001b[0m  4.2724\n",
      "    347        0.0098        0.0226  4.2867\n",
      "    348        \u001b[36m0.0097\u001b[0m        \u001b[32m0.0226\u001b[0m  2.2673\n",
      "    349        \u001b[36m0.0097\u001b[0m        \u001b[32m0.0226\u001b[0m  2.4999\n",
      "    350        0.0097        0.0226  4.2787\n",
      "    351        \u001b[36m0.0097\u001b[0m        \u001b[32m0.0226\u001b[0m  3.0879\n",
      "    352        \u001b[36m0.0097\u001b[0m        \u001b[32m0.0225\u001b[0m  2.3673\n",
      "    353        \u001b[36m0.0097\u001b[0m        0.0226  2.7075\n",
      "    354        \u001b[36m0.0097\u001b[0m        0.0225  2.2973\n",
      "    355        \u001b[36m0.0097\u001b[0m        \u001b[32m0.0225\u001b[0m  2.2072\n",
      "    356        \u001b[36m0.0097\u001b[0m        0.0226  2.7925\n",
      "    357        0.0097        \u001b[32m0.0225\u001b[0m  4.0559\n",
      "    358        \u001b[36m0.0097\u001b[0m        0.0225  4.8622\n",
      "    359        0.0097        0.0225  2.1815\n",
      "    360        \u001b[36m0.0096\u001b[0m        0.0225  2.5951\n",
      "    361        0.0097        \u001b[32m0.0225\u001b[0m  2.4992\n",
      "    362        0.0097        \u001b[32m0.0225\u001b[0m  2.2490\n",
      "    363        \u001b[36m0.0096\u001b[0m        \u001b[32m0.0225\u001b[0m  6.3767\n",
      "    364        \u001b[36m0.0096\u001b[0m        0.0225  4.8053\n",
      "    365        \u001b[36m0.0096\u001b[0m        0.0225  2.9506\n",
      "    366        0.0096        \u001b[32m0.0225\u001b[0m  2.2454\n",
      "    367        \u001b[36m0.0096\u001b[0m        0.0225  2.3488\n",
      "    368        0.0096        \u001b[32m0.0225\u001b[0m  2.8151\n",
      "    369        \u001b[36m0.0096\u001b[0m        0.0225  3.7983\n",
      "    370        0.0096        0.0225  2.9187\n",
      "    371        0.0096        \u001b[32m0.0225\u001b[0m  2.1067\n",
      "    372        \u001b[36m0.0096\u001b[0m        \u001b[32m0.0224\u001b[0m  5.4916\n",
      "    373        \u001b[36m0.0096\u001b[0m        0.0224  1.8560\n",
      "    374        \u001b[36m0.0096\u001b[0m        0.0224  2.4398\n",
      "    375        \u001b[36m0.0096\u001b[0m        \u001b[32m0.0224\u001b[0m  2.4224\n",
      "    376        0.0096        \u001b[32m0.0224\u001b[0m  4.7464\n",
      "    377        \u001b[36m0.0095\u001b[0m        0.0224  4.0375\n",
      "    378        0.0096        0.0224  2.4860\n",
      "    379        \u001b[36m0.0095\u001b[0m        \u001b[32m0.0224\u001b[0m  2.1741\n",
      "    380        \u001b[36m0.0095\u001b[0m        0.0224  2.4314\n",
      "    381        \u001b[36m0.0095\u001b[0m        \u001b[32m0.0224\u001b[0m  2.2661\n",
      "    382        \u001b[36m0.0095\u001b[0m        0.0224  2.3411\n",
      "    383        \u001b[36m0.0095\u001b[0m        0.0224  3.6266\n",
      "    384        \u001b[36m0.0095\u001b[0m        0.0224  2.4719\n",
      "    385        0.0095        \u001b[32m0.0224\u001b[0m  2.6547\n",
      "    386        \u001b[36m0.0095\u001b[0m        0.0224  2.0573\n",
      "    387        \u001b[36m0.0095\u001b[0m        0.0224  1.7830\n",
      "    388        0.0095        0.0224  2.5040\n",
      "    389        0.0095        0.0224  3.2629\n",
      "    390        \u001b[36m0.0095\u001b[0m        0.0224  1.8947\n",
      "    391        \u001b[36m0.0095\u001b[0m        0.0224  2.1420\n",
      "    392        \u001b[36m0.0095\u001b[0m        0.0224  2.3758\n",
      "    393        0.0095        0.0224  1.9438\n",
      "    394        \u001b[36m0.0095\u001b[0m        0.0224  2.2055\n",
      "    395        \u001b[36m0.0095\u001b[0m        \u001b[32m0.0224\u001b[0m  1.6552\n",
      "    396        \u001b[36m0.0094\u001b[0m        0.0224  2.5392\n",
      "    397        0.0094        0.0224  2.3613\n",
      "    398        0.0094        0.0224  3.6078\n",
      "    399        0.0094        0.0224  3.1601\n",
      "    400        \u001b[36m0.0094\u001b[0m        0.0224  2.1425\n",
      "    401        \u001b[36m0.0094\u001b[0m        0.0224  1.7987\n",
      "    402        \u001b[36m0.0094\u001b[0m        0.0224  2.3638\n",
      "    403        \u001b[36m0.0094\u001b[0m        \u001b[32m0.0224\u001b[0m  2.2221\n",
      "    404        \u001b[36m0.0094\u001b[0m        0.0224  2.3379\n",
      "    405        \u001b[36m0.0094\u001b[0m        0.0224  1.4907\n",
      "    406        \u001b[36m0.0094\u001b[0m        0.0224  4.0377\n",
      "    407        \u001b[36m0.0094\u001b[0m        0.0224  2.5264\n",
      "    408        \u001b[36m0.0094\u001b[0m        0.0224  1.7552\n",
      "    409        \u001b[36m0.0094\u001b[0m        0.0224  1.5908\n",
      "    410        0.0094        0.0224  1.7909\n",
      "    411        \u001b[36m0.0094\u001b[0m        0.0224  2.1737\n",
      "    412        0.0094        0.0224  2.3550\n",
      "    413        0.0094        0.0224  2.3772\n",
      "    414        \u001b[36m0.0094\u001b[0m        0.0224  2.2260\n",
      "    415        0.0094        0.0224  1.7694\n",
      "    416        0.0094        0.0224  1.7064\n",
      "    417        \u001b[36m0.0093\u001b[0m        0.0224  1.7022\n",
      "    418        \u001b[36m0.0093\u001b[0m        0.0224  1.9426\n",
      "    419        \u001b[36m0.0093\u001b[0m        0.0224  2.7905\n",
      "    420        0.0093        0.0224  2.1152\n",
      "    421        \u001b[36m0.0093\u001b[0m        0.0224  2.7644\n",
      "    422        \u001b[36m0.0093\u001b[0m        0.0224  2.4584\n",
      "    423        \u001b[36m0.0093\u001b[0m        0.0224  2.7401\n",
      "    424        \u001b[36m0.0093\u001b[0m        0.0224  1.8576\n",
      "    425        \u001b[36m0.0093\u001b[0m        0.0224  1.7864\n",
      "    426        \u001b[36m0.0093\u001b[0m        0.0224  1.7005\n",
      "    427        \u001b[36m0.0093\u001b[0m        0.0224  2.3786\n",
      "    428        \u001b[36m0.0093\u001b[0m        0.0224  2.7350\n",
      "    429        \u001b[36m0.0093\u001b[0m        0.0224  3.4465\n",
      "    430        \u001b[36m0.0093\u001b[0m        0.0224  3.7729\n",
      "    431        \u001b[36m0.0093\u001b[0m        0.0224  2.3610\n",
      "    432        \u001b[36m0.0093\u001b[0m        0.0224  2.0408\n",
      "    433        \u001b[36m0.0093\u001b[0m        0.0224  1.9489\n",
      "    434        0.0093        0.0224  2.5128\n",
      "    435        \u001b[36m0.0093\u001b[0m        0.0224  2.4175\n",
      "    436        \u001b[36m0.0093\u001b[0m        0.0224  3.9045\n",
      "    437        \u001b[36m0.0092\u001b[0m        0.0224  2.4242\n",
      "    438        \u001b[36m0.0092\u001b[0m        0.0224  2.3169\n",
      "    439        \u001b[36m0.0092\u001b[0m        0.0224  2.7540\n",
      "    440        \u001b[36m0.0092\u001b[0m        0.0224  2.3817\n",
      "    441        \u001b[36m0.0092\u001b[0m        0.0224  2.3509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    442        \u001b[36m0.0092\u001b[0m        0.0224  2.6723\n",
      "    443        0.0092        0.0224  2.4992\n",
      "    444        \u001b[36m0.0092\u001b[0m        0.0225  2.4378\n",
      "    445        0.0092        0.0224  1.6334\n",
      "    446        \u001b[36m0.0092\u001b[0m        0.0225  1.5213\n",
      "    447        0.0092        0.0224  1.8514\n",
      "    448        \u001b[36m0.0092\u001b[0m        0.0225  1.7614\n",
      "    449        \u001b[36m0.0092\u001b[0m        0.0224  2.2144\n",
      "    450        \u001b[36m0.0092\u001b[0m        0.0224  2.3741\n",
      "    451        \u001b[36m0.0092\u001b[0m        0.0224  2.2914\n",
      "    452        0.0092        0.0224  2.3662\n",
      "    453        \u001b[36m0.0092\u001b[0m        0.0225  3.7863\n",
      "    454        0.0092        0.0224  1.5784\n",
      "    455        \u001b[36m0.0092\u001b[0m        0.0224  1.4472\n",
      "    456        0.0092        0.0224  1.4661\n",
      "    457        \u001b[36m0.0092\u001b[0m        0.0225  2.0896\n",
      "    458        \u001b[36m0.0092\u001b[0m        0.0225  1.7442\n",
      "    459        \u001b[36m0.0092\u001b[0m        0.0225  1.8802\n",
      "    460        0.0092        0.0225  1.8426\n",
      "    461        0.0092        0.0225  2.5476\n",
      "    462        0.0092        0.0225  2.6798\n",
      "    463        \u001b[36m0.0091\u001b[0m        0.0225  2.3710\n",
      "    464        \u001b[36m0.0091\u001b[0m        0.0225  2.0946\n",
      "    465        \u001b[36m0.0091\u001b[0m        0.0225  2.4359\n",
      "    466        \u001b[36m0.0091\u001b[0m        0.0225  2.0317\n",
      "    467        \u001b[36m0.0091\u001b[0m        0.0225  2.3085\n",
      "    468        0.0091        0.0225  1.4724\n",
      "    469        0.0091        0.0225  2.1731\n",
      "    470        0.0091        0.0225  2.0596\n",
      "    471        0.0091        0.0225  2.9114\n",
      "    472        0.0091        0.0225  2.3231\n",
      "    473        0.0091        0.0225  2.6617\n",
      "    474        \u001b[36m0.0091\u001b[0m        0.0225  2.9315\n",
      "    475        \u001b[36m0.0091\u001b[0m        0.0225  2.3563\n",
      "    476        0.0091        0.0225  2.5823\n",
      "    477        \u001b[36m0.0091\u001b[0m        0.0225  2.3054\n",
      "    478        \u001b[36m0.0091\u001b[0m        0.0225  2.3123\n",
      "    479        \u001b[36m0.0091\u001b[0m        0.0225  2.3244\n",
      "    480        \u001b[36m0.0091\u001b[0m        0.0225  2.4231\n",
      "    481        0.0091        0.0225  2.5699\n",
      "    482        0.0091        0.0225  2.2514\n",
      "    483        \u001b[36m0.0091\u001b[0m        0.0225  2.3396\n",
      "    484        0.0091        0.0225  1.9592\n",
      "    485        \u001b[36m0.0091\u001b[0m        0.0225  4.1500\n",
      "    486        0.0091        0.0225  1.6693\n",
      "    487        \u001b[36m0.0091\u001b[0m        0.0225  2.3819\n",
      "    488        0.0091        0.0225  2.6238\n",
      "    489        \u001b[36m0.0091\u001b[0m        0.0225  2.6888\n",
      "    490        0.0091        0.0225  2.3266\n",
      "    491        \u001b[36m0.0090\u001b[0m        0.0225  2.5189\n",
      "    492        0.0090        0.0226  6.9567\n",
      "    493        \u001b[36m0.0090\u001b[0m        0.0225  3.2996\n",
      "    494        0.0090        0.0226  2.8669\n",
      "    495        \u001b[36m0.0090\u001b[0m        0.0225  2.4463\n",
      "    496        0.0090        0.0226  2.7933\n",
      "    497        \u001b[36m0.0090\u001b[0m        0.0225  2.2887\n",
      "    498        0.0090        0.0226  2.3529\n",
      "    499        \u001b[36m0.0090\u001b[0m        0.0225  2.6453\n",
      "    500        0.0090        0.0226  2.4282\n",
      "    501        \u001b[36m0.0090\u001b[0m        0.0225  2.3740\n",
      "    502        0.0090        0.0226  2.5023\n",
      "    503        \u001b[36m0.0090\u001b[0m        0.0225  2.7419\n",
      "    504        0.0090        0.0226  2.4660\n",
      "    505        \u001b[36m0.0090\u001b[0m        0.0225  2.3808\n",
      "    506        0.0090        0.0226  4.8580\n",
      "    507        \u001b[36m0.0090\u001b[0m        0.0225  3.1899\n",
      "    508        0.0090        0.0226  2.3290\n",
      "    509        \u001b[36m0.0090\u001b[0m        0.0226  2.3067\n",
      "    510        0.0090        0.0226  2.2711\n",
      "    511        \u001b[36m0.0090\u001b[0m        0.0226  2.4097\n",
      "    512        0.0090        0.0226  2.1562\n",
      "    513        \u001b[36m0.0090\u001b[0m        0.0226  2.6737\n",
      "    514        0.0090        0.0227  1.9351\n",
      "    515        \u001b[36m0.0090\u001b[0m        0.0226  2.1861\n",
      "    516        0.0090        0.0227  1.9430\n",
      "    517        \u001b[36m0.0089\u001b[0m        0.0226  2.1975\n",
      "    518        0.0090        0.0227  2.1147\n",
      "    519        \u001b[36m0.0089\u001b[0m        0.0226  5.9829\n",
      "    520        0.0090        0.0227  2.6928\n",
      "    521        \u001b[36m0.0089\u001b[0m        0.0226  2.2427\n",
      "    522        0.0090        0.0228  2.2464\n",
      "    523        \u001b[36m0.0089\u001b[0m        0.0226  5.2967\n",
      "    524        0.0089        0.0227  2.3754\n",
      "    525        \u001b[36m0.0089\u001b[0m        0.0226  2.3616\n",
      "    526        0.0089        0.0228  2.6848\n",
      "    527        \u001b[36m0.0089\u001b[0m        0.0226  2.2826\n",
      "    528        0.0089        0.0228  1.9649\n",
      "    529        \u001b[36m0.0089\u001b[0m        0.0226  1.8229\n",
      "    530        0.0089        0.0228  1.4946\n",
      "    531        \u001b[36m0.0089\u001b[0m        0.0226  5.3931\n",
      "    532        0.0089        0.0228  3.3163\n",
      "    533        \u001b[36m0.0089\u001b[0m        0.0226  2.6227\n",
      "    534        0.0089        0.0229  2.4314\n",
      "    535        \u001b[36m0.0089\u001b[0m        0.0227  2.2646\n",
      "    536        0.0089        0.0229  2.3674\n",
      "    537        \u001b[36m0.0089\u001b[0m        0.0227  1.9927\n",
      "    538        0.0089        0.0229  5.8775\n",
      "    539        \u001b[36m0.0089\u001b[0m        0.0227  1.8540\n",
      "    540        0.0089        0.0228  2.3914\n",
      "    541        \u001b[36m0.0089\u001b[0m        0.0227  2.4438\n",
      "    542        0.0089        0.0228  2.6552\n",
      "    543        \u001b[36m0.0089\u001b[0m        0.0227  5.1247\n",
      "    544        0.0089        0.0229  2.0476\n",
      "    545        \u001b[36m0.0089\u001b[0m        0.0227  2.2234\n",
      "    546        0.0089        0.0229  2.0992\n",
      "    547        0.0089        0.0228  3.3204\n",
      "    548        0.0089        0.0229  1.9931\n",
      "    549        \u001b[36m0.0089\u001b[0m        0.0228  2.1623\n",
      "    550        0.0089        0.0230  1.7885\n",
      "    551        0.0089        0.0228  2.3516\n",
      "    552        0.0089        0.0230  2.0344\n",
      "    553        0.0089        0.0228  2.8471\n",
      "    554        0.0089        0.0231  2.3366\n",
      "    555        0.0089        0.0228  2.3994\n",
      "    556        0.0089        0.0231  2.1663\n",
      "    557        0.0089        0.0228  2.2838\n",
      "    558        0.0089        0.0231  2.3150\n",
      "    559        \u001b[36m0.0088\u001b[0m        0.0228  2.3605\n",
      "    560        0.0089        0.0230  2.3988\n",
      "    561        \u001b[36m0.0088\u001b[0m        0.0228  2.4315\n",
      "    562        0.0088        0.0230  2.1022\n",
      "    563        \u001b[36m0.0088\u001b[0m        0.0228  2.0517\n",
      "    564        0.0088        0.0230  1.8486\n",
      "    565        \u001b[36m0.0088\u001b[0m        0.0229  6.1112\n",
      "    566        0.0088        0.0230  2.4397\n",
      "    567        \u001b[36m0.0088\u001b[0m        0.0229  2.7137\n",
      "    568        0.0088        0.0230  2.4684\n",
      "    569        0.0088        0.0230  2.6487\n",
      "    570        0.0088        0.0230  2.4853\n",
      "    571        0.0088        0.0230  2.4234\n",
      "    572        0.0088        0.0231  3.4406\n",
      "    573        0.0088        0.0230  1.8510\n",
      "    574        0.0088        0.0232  2.5116\n",
      "    575        0.0088        0.0230  1.4337\n",
      "    576        0.0088        0.0232  1.5159\n",
      "    577        0.0088        0.0229  7.3086\n",
      "    578        0.0088        0.0234  4.1832\n",
      "    579        0.0088        0.0228  2.1608\n",
      "    580        0.0088        0.0236  2.6539\n",
      "    581        0.0088        0.0228  2.5730\n",
      "    582        0.0089        0.0234  2.3253\n",
      "    583        0.0088        0.0229  2.2985\n",
      "    584        0.0088        0.0232  2.6952\n",
      "    585        \u001b[36m0.0088\u001b[0m        0.0229  2.9178\n",
      "    586        0.0088        0.0231  2.4520\n",
      "    587        \u001b[36m0.0087\u001b[0m        0.0230  2.7622\n",
      "    588        0.0087        0.0231  1.7215\n",
      "    589        \u001b[36m0.0087\u001b[0m        0.0231  2.5161\n",
      "    590        0.0087        0.0231  2.6845\n",
      "    591        \u001b[36m0.0087\u001b[0m        0.0231  2.1898\n",
      "    592        0.0087        0.0231  1.9932\n",
      "    593        0.0087        0.0232  2.2847\n",
      "    594        0.0087        0.0232  2.4278\n",
      "    595        0.0087        0.0232  2.2781\n",
      "    596        0.0087        0.0232  1.7602\n",
      "    597        0.0087        0.0233  1.7016\n",
      "    598        0.0087        0.0232  1.6975\n",
      "    599        0.0087        0.0233  1.7318\n",
      "    600        0.0087        0.0233  1.9180\n",
      "    601        0.0087        0.0234  2.1724\n",
      "    602        0.0087        0.0233  1.9408\n",
      "    603        0.0087        0.0235  1.9649\n",
      "    604        0.0087        0.0232  2.2175\n",
      "    605        0.0087        0.0237  1.8291\n",
      "    606        0.0088        0.0230  2.1328\n",
      "    607        0.0088        0.0239  3.5043\n",
      "    608        0.0088        0.0230  2.2456\n",
      "    609        0.0088        0.0240  2.2631\n",
      "    610        0.0088        0.0231  2.2322\n",
      "    611        0.0088        0.0237  2.4917\n",
      "    612        0.0088        0.0231  4.1938\n",
      "    613        0.0088        0.0233  2.3816\n",
      "    614        \u001b[36m0.0087\u001b[0m        0.0232  4.0948\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    615        \u001b[36m0.0087\u001b[0m        0.0233  2.0286\n",
      "    616        \u001b[36m0.0087\u001b[0m        0.0233  2.3000\n",
      "    617        0.0087        0.0233  2.1088\n",
      "    618        \u001b[36m0.0087\u001b[0m        0.0233  2.3810\n",
      "    619        0.0087        0.0234  1.7208\n",
      "    620        0.0087        0.0234  4.2354\n",
      "    621        0.0087        0.0234  1.8476\n",
      "    622        0.0087        0.0235  1.8640\n",
      "    623        0.0087        0.0235  1.8067\n",
      "    624        0.0087        0.0235  1.8680\n",
      "    625        0.0087        0.0236  1.9456\n",
      "    626        0.0087        0.0236  2.7603\n",
      "    627        0.0087        0.0236  2.3349\n",
      "    628        0.0087        0.0237  1.8580\n",
      "    629        0.0087        0.0236  7.3926\n",
      "    630        0.0087        0.0238  2.3233\n",
      "    631        0.0087        0.0236  2.0887\n",
      "    632        0.0087        0.0239  2.2520\n",
      "    633        0.0087        0.0234  1.8905\n",
      "    634        0.0087        0.0242  2.2323\n",
      "    635        0.0087        0.0232  2.8707\n",
      "    636        0.0087        0.0246  1.8375\n",
      "    637        0.0088        0.0231  1.7653\n",
      "    638        0.0088        0.0246  2.5648\n",
      "    639        0.0088        0.0233  2.2665\n",
      "    640        0.0088        0.0240  2.0827\n",
      "    641        0.0087        0.0234  2.1940\n",
      "    642        0.0087        0.0236  1.5792\n",
      "    643        \u001b[36m0.0086\u001b[0m        0.0235  1.7701\n",
      "    644        \u001b[36m0.0086\u001b[0m        0.0236  1.8729\n",
      "    645        \u001b[36m0.0086\u001b[0m        0.0236  2.1620\n",
      "    646        \u001b[36m0.0086\u001b[0m        0.0236  3.8197\n",
      "    647        \u001b[36m0.0086\u001b[0m        0.0236  2.4315\n",
      "    648        \u001b[36m0.0086\u001b[0m        0.0237  4.8231\n",
      "    649        0.0086        0.0237  2.0059\n",
      "    650        0.0086        0.0238  2.9954\n",
      "    651        0.0086        0.0238  2.1189\n",
      "    652        0.0086        0.0239  2.8832\n",
      "    653        0.0086        0.0239  3.4739\n",
      "    654        0.0086        0.0240  3.0286\n",
      "    655        0.0086        0.0240  3.1267\n",
      "    656        0.0086        0.0240  3.0657\n",
      "    657        0.0086        0.0241  3.0532\n",
      "    658        0.0086        0.0240  8.0739\n",
      "    659        0.0086        0.0243  6.1059\n",
      "    660        0.0086        0.0240  2.2820\n",
      "    661        0.0086        0.0245  2.1282\n",
      "    662        0.0086        0.0238  1.9355\n",
      "    663        0.0086        0.0248  3.8198\n",
      "    664        0.0087        0.0235  2.8859\n",
      "    665        0.0087        0.0255  2.5198\n",
      "    666        0.0087        0.0234  1.9670\n",
      "    667        0.0088        0.0255  1.4232\n",
      "    668        0.0088        0.0235  1.7626\n",
      "    669        0.0088        0.0245  1.8407\n",
      "    670        0.0087        0.0236  2.0641\n",
      "    671        0.0086        0.0241  4.8287\n",
      "    672        \u001b[36m0.0086\u001b[0m        0.0238  2.5231\n",
      "    673        \u001b[36m0.0086\u001b[0m        0.0240  3.9945\n",
      "    674        \u001b[36m0.0085\u001b[0m        0.0239  2.4637\n",
      "    675        \u001b[36m0.0085\u001b[0m        0.0241  2.3277\n",
      "    676        \u001b[36m0.0085\u001b[0m        0.0240  2.1870\n",
      "    677        \u001b[36m0.0085\u001b[0m        0.0242  2.4057\n",
      "    678        0.0085        0.0242  2.0560\n",
      "    679        0.0085        0.0243  2.3475\n",
      "    680        0.0085        0.0243  2.2091\n",
      "    681        0.0085        0.0244  2.3528\n",
      "    682        0.0086        0.0244  2.3183\n",
      "    683        0.0086        0.0245  2.4369\n",
      "    684        0.0086        0.0246  2.6353\n",
      "    685        0.0086        0.0247  2.1894\n",
      "    686        0.0086        0.0247  1.8529\n",
      "    687        0.0086        0.0248  1.8588\n",
      "    688        0.0086        0.0249  1.9392\n",
      "    689        0.0086        0.0248  4.2032\n",
      "    690        0.0086        0.0251  2.4698\n",
      "    691        0.0086        0.0248  2.8106\n",
      "    692        0.0086        0.0254  2.4884\n",
      "    693        0.0086        0.0243  2.5258\n",
      "    694        0.0086        0.0264  1.7696\n",
      "    695        0.0086        0.0235  2.1191\n",
      "    696        0.0086        0.0275  2.7931\n",
      "    697        0.0088        0.0237  2.3079\n",
      "    698        0.0088        0.0260  2.2949\n",
      "    699        0.0088        0.0240  2.3321\n",
      "    700        0.0087        0.0255  2.8402\n",
      "    701        0.0086        0.0241  1.7947\n",
      "    702        0.0086        0.0251  2.9119\n",
      "    703        0.0085        0.0243  2.3869\n",
      "    704        \u001b[36m0.0085\u001b[0m        0.0249  2.3841\n",
      "    705        \u001b[36m0.0085\u001b[0m        0.0246  2.9200\n",
      "    706        \u001b[36m0.0085\u001b[0m        0.0250  2.1287\n",
      "    707        0.0085        0.0248  2.0130\n",
      "    708        0.0085        0.0251  1.9736\n",
      "    709        0.0085        0.0251  2.4759\n",
      "    710        0.0085        0.0253  1.6844\n",
      "    711        0.0085        0.0253  2.2583\n",
      "    712        0.0085        0.0255  2.4913\n",
      "    713        0.0085        0.0255  3.2775\n",
      "    714        0.0085        0.0258  1.9680\n",
      "    715        0.0085        0.0256  1.8684\n",
      "    716        0.0085        0.0261  3.5426\n",
      "    717        0.0085        0.0256  2.3832\n",
      "    718        0.0085        0.0269  2.3320\n",
      "    719        0.0085        0.0249  2.4715\n",
      "    720        0.0085        0.0287  2.2473\n",
      "    721        0.0086        0.0239  2.1894\n",
      "    722        0.0086        0.0299  3.9120\n",
      "    723        0.0089        0.0242  2.4025\n",
      "    724        0.0088        0.0273  3.7311\n",
      "    725        0.0087        0.0246  2.7475\n",
      "    726        0.0087        0.0273  1.9882\n",
      "    727        0.0087        0.0247  2.2400\n",
      "    728        0.0086        0.0269  1.5953\n",
      "    729        0.0086        0.0248  2.2300\n",
      "    730        0.0085        0.0264  2.7490\n",
      "    731        \u001b[36m0.0085\u001b[0m        0.0252  2.3306\n",
      "    732        \u001b[36m0.0084\u001b[0m        0.0263  2.3700\n",
      "    733        \u001b[36m0.0084\u001b[0m        0.0257  2.2334\n",
      "    734        \u001b[36m0.0084\u001b[0m        0.0264  1.9553\n",
      "    735        0.0084        0.0261  1.7723\n",
      "    736        0.0084        0.0267  1.9594\n",
      "    737        0.0084        0.0265  2.5866\n",
      "    738        0.0084        0.0271  2.7593\n",
      "    739        0.0084        0.0268  2.8958\n",
      "    740        0.0084        0.0277  2.5672\n",
      "    741        0.0085        0.0270  2.2905\n",
      "    742        0.0085        0.0286  2.4029\n",
      "    743        0.0085        0.0265  2.4784\n",
      "    744        0.0085        0.0311  4.6392\n",
      "    745        0.0086        0.0247  2.3357\n",
      "    746        0.0085        0.0336  2.7837\n",
      "    747        0.0088        0.0246  1.7572\n",
      "    748        0.0088        0.0308  1.7334\n",
      "    749        0.0088        0.0252  2.0891\n",
      "    750        0.0088        0.0318  3.1572\n",
      "    751        0.0090        0.0257  2.0331\n",
      "    752        0.0089        0.0295  2.1047\n",
      "    753        0.0089        0.0256  3.3282\n",
      "    754        0.0087        0.0275  1.8832\n",
      "    755        0.0086        0.0251  2.7901\n",
      "    756        \u001b[36m0.0084\u001b[0m        0.0260  1.9379\n",
      "    757        \u001b[36m0.0083\u001b[0m        0.0260  2.3050\n",
      "    758        \u001b[36m0.0083\u001b[0m        0.0263  2.4602\n",
      "    759        0.0083        0.0266  2.3112\n",
      "    760        0.0083        0.0269  2.4918\n",
      "    761        0.0083        0.0272  2.7822\n",
      "    762        0.0083        0.0275  2.0505\n",
      "    763        0.0083        0.0277  2.0042\n",
      "    764        0.0083        0.0280  2.4496\n",
      "    765        0.0084        0.0283  2.1464\n",
      "    766        0.0084        0.0286  4.2608\n",
      "    767        0.0084        0.0289  1.8755\n",
      "    768        0.0084        0.0292  2.5023\n",
      "    769        0.0084        0.0295  2.5761\n",
      "    770        0.0084        0.0298  1.9160\n",
      "    771        0.0084        0.0301  1.7113\n",
      "    772        0.0084        0.0304  1.6896\n",
      "    773        0.0084        0.0307  2.3507\n",
      "    774        0.0084        0.0310  2.8901\n",
      "    775        0.0084        0.0313  2.6666\n",
      "    776        0.0084        0.0316  4.8819\n",
      "    777        0.0085        0.0318  2.1952\n",
      "    778        0.0085        0.0322  2.5413\n",
      "    779        0.0085        0.0323  2.6064\n",
      "    780        0.0085        0.0325  2.5115\n",
      "    781        0.0085        0.0335  3.0257\n",
      "    782        0.0085        0.0292  2.6378\n",
      "    783        0.0084        0.0400  2.6778\n",
      "    784        0.0085        0.0248  6.0557\n",
      "    785        0.0086        0.0432  2.7401\n",
      "    786        0.0090        0.0270  2.4932\n",
      "    787        0.0092        0.0367  2.1075\n",
      "    788        0.0093        0.0312  1.4566\n",
      "    789        0.0097        0.0336  1.5731\n",
      "    790        0.0100        0.0305  1.5350\n",
      "    791        0.0098        0.0269  1.8852\n",
      "    792        0.0089        0.0249  1.9605\n",
      "    793        \u001b[36m0.0082\u001b[0m        0.0253  1.7744\n",
      "    794        \u001b[36m0.0081\u001b[0m        0.0256  2.1847\n",
      "    795        \u001b[36m0.0081\u001b[0m        0.0260  3.2654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    796        \u001b[36m0.0081\u001b[0m        0.0265  2.3348\n",
      "    797        0.0081        0.0270  1.7634\n",
      "    798        0.0081        0.0274  2.0629\n",
      "    799        0.0082        0.0278  2.1161\n",
      "    800        0.0082        0.0282  2.0298\n",
      "    801        0.0082        0.0286  1.9832\n",
      "    802        0.0082        0.0290  2.2902\n",
      "    803        0.0082        0.0294  2.5324\n",
      "    804        0.0083        0.0298  2.3451\n",
      "    805        0.0083        0.0302  2.2453\n",
      "    806        0.0083        0.0306  5.4505\n",
      "    807        0.0083        0.0310  2.1674\n",
      "    808        0.0083        0.0314  2.0414\n",
      "    809        0.0083        0.0318  1.6147\n",
      "    810        0.0083        0.0321  2.4435\n",
      "    811        0.0084        0.0324  2.4704\n",
      "    812        0.0084        0.0327  2.5981\n",
      "    813        0.0084        0.0329  2.3949\n",
      "    814        0.0084        0.0332  2.4269\n",
      "    815        0.0084        0.0334  2.5525\n",
      "    816        0.0084        0.0336  2.0336\n",
      "    817        0.0084        0.0337  1.8095\n",
      "    818        0.0084        0.0339  1.8046\n",
      "    819        0.0084        0.0340  1.8949\n",
      "    820        0.0084        0.0342  3.2379\n",
      "    821        0.0084        0.0343  2.7888\n",
      "    822        0.0084        0.0344  2.2745\n",
      "    823        0.0084        0.0345  2.3873\n",
      "    824        0.0084        0.0346  2.5254\n",
      "    825        0.0084        0.0347  2.3636\n",
      "    826        0.0084        0.0348  2.7546\n",
      "    827        0.0084        0.0349  2.2637\n",
      "    828        0.0084        0.0350  2.3495\n",
      "    829        0.0084        0.0350  2.8061\n",
      "    830        0.0084        0.0351  2.6164\n",
      "    831        0.0084        0.0352  2.5008\n",
      "    832        0.0084        0.0353  1.9705\n",
      "    833        0.0084        0.0354  1.9088\n",
      "    834        0.0084        0.0354  3.0995\n",
      "    835        0.0084        0.0357  2.5893\n",
      "    836        0.0084        0.0353  2.3768\n",
      "    837        0.0084        0.0367  2.7839\n",
      "    838        0.0084        0.0337  2.2724\n",
      "    839        0.0084        0.0413  2.7220\n",
      "    840        0.0084        0.0271  1.5622\n",
      "    841        0.0084        0.0492  1.4724\n",
      "    842        0.0087        0.0264  1.5063\n",
      "    843        0.0090        0.0518  2.7799\n",
      "    844        0.0092        0.0328  2.8361\n",
      "    845        0.0100        0.0317  2.9150\n",
      "    846        0.0101        0.0299  2.5622\n",
      "    847        0.0096        0.0267  2.6359\n",
      "    848        0.0087        0.0256  2.6936\n",
      "    849        0.0082        0.0263  2.1142\n",
      "    850        \u001b[36m0.0081\u001b[0m        0.0269  2.2331\n",
      "    851        \u001b[36m0.0081\u001b[0m        0.0276  2.6443\n",
      "    852        \u001b[36m0.0081\u001b[0m        0.0283  2.4943\n",
      "    853        0.0081        0.0289  2.8466\n",
      "    854        0.0081        0.0296  2.5557\n",
      "    855        0.0081        0.0301  2.3513\n",
      "    856        0.0081        0.0307  1.4592\n",
      "    857        0.0082        0.0313  1.4638\n",
      "    858        0.0082        0.0318  1.6103\n",
      "    859        0.0082        0.0323  1.6196\n",
      "    860        0.0082        0.0328  2.4350\n",
      "    861        0.0082        0.0332  2.6704\n",
      "    862        0.0082        0.0336  1.8669\n",
      "    863        0.0083        0.0340  2.4757\n",
      "    864        0.0083        0.0343  2.6634\n",
      "    865        0.0083        0.0346  1.7859\n",
      "    866        0.0083        0.0348  2.6603\n",
      "    867        0.0083        0.0350  2.4409\n",
      "    868        0.0083        0.0352  2.6280\n",
      "    869        0.0083        0.0354  2.0779\n",
      "    870        0.0083        0.0356  2.7902\n",
      "    871        0.0083        0.0357  2.3545\n",
      "    872        0.0083        0.0358  2.0972\n",
      "    873        0.0083        0.0359  2.7742\n",
      "    874        0.0083        0.0361  1.9308\n",
      "    875        0.0083        0.0361  2.5827\n",
      "    876        0.0083        0.0363  2.3258\n",
      "    877        0.0083        0.0362  1.9756\n",
      "    878        0.0083        0.0365  2.1012\n",
      "    879        0.0083        0.0363  2.0964\n",
      "    880        0.0083        0.0368  2.4119\n",
      "    881        0.0083        0.0363  6.6681\n",
      "    882        0.0083        0.0372  3.0053\n",
      "    883        0.0083        0.0361  2.5644\n",
      "    884        0.0083        0.0378  2.2775\n",
      "    885        0.0083        0.0357  2.4649\n",
      "    886        0.0083        0.0388  3.1138\n",
      "    887        0.0083        0.0346  2.1766\n",
      "    888        0.0083        0.0417  2.6248\n",
      "    889        0.0083        0.0314  3.4344\n",
      "    890        0.0084        0.0500  2.4781\n",
      "    891        0.0084        0.0270  2.8409\n",
      "    892        0.0086        0.0594  2.0641\n",
      "    893        0.0090        0.0316  2.1153\n",
      "    894        0.0099        0.0346  2.8028\n",
      "    895        0.0101        0.0356  2.5826\n",
      "    896        0.0103        0.0294  2.6659\n",
      "    897        0.0092        0.0266  1.7672\n",
      "    898        0.0083        0.0267  1.8724\n",
      "    899        \u001b[36m0.0080\u001b[0m        0.0275  2.1935\n",
      "    900        \u001b[36m0.0080\u001b[0m        0.0282  2.0743\n",
      "    901        \u001b[36m0.0080\u001b[0m        0.0290  2.0084\n",
      "    902        0.0080        0.0297  2.0027\n",
      "    903        0.0080        0.0304  3.1806\n",
      "    904        0.0080        0.0311  2.3230\n",
      "    905        0.0081        0.0317  2.1016\n",
      "    906        0.0081        0.0324  2.1843\n",
      "    907        0.0081        0.0330  2.4745\n",
      "    908        0.0081        0.0336  2.0586\n",
      "    909        0.0082        0.0341  1.9097\n",
      "    910        0.0082        0.0346  1.7874\n",
      "    911        0.0082        0.0351  1.8334\n",
      "    912        0.0082        0.0355  1.6991\n",
      "    913        0.0082        0.0358  1.8926\n",
      "    914        0.0082        0.0361  3.1991\n",
      "    915        0.0082        0.0364  3.8732\n",
      "    916        0.0082        0.0366  2.1380\n",
      "    917        0.0082        0.0368  2.3437\n",
      "    918        0.0083        0.0370  2.7556\n",
      "    919        0.0083        0.0371  2.2704\n",
      "    920        0.0083        0.0372  2.3202\n",
      "    921        0.0083        0.0374  2.4541\n",
      "    922        0.0083        0.0375  1.8866\n",
      "    923        0.0083        0.0376  1.7069\n",
      "    924        0.0083        0.0377  2.1424\n",
      "    925        0.0083        0.0377  1.8647\n",
      "    926        0.0083        0.0378  1.6150\n",
      "    927        0.0083        0.0379  2.0508\n",
      "    928        0.0083        0.0380  2.0251\n",
      "    929        0.0083        0.0381  2.4369\n",
      "    930        0.0083        0.0381  2.3384\n",
      "    931        0.0083        0.0382  2.0643\n",
      "    932        0.0083        0.0383  2.7754\n",
      "    933        0.0083        0.0384  2.5773\n",
      "    934        0.0083        0.0384  2.4791\n",
      "    935        0.0083        0.0385  2.5048\n",
      "    936        0.0083        0.0386  2.3997\n",
      "    937        0.0083        0.0386  2.4047\n",
      "    938        0.0083        0.0388  2.7107\n",
      "    939        0.0083        0.0388  2.6133\n",
      "    940        0.0083        0.0389  7.2647\n",
      "    941        0.0083        0.0390  2.3906\n",
      "    942        0.0083        0.0391  4.3913\n",
      "    943        0.0083        0.0390  2.0225\n",
      "    944        0.0083        0.0392  2.0504\n",
      "    945        0.0083        0.0393  3.9172\n",
      "    946        0.0083        0.0393  2.5482\n",
      "    947        0.0083        0.0394  2.3312\n",
      "    948        0.0083        0.0390  2.5136\n",
      "    949        0.0083        0.0402  2.4755\n",
      "    950        0.0083        0.0359  2.1553\n",
      "    951        0.0083        0.0431  2.2140\n",
      "    952        0.0084        0.0325  2.9565\n",
      "    953        0.0085        0.0516  3.1123\n",
      "    954        0.0088        0.0296  2.5318\n",
      "    955        0.0089        0.0573  4.2605\n",
      "    956        0.0090        0.0294  6.1512\n",
      "    957        0.0094        0.0302  2.7875\n",
      "    958        0.0089        0.0310  1.9881\n",
      "    959        0.0083        0.0317  1.8360\n",
      "    960        0.0081        0.0329  1.8251\n",
      "    961        0.0080        0.0340  1.7853\n",
      "    962        0.0080        0.0350  1.7139\n",
      "    963        0.0080        0.0360  1.8291\n",
      "    964        0.0080        0.0368  1.7364\n",
      "    965        0.0081        0.0375  1.7382\n",
      "    966        0.0081        0.0381  1.7278\n",
      "    967        0.0081        0.0386  2.4879\n",
      "    968        0.0081        0.0390  2.1838\n",
      "    969        0.0081        0.0393  1.9927\n",
      "    970        0.0082        0.0395  1.9096\n",
      "    971        0.0082        0.0398  1.9612\n",
      "    972        0.0082        0.0400  1.9934\n",
      "    973        0.0082        0.0401  1.5217\n",
      "    974        0.0082        0.0403  2.6139\n",
      "    975        0.0082        0.0404  2.0048\n",
      "    976        0.0082        0.0405  1.9543\n",
      "    977        0.0082        0.0406  2.2609\n",
      "    978        0.0082        0.0407  2.9320\n",
      "    979        0.0082        0.0408  2.4476\n",
      "    980        0.0082        0.0409  2.2641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    981        0.0082        0.0410  2.5763\n",
      "    982        0.0082        0.0411  7.7158\n",
      "    983        0.0082        0.0412  4.3456\n",
      "    984        0.0082        0.0413  2.3057\n",
      "    985        0.0082        0.0414  2.3442\n",
      "    986        0.0082        0.0415  1.9492\n",
      "    987        0.0082        0.0416  2.0061\n",
      "    988        0.0082        0.0417  2.0272\n",
      "    989        0.0082        0.0418  2.3404\n",
      "    990        0.0082        0.0418  1.9195\n",
      "    991        0.0082        0.0420  2.1156\n",
      "    992        0.0082        0.0422  1.9028\n",
      "    993        0.0082        0.0420  1.7617\n",
      "    994        0.0082        0.0424  2.4214\n",
      "    995        0.0082        0.0426  1.7712\n",
      "    996        0.0082        0.0421  1.9404\n",
      "    997        0.0082        0.0429  2.0737\n",
      "    998        0.0082        0.0420  1.8130\n",
      "    999        0.0082        0.0432  4.2314\n",
      "   1000        0.0082        0.0413  2.7723\n",
      "   1001        0.0083        0.0420  3.0544\n",
      "   1002        0.0082        0.0399  2.1673\n",
      "   1003        0.0083        0.0350  2.1011\n",
      "   1004        0.0083        0.0459  2.9950\n",
      "   1005        0.0086        0.0340  2.6120\n",
      "   1006        0.0086        0.0543  7.2216\n",
      "   1007        0.0087        0.0308  1.7400\n",
      "   1008        0.0087        0.0472  2.3758\n",
      "   1009        0.0085        0.0319  3.2135\n",
      "   1010        0.0083        0.0423  2.3472\n",
      "   1011        0.0082        0.0364  3.8641\n",
      "   1012        0.0081        0.0432  2.4487\n",
      "   1013        0.0080        0.0388  2.5055\n",
      "   1014        0.0081        0.0446  1.6007\n",
      "   1015        0.0081        0.0395  2.9574\n",
      "   1016        0.0081        0.0458  2.4429\n",
      "   1017        0.0081        0.0394  1.7967\n",
      "   1018        0.0081        0.0469  3.0134\n",
      "   1019        0.0081        0.0386  2.1564\n",
      "   1020        0.0082        0.0482  1.4220\n",
      "   1021        0.0081        0.0372  2.6943\n",
      "   1022        0.0082        0.0497  2.8263\n",
      "   1023        0.0082        0.0350  3.2703\n",
      "   1024        0.0082        0.0512  5.1988\n",
      "   1025        0.0082        0.0328  2.0853\n",
      "   1026        0.0083        0.0516  2.3563\n",
      "   1027        0.0083        0.0326  3.2784\n",
      "   1028        0.0083        0.0521  3.4859\n",
      "   1029        0.0083        0.0344  2.0766\n",
      "   1030        0.0083        0.0520  2.0828\n",
      "   1031        0.0082        0.0364  1.7717\n",
      "   1032        0.0082        0.0506  1.9903\n",
      "   1033        0.0082        0.0385  1.9641\n",
      "   1034        0.0082        0.0490  2.0092\n",
      "   1035        0.0082        0.0405  2.0098\n",
      "   1036        0.0082        0.0479  1.9950\n",
      "   1037        0.0081        0.0425  1.7583\n",
      "   1038        0.0081        0.0467  2.3233\n",
      "   1039        0.0081        0.0444  2.7935\n",
      "   1040        0.0081        0.0455  2.3448\n",
      "   1041        0.0081        0.0463  2.3589\n",
      "   1042        0.0081        0.0439  2.3338\n",
      "   1043        0.0082        0.0481  2.3603\n",
      "   1044        0.0081        0.0419  5.9054\n",
      "   1045        0.0082        0.0499  3.5195\n",
      "   1046        0.0082        0.0396  1.8033\n",
      "   1047        0.0082        0.0530  2.7519\n",
      "   1048        0.0082        0.0371  2.2309\n",
      "   1049        0.0083        0.0590  3.6096\n",
      "   1050        0.0083        0.0362  2.1989\n",
      "   1051        0.0083        0.0607  2.1552\n",
      "   1052        0.0084        0.0387  2.2849\n",
      "   1053        0.0085        0.0558  2.3548\n",
      "   1054        0.0084        0.0411  2.2998\n",
      "   1055        0.0084        0.0523  2.9991\n",
      "   1056        0.0083        0.0420  4.8307\n",
      "   1057        0.0082        0.0508  2.6258\n",
      "   1058        0.0081        0.0429  1.8477\n",
      "   1059        0.0081        0.0511  1.8123\n",
      "   1060        0.0080        0.0430  1.9690\n",
      "   1061        0.0081        0.0520  1.7546\n",
      "   1062        0.0081        0.0422  2.1513\n",
      "   1063        0.0081        0.0530  1.8229\n",
      "   1064        0.0081        0.0409  1.5996\n",
      "   1065        0.0081        0.0544  1.3904\n",
      "   1066        0.0081        0.0392  1.4646\n",
      "   1067        0.0082        0.0562  2.3498\n",
      "   1068        0.0082        0.0375  2.7733\n",
      "   1069        0.0082        0.0581  2.7114\n",
      "   1070        0.0082        0.0368  2.3236\n",
      "   1071        0.0082        0.0592  3.1863\n",
      "   1072        0.0082        0.0381  2.7132\n",
      "   1073        0.0082        0.0578  2.4565\n",
      "   1074        0.0082        0.0416  2.7099\n",
      "   1075        0.0082        0.0539  2.7900\n",
      "   1076        0.0081        0.0463  2.5040\n",
      "   1077        0.0081        0.0503  2.9063\n",
      "   1078        0.0081        0.0503  4.1868\n",
      "   1079        0.0081        0.0478  2.7043\n",
      "   1080        0.0081        0.0526  2.4034\n",
      "   1081        0.0081        0.0459  2.2245\n",
      "   1082        0.0081        0.0538  1.8869\n",
      "   1083        0.0081        0.0451  2.6476\n",
      "   1084        0.0081        0.0546  2.5394\n",
      "   1085        0.0081        0.0466  2.4386\n",
      "   1086        0.0081        0.0533  2.7573\n",
      "   1087        0.0081        0.0511  1.8130\n",
      "   1088        0.0081        0.0464  2.5239\n",
      "   1089        0.0082        0.0529  1.6202\n",
      "   1090        0.0082        0.0348  2.3458\n",
      "   1091        0.0083        0.0468  2.3538\n",
      "   1092        0.0083        0.0383  2.8315\n",
      "   1093        0.0082        0.0509  2.3113\n",
      "   1094        0.0081        0.0408  2.4439\n",
      "   1095        0.0081        0.0547  2.0559\n",
      "   1096        0.0081        0.0438  2.5944\n",
      "   1097        0.0081        0.0535  4.4547\n",
      "   1098        0.0081        0.0483  2.5348\n",
      "   1099        0.0081        0.0451  2.6264\n",
      "   1100        0.0082        0.0495  2.3206\n",
      "   1101        0.0081        0.0361  2.0170\n",
      "   1102        0.0082        0.0459  2.3349\n",
      "   1103        0.0082        0.0391  1.7869\n",
      "   1104        0.0081        0.0509  2.0742\n",
      "   1105        0.0081        0.0411  5.8028\n",
      "   1106        0.0081        0.0549  2.7090\n",
      "   1107        0.0081        0.0439  2.1475\n",
      "   1108        0.0081        0.0535  2.8970\n",
      "   1109        0.0081        0.0481  5.5166\n",
      "   1110        0.0081        0.0436  2.7271\n",
      "   1111        0.0082        0.0477  2.9363\n",
      "   1112        0.0082        0.0367  3.9316\n",
      "   1113        0.0082        0.0453  2.2888\n",
      "   1114        0.0081        0.0401  2.2539\n",
      "   1115        0.0081        0.0502  2.2784\n",
      "   1116        0.0081        0.0428  2.2641\n",
      "   1117        0.0081        0.0531  2.6702\n",
      "   1118        0.0081        0.0463  3.4097\n",
      "   1119        0.0081        0.0503  2.6770\n",
      "   1120        0.0081        0.0492  2.9129\n",
      "   1121        0.0081        0.0414  4.8287\n",
      "   1122        0.0082        0.0477  2.3959\n",
      "   1123        0.0081        0.0370  2.4817\n",
      "   1124        0.0082        0.0465  3.0481\n",
      "   1125        0.0081        0.0402  1.9563\n",
      "   1126        0.0081        0.0520  2.2257\n",
      "   1127        0.0081        0.0422  2.0426\n",
      "   1128        0.0081        0.0555  5.5526\n",
      "   1129        0.0081        0.0458  1.9374\n",
      "   1130        0.0081        0.0482  3.3121\n",
      "   1131        0.0082        0.0458  2.4660\n",
      "   1132        0.0082        0.0393  1.8130\n",
      "   1133        0.0083        0.0436  5.5164\n",
      "   1134        0.0082        0.0435  2.7265\n",
      "   1135        0.0082        0.0483  3.6442\n",
      "   1136        0.0081        0.0446  4.4767\n",
      "   1137        0.0081        0.0514  2.2953\n",
      "   1138        0.0080        0.0473  2.0097\n",
      "   1139        0.0080        0.0512  2.3617\n",
      "   1140        0.0080        0.0506  2.3739\n",
      "   1141        0.0080        0.0478  2.2350\n",
      "   1142        0.0081        0.0527  2.2855\n",
      "   1143        0.0080        0.0423  2.3407\n",
      "   1144        0.0081        0.0559  2.3605\n",
      "   1145        0.0081        0.0383  2.3555\n",
      "   1146        0.0081        0.0642  2.3113\n",
      "   1147        0.0082        0.0415  2.5597\n",
      "   1148        0.0082        0.0617  2.3484\n",
      "   1149        0.0083        0.0498  2.3979\n",
      "   1150        0.0082        0.0396  2.4671\n",
      "   1151        0.0084        0.0484  2.3265\n",
      "   1152        0.0081        0.0440  2.1953\n",
      "   1153        0.0081        0.0531  2.1439\n",
      "   1154        \u001b[36m0.0080\u001b[0m        0.0480  2.6517\n",
      "   1155        \u001b[36m0.0080\u001b[0m        0.0545  2.0775\n",
      "   1156        \u001b[36m0.0080\u001b[0m        0.0519  2.4458\n",
      "   1157        0.0080        0.0531  2.3454\n",
      "   1158        0.0080        0.0548  2.2315\n",
      "   1159        0.0080        0.0510  2.4383\n",
      "   1160        0.0080        0.0565  2.3612\n",
      "   1161        0.0080        0.0500  2.1008\n",
      "   1162        0.0080        0.0578  3.8433\n",
      "   1163        0.0080        0.0527  2.3944\n",
      "   1164        0.0080        0.0553  2.3756\n",
      "   1165        0.0081        0.0541  3.3694\n",
      "   1166        0.0081        0.0419  4.4182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1167        0.0082        0.0504  2.2813\n",
      "   1168        0.0081        0.0392  3.1949\n",
      "   1169        0.0082        0.0514  2.6350\n",
      "   1170        0.0081        0.0428  2.2321\n",
      "   1171        0.0081        0.0599  2.7139\n",
      "   1172        0.0081        0.0463  2.6834\n",
      "   1173        0.0081        0.0571  2.0226\n",
      "   1174        0.0082        0.0476  6.9536\n",
      "   1175        0.0081        0.0396  2.7987\n",
      "   1176        0.0084        0.0445  2.4170\n",
      "   1177        0.0082        0.0529  2.3813\n",
      "   1178        0.0082        0.0518  2.6702\n",
      "   1179        0.0080        0.0522  2.2924\n",
      "   1180        0.0080        0.0545  1.9571\n",
      "   1181        \u001b[36m0.0080\u001b[0m        0.0549  2.5828\n",
      "   1182        \u001b[36m0.0080\u001b[0m        0.0533  7.3160\n",
      "   1183        0.0080        0.0570  2.2965\n",
      "   1184        0.0080        0.0508  2.4403\n",
      "   1185        0.0080        0.0587  3.1398\n",
      "   1186        0.0080        0.0499  2.5023\n",
      "   1187        0.0080        0.0604  1.7772\n",
      "   1188        0.0080        0.0534  1.8507\n",
      "   1189        0.0080        0.0572  1.7157\n",
      "   1190        0.0081        0.0549  2.6139\n",
      "   1191        0.0080        0.0412  2.8284\n",
      "   1192        0.0082        0.0503  5.1550\n",
      "   1193        0.0081        0.0432  2.2307\n",
      "   1194        0.0081        0.0577  4.3729\n",
      "   1195        0.0081        0.0428  2.4036\n",
      "   1196        0.0081        0.0678  2.5639\n",
      "   1197        0.0081        0.0474  2.4505\n",
      "   1198        0.0081        0.0570  2.3324\n",
      "   1199        0.0082        0.0496  2.4152\n",
      "   1200        0.0081        0.0392  1.9340\n",
      "   1201        0.0083        0.0462  2.0734\n",
      "   1202        0.0081        0.0524  1.8826\n",
      "   1203        0.0081        0.0532  2.4862\n",
      "   1204        0.0080        0.0556  1.8340\n",
      "   1205        \u001b[36m0.0079\u001b[0m        0.0534  1.8134\n",
      "   1206        0.0080        0.0582  2.0226\n",
      "   1207        \u001b[36m0.0079\u001b[0m        0.0486  1.9520\n",
      "   1208        0.0080        0.0602  2.8397\n",
      "   1209        0.0080        0.0432  2.4567\n",
      "   1210        0.0080        0.0633  2.3053\n",
      "   1211        0.0081        0.0440  2.3244\n",
      "   1212        0.0080        0.0697  2.3800\n",
      "   1213        0.0081        0.0503  2.2265\n",
      "   1214        0.0080        0.0607  4.2615\n",
      "   1215        0.0081        0.0547  2.3251\n",
      "   1216        0.0080        0.0386  2.4204\n",
      "   1217        0.0082        0.0476  2.3352\n",
      "   1218        0.0080        0.0525  6.9495\n",
      "   1219        0.0079        0.0562  2.5353\n",
      "   1220        0.0080        0.0561  1.8880\n",
      "   1231        0.0083        0.0475  2.4169\n",
      "   1232        0.0081        0.0542  2.6327\n",
      "   1233        0.0081        0.0569  2.4760\n",
      "   1234        \u001b[36m0.0079\u001b[0m        0.0537  2.1745\n",
      "   1235        \u001b[36m0.0079\u001b[0m        0.0605  2.3440\n",
      "   1236        \u001b[36m0.0079\u001b[0m        0.0568  2.2564\n",
      "   1237        \u001b[36m0.0079\u001b[0m        0.0577  2.2868\n",
      "   1238        0.0080        0.0581  2.2320\n",
      "   1239        0.0080        0.0466  2.3880\n",
      "   1240        0.0080        0.0594  2.9672\n",
      "   1241        0.0080        0.0387  2.2040\n",
      "   1242        0.0080        0.0576  1.4461\n",
      "   1243        0.0080        0.0512  1.8952\n",
      "   1244        0.0080        0.0631  1.5897\n",
      "   1245        0.0080        0.0524  2.2892\n",
      "   1246        0.0080        0.0467  2.8349\n",
      "   1247        0.0081        0.0502  1.8212\n",
      "   1248        0.0080        0.0449  2.0390\n",
      "   1249        0.0081        0.0520  2.0051\n",
      "   1250        0.0080        0.0455  2.1318\n",
      "   1251        0.0080        0.0586  2.0296\n",
      "   1252        0.0080        0.0454  2.4526\n",
      "   1253        0.0080        0.0677  2.0789\n",
      "   1254        0.0080        0.0490  2.6026\n",
      "   1255        0.0080        0.0587  2.1002\n",
      "   1256        0.0081        0.0477  2.8244\n",
      "   1257        0.0080        0.0415  2.7534\n",
      "   1258        0.0083        0.0464  2.8123\n",
      "   1259        0.0080        0.0562  2.3894\n",
      "   1260        0.0080        0.0560  2.4913\n",
      "   1261        \u001b[36m0.0079\u001b[0m        0.0562  2.3170\n",
      "   1262        \u001b[36m0.0079\u001b[0m        0.0591  2.3489\n",
      "   1263        0.0079        0.0582  2.4780\n",
      "   1264        \u001b[36m0.0079\u001b[0m        0.0567  2.3150\n",
      "   1265        0.0079        0.0593  2.2806\n",
      "   1266        0.0079        0.0504  1.8765\n",
      "   1267        0.0079        0.0628  4.5258\n",
      "   1268        0.0080        0.0441  2.3890\n",
      "   1269        0.0080        0.0700  2.7557\n",
      "   1270        0.0080        0.0450  2.8740\n",
      "   1271        0.0080        0.0773  2.5568\n",
      "   1272        0.0081        0.0503  2.3090\n",
      "   1273        0.0080        0.0510  2.4693\n",
      "   1274        0.0082        0.0508  2.7595\n",
      "   1275        0.0080        0.0449  2.7048\n",
      "   1276        0.0081        0.0539  2.5695\n",
      "   1277        0.0079        0.0486  2.5309\n",
      "   1278        0.0079        0.0632  2.2988\n",
      "   1279        0.0079        0.0492  2.7537\n",
      "   1280        0.0079        0.0670  2.4760\n",
      "   1281        0.0079        0.0532  2.3585\n",
      "   1282        0.0079        0.0638  2.4503\n",
      "   1283        0.0080        0.0540  2.4053\n",
      "   1284        0.0079        0.0472  3.5612\n",
      "   1285        0.0080        0.0548  2.6489\n",
      "   1286        0.0080        0.0389  1.8522\n",
      "   1287        0.0080        0.0513  2.1946\n",
      "   1288        0.0079        0.0509  2.1424\n",
      "   1289        0.0079        0.0618  2.3172\n",
      "   1290        0.0079        0.0518  2.4430\n",
      "   1291        0.0079        0.0626  2.2203\n",
      "   1292        0.0080        0.0499  3.8867\n",
      "   1293        0.0079        0.0491  2.1155\n",
      "   1294        0.0080        0.0486  1.9326\n",
      "   1295        0.0080        0.0416  1.9247\n",
      "   1296        0.0081        0.0453  2.3185\n",
      "   1297        0.0079        0.0534  2.3790\n",
      "   1298        0.0080        0.0550  2.3467\n",
      "   1299        0.0079        0.0484  2.5192\n",
      "   1300        0.0079        0.0618  2.1512\n",
      "   1301        0.0079        0.0465  2.0423\n",
      "   1302        0.0079        0.0688  2.6890\n",
      "   1303        0.0080        0.0481  2.5362\n",
      "   1304        0.0079        0.0550  2.3406\n",
      "   1305        0.0080        0.0474  2.7763\n",
      "   1306        0.0080        0.0400  2.5493\n",
      "   1307        0.0082        0.0441  2.1516\n",
      "   1308        0.0079        0.0563  2.5770\n",
      "   1309        0.0079        0.0552  2.3015\n",
      "   1310        \u001b[36m0.0078\u001b[0m        0.0562  2.4361\n",
      "   1311        \u001b[36m0.0078\u001b[0m        0.0585  2.1397\n",
      "   1312        0.0079        0.0567  1.6442\n",
      "   1313        0.0079        0.0579  5.6258\n",
      "   1314        0.0079        0.0574  1.8637\n",
      "   1315        0.0079        0.0544  2.5424\n",
      "   1316        0.0079        0.0604  3.3222\n",
      "   1317        0.0079        0.0537  2.3037\n",
      "   1318        0.0079        0.0658  3.3759\n",
      "   1319        0.0079        0.0490  2.5454\n",
      "   1320        0.0079        0.0532  2.3688\n",
      "   1321        0.0080        0.0474  2.2797\n",
      "   1322        0.0080        0.0373  2.2090\n",
      "   1323        0.0082        0.0393  2.6373\n",
      "   1324        0.0080        0.0628  3.9646\n",
      "   1325        0.0080        0.0497  2.3719\n",
      "   1326        0.0079        0.0610  2.1964\n",
      "   1327        0.0078        0.0507  1.8807\n",
      "   1328        0.0078        0.0604  2.4453\n",
      "   1329        0.0079        0.0479  3.2976\n",
      "   1330        0.0079        0.0634  3.9943\n",
      "   1331        0.0079        0.0460  2.7884\n",
      "   1332        0.0079        0.0694  2.7460\n",
      "   1333        0.0079        0.0482  3.3537\n",
      "   1334        0.0079        0.0667  3.0767\n",
      "   1335        0.0080        0.0432  1.8279\n",
      "   1336        0.0079        0.0484  5.6886\n",
      "   1337        0.0080        0.0509  2.8496\n",
      "   1338        0.0079        0.0380  2.4212\n",
      "   1339        0.0080        0.0447  2.2916\n",
      "   1340        0.0079        0.0521  2.4919\n",
      "   1341        \u001b[36m0.0078\u001b[0m        0.0570  1.8165\n",
      "   1342        0.0078        0.0511  2.3991\n",
      "   1343        \u001b[36m0.0078\u001b[0m        0.0609  2.6502\n",
      "   1344        0.0078        0.0514  1.9028\n",
      "   1345        0.0078        0.0599  2.4668\n",
      "   1346        0.0079        0.0492  2.9283\n",
      "   1347        0.0079        0.0535  2.3989\n",
      "   1348        0.0079        0.0526  2.3177\n",
      "   1349        0.0079        0.0423  2.2887\n",
      "   1350        0.0079        0.0605  4.0403\n",
      "   1351        0.0080        0.0304  2.9484\n",
      "   1352        0.0080        0.0422  1.6052\n",
      "   1353        0.0079        0.0489  2.3873\n",
      "   1354        0.0079        0.0467  2.2516\n",
      "   1355        0.0080        0.0417  2.2480\n",
      "   1356        0.0079        0.0431  2.4392\n",
      "   1357        0.0080        0.0423  2.7142\n",
      "   1358        0.0079        0.0451  2.8777\n",
      "   1359        0.0079        0.0476  2.2382\n",
      "   1360        0.0079        0.0399  1.7505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1361        0.0079        0.0489  1.7073\n",
      "   1362        0.0079        0.0390  1.7649\n",
      "   1363        0.0078        0.0515  2.0948\n",
      "   1364        0.0079        0.0376  2.1378\n",
      "   1365        0.0078        0.0533  1.8433\n",
      "   1366        0.0079        0.0404  1.8692\n",
      "   1367        0.0078        0.0616  1.7104\n",
      "   1368        0.0079        0.0423  2.7982\n",
      "   1369        0.0078        0.0664  2.5322\n",
      "   1370        0.0080        0.0387  2.3394\n",
      "   1371        0.0079        0.0503  2.6618\n",
      "   1372        0.0080        0.0473  2.1228\n",
      "   1373        0.0079        0.0374  2.1592\n",
      "   1374        0.0081        0.0408  1.8787\n",
      "   1375        0.0078        0.0515  1.8495\n",
      "   1376        \u001b[36m0.0078\u001b[0m        0.0520  2.6013\n",
      "   1377        \u001b[36m0.0078\u001b[0m        0.0510  2.3098\n",
      "   1378        \u001b[36m0.0078\u001b[0m        0.0541  5.0843\n",
      "   1379        0.0078        0.0503  2.6582\n",
      "   1380        0.0078        0.0562  2.4178\n",
      "   1381        0.0078        0.0504  2.3086\n",
      "   1382        0.0078        0.0582  2.3536\n",
      "   1383        0.0078        0.0497  2.3937\n",
      "   1384        0.0078        0.0568  1.8014\n",
      "   1385        0.0078        0.0493  1.7135\n",
      "   1386        0.0079        0.0598  1.6152\n",
      "   1387        0.0078        0.0515  1.9244\n",
      "   1388        0.0079        0.0630  2.5549\n",
      "   1389        0.0078        0.0438  2.8038\n",
      "   1390        0.0079        0.0606  1.6098\n",
      "   1391        0.0079        0.0320  1.5331\n",
      "   1392        0.0079        0.0410  1.4693\n",
      "   1393        0.0079        0.0517  2.3130\n",
      "   1394        0.0078        0.0521  2.5592\n",
      "   1395        0.0079        0.0421  2.3999\n",
      "   1396        0.0078        0.0467  3.0038\n",
      "   1397        0.0079        0.0417  2.8332\n",
      "   1398        0.0079        0.0387  1.7981\n",
      "   1399        0.0079        0.0390  2.1472\n",
      "   1400        0.0078        0.0444  1.8578\n",
      "   1401        0.0078        0.0473  1.8309\n",
      "   1402        0.0078        0.0374  1.7960\n",
      "   1403        0.0078        0.0458  1.7202\n",
      "   1404        0.0078        0.0359  2.5407\n",
      "   1405        0.0078        0.0439  2.5166\n",
      "   1406        0.0078        0.0369  2.6809\n",
      "   1407        0.0078        0.0468  2.5886\n",
      "   1408        0.0078        0.0339  2.3227\n",
      "   1409        0.0078        0.0478  2.4775\n",
      "   1410        0.0078        0.0373  2.1456\n",
      "   1411        0.0078        0.0567  2.8558\n",
      "   1412        0.0079        0.0374  2.0220\n",
      "   1413        0.0078        0.0647  2.3537\n",
      "   1414        0.0079        0.0368  3.0011\n",
      "   1415        0.0079        0.0530  4.0447\n",
      "   1416        0.0079        0.0405  1.5808\n",
      "   1417        0.0079        0.0387  1.4459\n",
      "   1418        0.0080        0.0402  2.0714\n",
      "   1419        0.0078        0.0456  2.4697\n",
      "   1420        0.0078        0.0498  2.7572\n",
      "   1421        0.0078        0.0395  2.5109\n",
      "   1422        \u001b[36m0.0077\u001b[0m        0.0497  2.9237\n",
      "   1423        0.0078        0.0344  2.6038\n",
      "   1424        0.0078        0.0429  3.3879\n",
      "   1425        0.0078        0.0410  2.4352\n",
      "   1426        0.0078        0.0484  2.8541\n",
      "   1427        0.0078        0.0353  3.7367\n",
      "   1428        0.0078        0.0511  3.1705\n",
      "   1429        0.0078        0.0357  5.1667\n",
      "   1430        0.0078        0.0535  4.5348\n",
      "   1431        0.0079        0.0388  2.4150\n",
      "   1432        0.0078        0.0578  3.2513\n",
      "   1433        0.0079        0.0413  2.0442\n",
      "   1434        0.0078        0.0533  2.4938\n",
      "   1435        0.0079        0.0357  2.0942\n",
      "   1436        0.0079        0.0446  2.6103\n",
      "   1437        0.0079        0.0450  2.5948\n",
      "   1438        0.0078        0.0339  1.8064\n",
      "   1439        0.0079        0.0384  2.1823\n",
      "   1440        \u001b[36m0.0077\u001b[0m        0.0442  2.2938\n",
      "   1441        0.0077        0.0474  1.7866\n",
      "   1442        0.0078        0.0391  1.5017\n",
      "   1443        \u001b[36m0.0077\u001b[0m        0.0502  2.4736\n",
      "   1444        0.0078        0.0328  1.9090\n",
      "   1445        0.0078        0.0439  1.6713\n",
      "   1446        0.0078        0.0386  1.6877\n",
      "   1447        0.0078        0.0488  2.0316\n",
      "   1448        0.0079        0.0352  2.0783\n",
      "   1449        0.0077        0.0533  1.6455\n",
      "   1450        0.0079        0.0367  1.9398\n",
      "   1451        0.0078        0.0563  3.1966\n",
      "   1452        0.0078        0.0402  2.0606\n",
      "   1453        0.0078        0.0554  2.0301\n",
      "   1454        0.0078        0.0365  1.7364\n",
      "   1455        0.0079        0.0499  2.3333\n",
      "   1456        0.0078        0.0448  2.3838\n",
      "   1457        0.0078        0.0388  2.3894\n",
      "   1458        0.0078        0.0453  2.3935\n",
      "   1459        0.0078        0.0314  3.1509\n",
      "   1460        0.0078        0.0366  2.2562\n",
      "   1461        \u001b[36m0.0077\u001b[0m        0.0429  1.4448\n",
      "   1462        0.0077        0.0443  2.1450\n",
      "   1463        0.0078        0.0386  1.8344\n",
      "   1464        \u001b[36m0.0077\u001b[0m        0.0522  2.3852\n",
      "   1465        0.0078        0.0325  1.9822\n",
      "   1466        0.0077        0.0510  1.6087\n",
      "   1467        0.0078        0.0350  2.5037\n",
      "   1468        0.0077        0.0515  4.2047\n",
      "   1469        0.0078        0.0386  2.2176\n",
      "   1470        0.0078        0.0546  2.8373\n",
      "   1471        0.0079        0.0422  2.2723\n",
      "   1472        0.0078        0.0495  2.2798\n",
      "   1473        0.0078        0.0373  2.2922\n",
      "   1474        0.0079        0.0432  2.3280\n",
      "   1475        0.0078        0.0455  2.2319\n",
      "   1476        0.0078        0.0329  2.3540\n",
      "   1477        0.0077        0.0414  2.5686\n",
      "   1478        0.0077        0.0362  2.5259\n",
      "   1479        0.0077        0.0440  3.1537\n",
      "   1480        0.0078        0.0313  2.3162\n",
      "   1481        0.0077        0.0405  2.7807\n",
      "   1482        0.0077        0.0371  3.7290\n",
      "   1483        0.0077        0.0456  5.5673\n",
      "   1484        0.0078        0.0330  2.2891\n",
      "   1485        0.0077        0.0497  2.4007\n",
      "   1486        0.0078        0.0325  4.0878\n",
      "   1487        0.0077        0.0478  1.6645\n",
      "   1488        0.0078        0.0373  3.5348\n",
      "   1489        0.0077        0.0508  2.7006\n",
      "   1490        0.0078        0.0398  2.9245\n",
      "   1491        0.0077        0.0498  2.4260\n",
      "   1492        0.0078        0.0365  1.9844\n",
      "   1493        0.0078        0.0473  2.0080\n",
      "   1494        0.0077        0.0438  2.3833\n",
      "   1495        0.0078        0.0395  2.8339\n",
      "   1496        \u001b[36m0.0077\u001b[0m        0.0488  2.7865\n",
      "   1497        0.0078        0.0307  2.5035\n",
      "   1498        0.0077        0.0404  1.8317\n",
      "   1499        0.0077        0.0383  1.6339\n",
      "   1500        0.0077        0.0458  1.8621\n",
      "   1501        0.0079        0.0344  2.5200\n",
      "   1502        0.0077        0.0550  2.9485\n",
      "   1503        0.0078        0.0321  7.0370\n",
      "   1504        0.0077        0.0483  2.5625\n",
      "   1505        0.0077        0.0403  2.8246\n",
      "   1506        0.0077        0.0503  1.6206\n",
      "   1507        0.0078        0.0422  2.4322\n",
      "   1508        0.0077        0.0460  3.3665\n",
      "   1509        0.0077        0.0422  5.8309\n",
      "   1510        0.0078        0.0454  2.2026\n",
      "   1511        \u001b[36m0.0076\u001b[0m        0.0470  2.4002\n",
      "   1512        0.0078        0.0416  1.8368\n",
      "   1513        \u001b[36m0.0076\u001b[0m        0.0533  1.9089\n",
      "   1514        0.0078        0.0367  3.3186\n",
      "   1515        0.0077        0.0584  2.3965\n",
      "   1516        0.0077        0.0361  2.2319\n",
      "   1517        0.0077        0.0585  3.4520\n",
      "   1518        0.0077        0.0426  3.2749\n",
      "   1519        0.0078        0.0518  2.6123\n",
      "   1520        0.0077        0.0417  2.9133\n",
      "   1521        0.0078        0.0552  2.7267\n",
      "   1522        0.0077        0.0403  2.7521\n",
      "   1523        0.0076        0.0582  4.0312\n",
      "   1524        0.0077        0.0453  2.7719\n",
      "   1525        0.0077        0.0527  1.9153\n",
      "   1526        0.0076        0.0521  1.9228\n",
      "   1527        0.0078        0.0432  2.4037\n",
      "   1528        \u001b[36m0.0076\u001b[0m        0.0624  2.1462\n",
      "   1529        0.0078        0.0357  2.1890\n",
      "   1530        0.0077        0.0588  1.5694\n",
      "   1531        0.0078        0.0370  1.4388\n",
      "   1532        0.0076        0.0612  1.4613\n",
      "   1533        0.0078        0.0381  2.4734\n",
      "   1534        0.0077        0.0546  2.1009\n",
      "   1535        0.0078        0.0398  1.8805\n",
      "   1536        0.0077        0.0520  1.7505\n",
      "   1537        0.0079        0.0417  2.2061\n",
      "   1538        0.0077        0.0519  2.1927\n",
      "   1539        0.0079        0.0412  2.1387\n",
      "   1540        0.0077        0.0521  1.9514\n",
      "   1541        0.0078        0.0392  2.2267\n",
      "   1542        0.0076        0.0514  2.5155\n",
      "   1543        0.0078        0.0348  2.4292\n",
      "   1544        0.0076        0.0450  2.8465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1545        0.0077        0.0385  3.1792\n",
      "   1546        0.0077        0.0467  1.7942\n",
      "   1547        0.0078        0.0351  2.4292\n",
      "   1548        0.0076        0.0469  2.6635\n",
      "   1549        0.0078        0.0345  2.2355\n",
      "   1550        0.0076        0.0450  2.7392\n",
      "   1551        0.0077        0.0341  2.4683\n",
      "   1552        0.0077        0.0445  2.4065\n",
      "   1553        0.0078        0.0359  2.1748\n",
      "   1554        0.0076        0.0467  2.1860\n",
      "   1555        0.0078        0.0335  2.7235\n",
      "   1556        0.0076        0.0462  2.4805\n",
      "   1557        0.0077        0.0337  2.3704\n",
      "   1558        0.0076        0.0447  2.4971\n",
      "   1559        0.0077        0.0351  2.2714\n",
      "   1560        0.0076        0.0461  2.1327\n",
      "   1561        0.0078        0.0357  1.8681\n",
      "   1562        0.0076        0.0490  1.9791\n",
      "   1563        0.0077        0.0350  2.3571\n",
      "   1564        0.0076        0.0502  2.8428\n",
      "   1565        0.0077        0.0356  2.2331\n",
      "   1566        0.0076        0.0506  2.1220\n",
      "   1567        0.0077        0.0387  3.3440\n",
      "   1568        0.0077        0.0518  2.5471\n",
      "   1569        0.0077        0.0414  2.2874\n",
      "   1570        0.0077        0.0511  2.8128\n",
      "   1571        0.0076        0.0421  2.4610\n",
      "   1572        0.0076        0.0483  2.7041\n",
      "   1573        0.0076        0.0451  2.3945\n",
      "   1574        0.0078        0.0497  2.1420\n",
      "   1575        0.0076        0.0402  2.3067\n",
      "   1576        \u001b[36m0.0075\u001b[0m        0.0537  2.3710\n",
      "   1577        0.0076        0.0439  2.7374\n",
      "   1578        0.0078        0.0537  3.2174\n",
      "   1579        0.0076        0.0395  3.4246\n",
      "   1580        0.0076        0.0579  2.2909\n",
      "   1581        0.0077        0.0434  2.4709\n",
      "   1582        0.0077        0.0558  2.2117\n",
      "   1583        0.0076        0.0486  2.3683\n",
      "   1584        0.0077        0.0464  1.7571\n",
      "   1585        0.0075        0.0566  1.5415\n",
      "   1586        0.0077        0.0427  1.5114\n",
      "   1587        0.0076        0.0598  2.4334\n",
      "   1588        0.0077        0.0377  2.8990\n",
      "   1589        0.0076        0.0585  2.4041\n",
      "   1590        0.0077        0.0420  1.8363\n",
      "   1591        0.0076        0.0595  2.9383\n",
      "   1592        0.0077        0.0464  2.2516\n",
      "   1593        0.0077        0.0554  7.6038\n",
      "   1594        0.0076        0.0509  7.3345\n",
      "   1595        0.0077        0.0487  4.4242\n",
      "   1596        \u001b[36m0.0075\u001b[0m        0.0545  1.8917\n",
      "   1597        0.0077        0.0445  2.1679\n",
      "   1598        0.0076        0.0590  2.8753\n",
      "   1599        0.0077        0.0445  1.9847\n",
      "   1600        0.0077        0.0582  2.2231\n",
      "   1601        0.0077        0.0500  2.4120\n",
      "   1602        0.0077        0.0504  1.8595\n",
      "   1603        0.0078        0.0497  5.5114\n",
      "   1604        0.0077        0.0506  2.9907\n",
      "   1605        0.0077        0.0456  1.8950\n",
      "   1606        \u001b[36m0.0074\u001b[0m        0.0560  3.0166\n",
      "   1607        0.0076        0.0419  2.7493\n",
      "   1608        0.0075        0.0599  1.9037\n",
      "   1609        0.0076        0.0420  1.9329\n",
      "   1610        0.0076        0.0581  1.8160\n",
      "   1611        0.0076        0.0459  1.8219\n",
      "   1612        0.0076        0.0568  1.6849\n",
      "   1613        0.0076        0.0506  2.1058\n",
      "   1614        0.0077        0.0551  2.6015\n",
      "   1615        0.0076        0.0516  2.2714\n",
      "   1616        0.0076        0.0463  2.7404\n",
      "   1617        0.0075        0.0589  1.5686\n",
      "   1618        0.0077        0.0411  1.4483\n",
      "   1619        0.0076        0.0603  1.4710\n",
      "   1620        0.0077        0.0420  1.7092\n",
      "   1621        0.0075        0.0603  1.8578\n",
      "   1622        0.0076        0.0455  2.4242\n",
      "   1623        0.0076        0.0570  2.0894\n",
      "   1624        0.0076        0.0507  1.8553\n",
      "   1625        0.0076        0.0546  1.6436\n",
      "   1626        0.0075        0.0525  2.5026\n",
      "   1627        0.0076        0.0498  2.5244\n",
      "   1628        0.0075        0.0542  2.3676\n",
      "   1629        0.0076        0.0463  1.9086\n",
      "   1630        0.0075        0.0600  1.8367\n",
      "   1631        0.0077        0.0439  2.0682\n",
      "   1632        0.0077        0.0571  2.3596\n",
      "   1633        0.0077        0.0483  3.0965\n",
      "   1634        0.0075        0.0471  2.6489\n",
      "   1635        0.0074        0.0579  2.3699\n",
      "   1636        0.0077        0.0464  2.2950\n",
      "   1637        0.0076        0.0569  3.0238\n",
      "   1638        0.0077        0.0476  4.2671\n",
      "   1639        0.0075        0.0467  2.0998\n",
      "   1640        \u001b[36m0.0074\u001b[0m        0.0577  2.0908\n",
      "   1641        0.0076        0.0468  1.8394\n",
      "   1642        0.0076        0.0582  3.0314\n",
      "   1643        0.0076        0.0508  2.3965\n",
      "   1644        0.0076        0.0536  2.2567\n",
      "   1645        0.0075        0.0467  2.2131\n",
      "   1646        0.0074        0.0571  2.3502\n",
      "   1647        0.0076        0.0445  2.4817\n",
      "   1648        0.0075        0.0582  2.5151\n",
      "   1649        0.0076        0.0468  2.1140\n",
      "   1650        0.0076        0.0572  4.1018\n",
      "   1651        0.0076        0.0493  2.3593\n",
      "   1652        0.0075        0.0509  2.4190\n",
      "   1653        0.0075        0.0517  2.1887\n"
     ]
    }
   ],
   "source": [
    "lstm_moro.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = datasets['^GSPC'][\"target\"]\n",
    "ext_dataset = datasets['^GSPC'][\"features\"]['ext']\n",
    "ext_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "# sa = load('ext_encoder_better')\n",
    "ext_train, ext_val = ext_scaler.fit_transform(ext_dataset.iloc[train_dates].to_numpy(np.float32)), ext_scaler.transform(ext_dataset.iloc[val_dates].to_numpy(np.float32))\n",
    "y_train, y_val = y_scaler.fit_transform(y.iloc[train_dates].to_numpy(np.float32)[...,None]), y_scaler.transform(y.iloc[val_dates].to_numpy(np.float32)[...,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ext_encoder = get_encoder(ext_train, ext_val, sa_hidden_size=10)\n",
    "# save(ext_encoder, 'ext_encoder_better')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ext_encoder = load('ext_encoder_better')\n",
    "# ext_sa_train, ext_sa_val = encode(ext_train, ext_encoder), encode(ext_val, ext_encoder)\n",
    "# ext_sa = np.concatenate((ext_sa_train, ext_sa_val))\n",
    "# y_data = np.concatenate((y_train, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #modello del paper che usa stacked autoencoders usando LSTM paper Moro\n",
    "# n_days = 5\n",
    "\n",
    "# batch_size = 20\n",
    "# #TODO test e trova numero epoche\n",
    "\n",
    "# lstm_sa = NeuralNetRegressor(\n",
    "#     module=SequenceDouble,\n",
    "#     optimizer=optim.Adam,\n",
    "#     batch_size=batch_size,\n",
    "#     max_epochs=350, # trovato empiricamente\n",
    "#     train_split=tss_split,\n",
    "#     callbacks=[\n",
    "#         callbacks.EpochScoring('neg_mean_absolute_error', lower_is_better=False),\n",
    "#         callbacks.Checkpoint(monitor='valid_loss_best', f_pickle='lstm_sa_best'),\n",
    "        \n",
    "#     ],\n",
    "    \n",
    "#     module__nb_features=ext_sa_train.shape[1],\n",
    "#     module__hidden_size=256,\n",
    "#     optimizer__lr=0.0001,\n",
    "# #     optimizer__weight_decay=0,\n",
    "# #     optimizer__momentum=0.9,\n",
    "#     iterator_train__shuffle = True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = days_group(ext_sa, n_days=n_days)\n",
    "y = y_data[n_days:]\n",
    "l1 = len(np.split(x, [len(ext_sa_train)])[0])\n",
    "l2 = len(np.split(x, [len(ext_sa_train)])[1])\n",
    "half = PredefinedSplit(np.concatenate((np.ones(l1)*-1,np.ones(l2))))\n",
    "half_split =  CVSplit(cv=half, stratified=False, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_sa.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm = lstm_sa\n",
    "# valid_losses = lstm.history[:, 'valid_loss']\n",
    "# train_losses = lstm.history[:, 'train_loss']\n",
    "# plt.figure(figsize=(12,7))\n",
    "# plt.plot(valid_losses, label='valid_loss')\n",
    "# plt.plot(train_losses, label='train_loss')\n",
    "# plt.xticks(np.arange(len(valid_losses)+1, step=50))\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(lstm_sa, 'lstm_sa_5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['explained_variance', 'r2', 'max_error', 'neg_median_absolute_error', 'neg_mean_absolute_error', 'neg_mean_squared_error', 'neg_mean_squared_log_error', 'neg_root_mean_squared_error', 'neg_mean_poisson_deviance', 'neg_mean_gamma_deviance', 'accuracy', 'roc_auc', 'roc_auc_ovr', 'roc_auc_ovo', 'roc_auc_ovr_weighted', 'roc_auc_ovo_weighted', 'balanced_accuracy', 'average_precision', 'neg_log_loss', 'neg_brier_score', 'adjusted_rand_score', 'homogeneity_score', 'completeness_score', 'v_measure_score', 'mutual_info_score', 'adjusted_mutual_info_score', 'normalized_mutual_info_score', 'fowlkes_mallows_score', 'precision', 'precision_macro', 'precision_micro', 'precision_samples', 'precision_weighted', 'recall', 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'f1', 'f1_macro', 'f1_micro', 'f1_samples', 'f1_weighted', 'jaccard', 'jaccard_macro', 'jaccard_micro', 'jaccard_samples', 'jaccard_weighted'])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn.metrics.SCORERS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = datasets['^GSPC'][\"target\"]\n",
    "ohlcv_dataset = datasets['^GSPC'][\"features\"]['ohlcv']\n",
    "ohlcv_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "# sa = load('ext_encoder_better')\n",
    "ohlcv_train, ohlcv_val = ohlcv_scaler.fit_transform(ohlcv_dataset.iloc[train_dates].to_numpy(np.float32)), ohlcv_scaler.transform(ohlcv_dataset.iloc[val_dates].to_numpy(np.float32))\n",
    "y_train, y_val = y_scaler.fit_transform(y.iloc[train_dates].to_numpy(np.float32)[...,None]), y_scaler.transform(y.iloc[val_dates].to_numpy(np.float32)[...,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohlcv = np.concatenate((ohlcv_train, ohlcv_val))\n",
    "y_data = np.concatenate((y_train, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = days_group(ohlcv, n_days=n_days)\n",
    "y = y_data[n_days:]\n",
    "l1 = len(np.split(x, [(len(ohlcv)*2)//3])[0])\n",
    "l2 = len(np.split(x, [(len(ohlcv)*2)//3])[1])\n",
    "half = PredefinedSplit(np.concatenate((np.ones(l1)*-1,np.ones(l2))))\n",
    "half_split =  CVSplit(cv=half, stratified=False, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modello del paper che usa attention mechanism\n",
    "#TODO testa con nuova lr\n",
    "#Adam lr=0.0001, 0.05 loss, 80% accuracy, amtcha il paper\n",
    "#SGD lr = 0.0001, \n",
    "n_days = 5\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "lstm_att = NeuralNetRegressor(\n",
    "    module=SequenceDoubleAtt,\n",
    "    optimizer=optim.SGD,\n",
    "    batch_size=batch_size,\n",
    "    max_epochs=250, # trovato empiricamente\n",
    "    train_split=half_split,\n",
    "    callbacks=[\n",
    "        callbacks.EpochScoring('neg_mean_absolute_error', lower_is_better=False),\n",
    "        callbacks.EpochScoring('r2', lower_is_better=False),\n",
    "        callbacks.Checkpoint(monitor='valid_loss_best', f_pickle='lstm_sa_best')        \n",
    "    ],\n",
    "    \n",
    "    module__nb_features=ohlcv_train.shape[1],\n",
    "    module__hidden_size=256,\n",
    "#     module__nb_layers= 5,\n",
    "    optimizer__lr=0.0001, #TODO prova con questa lr\n",
    "#     optimizer__weight_decay=0,\n",
    "#     optimizer__momentum=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    neg_mean_absolute_error      r2    train_loss    valid_loss    cp     dur\n",
      "-------  -------------------------  ------  ------------  ------------  ----  ------\n",
      "      1                    \u001b[36m-1.8136\u001b[0m  \u001b[32m0.0412\u001b[0m        \u001b[35m1.3316\u001b[0m        \u001b[31m4.0114\u001b[0m     +  2.8761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2                    \u001b[36m-1.3799\u001b[0m  \u001b[32m0.3827\u001b[0m        \u001b[35m0.5683\u001b[0m        \u001b[31m2.5828\u001b[0m     +  2.0920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3                    \u001b[36m-1.2893\u001b[0m  \u001b[32m0.4352\u001b[0m        \u001b[35m0.3533\u001b[0m        \u001b[31m2.3629\u001b[0m     +  4.5807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4                    \u001b[36m-1.0899\u001b[0m  \u001b[32m0.6256\u001b[0m        \u001b[35m0.2617\u001b[0m        \u001b[31m1.5663\u001b[0m     +  3.3596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5                    \u001b[36m-0.8004\u001b[0m  \u001b[32m0.7901\u001b[0m        \u001b[35m0.1742\u001b[0m        \u001b[31m0.8782\u001b[0m     +  2.3338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6                    \u001b[36m-0.6082\u001b[0m  \u001b[32m0.8679\u001b[0m        \u001b[35m0.1202\u001b[0m        \u001b[31m0.5528\u001b[0m     +  3.7345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7                    \u001b[36m-0.5157\u001b[0m  \u001b[32m0.9007\u001b[0m        \u001b[35m0.0920\u001b[0m        \u001b[31m0.4157\u001b[0m     +  1.9537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      8                    \u001b[36m-0.4948\u001b[0m  \u001b[32m0.9126\u001b[0m        \u001b[35m0.0788\u001b[0m        \u001b[31m0.3655\u001b[0m     +  2.1824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      9                    -0.5000  \u001b[32m0.9162\u001b[0m        \u001b[35m0.0726\u001b[0m        \u001b[31m0.3505\u001b[0m     +  2.2169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10                    -0.5046  0.9150        \u001b[35m0.0676\u001b[0m        0.3555        2.1831\n",
      "     11                    -0.5065  0.9115        \u001b[35m0.0623\u001b[0m        0.3702        2.1370\n",
      "     12                    -0.5133  0.9073        \u001b[35m0.0564\u001b[0m        0.3877        1.8779\n",
      "     13                    -0.5194  0.9045        \u001b[35m0.0511\u001b[0m        0.3994        2.1530\n",
      "     14                    -0.5187  0.9045        \u001b[35m0.0464\u001b[0m        0.3994        1.8991\n",
      "     15                    -0.5104  0.9078        \u001b[35m0.0424\u001b[0m        0.3857        1.8742\n",
      "     16                    \u001b[36m-0.4942\u001b[0m  0.9140        \u001b[35m0.0391\u001b[0m        0.3600        1.4796\n",
      "     17                    \u001b[36m-0.4715\u001b[0m  \u001b[32m0.9220\u001b[0m        \u001b[35m0.0364\u001b[0m        \u001b[31m0.3263\u001b[0m     +  1.5446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     18                    \u001b[36m-0.4450\u001b[0m  \u001b[32m0.9309\u001b[0m        \u001b[35m0.0342\u001b[0m        \u001b[31m0.2892\u001b[0m     +  3.5576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     19                    \u001b[36m-0.4171\u001b[0m  \u001b[32m0.9396\u001b[0m        \u001b[35m0.0326\u001b[0m        \u001b[31m0.2526\u001b[0m     +  3.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     20                    \u001b[36m-0.3892\u001b[0m  \u001b[32m0.9475\u001b[0m        \u001b[35m0.0313\u001b[0m        \u001b[31m0.2196\u001b[0m     +  2.7937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     21                    \u001b[36m-0.3623\u001b[0m  \u001b[32m0.9542\u001b[0m        \u001b[35m0.0303\u001b[0m        \u001b[31m0.1918\u001b[0m     +  1.5216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     22                    \u001b[36m-0.3388\u001b[0m  \u001b[32m0.9595\u001b[0m        \u001b[35m0.0296\u001b[0m        \u001b[31m0.1696\u001b[0m     +  2.7589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     23                    \u001b[36m-0.3196\u001b[0m  \u001b[32m0.9635\u001b[0m        \u001b[35m0.0292\u001b[0m        \u001b[31m0.1529\u001b[0m     +  2.6685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     24                    \u001b[36m-0.3047\u001b[0m  \u001b[32m0.9663\u001b[0m        \u001b[35m0.0288\u001b[0m        \u001b[31m0.1410\u001b[0m     +  2.8253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     25                    \u001b[36m-0.2929\u001b[0m  \u001b[32m0.9682\u001b[0m        \u001b[35m0.0286\u001b[0m        \u001b[31m0.1329\u001b[0m     +  2.7304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     26                    \u001b[36m-0.2841\u001b[0m  \u001b[32m0.9695\u001b[0m        \u001b[35m0.0284\u001b[0m        \u001b[31m0.1278\u001b[0m     +  2.6045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     27                    \u001b[36m-0.2780\u001b[0m  \u001b[32m0.9702\u001b[0m        \u001b[35m0.0283\u001b[0m        \u001b[31m0.1248\u001b[0m     +  2.8354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     28                    \u001b[36m-0.2738\u001b[0m  \u001b[32m0.9705\u001b[0m        \u001b[35m0.0281\u001b[0m        \u001b[31m0.1233\u001b[0m     +  2.4551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     29                    \u001b[36m-0.2714\u001b[0m  \u001b[32m0.9707\u001b[0m        \u001b[35m0.0280\u001b[0m        \u001b[31m0.1228\u001b[0m     +  3.0589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     30                    \u001b[36m-0.2698\u001b[0m  0.9706        \u001b[35m0.0278\u001b[0m        0.1228        4.7593\n",
      "     31                    \u001b[36m-0.2689\u001b[0m  0.9705        \u001b[35m0.0275\u001b[0m        0.1233        2.2959\n",
      "     32                    \u001b[36m-0.2684\u001b[0m  0.9704        \u001b[35m0.0272\u001b[0m        0.1240        2.3408\n",
      "     33                    \u001b[36m-0.2683\u001b[0m  0.9701        \u001b[35m0.0269\u001b[0m        0.1250        2.7325\n",
      "     34                    -0.2684  0.9699        \u001b[35m0.0266\u001b[0m        0.1261        2.2905\n",
      "     35                    -0.2685  0.9696        \u001b[35m0.0262\u001b[0m        0.1273        2.7350\n",
      "     36                    -0.2687  0.9693        \u001b[35m0.0259\u001b[0m        0.1284        2.0394\n",
      "     37                    -0.2689  0.9690        \u001b[35m0.0256\u001b[0m        0.1296        2.3952\n",
      "     38                    -0.2691  0.9688        \u001b[35m0.0252\u001b[0m        0.1307        2.4025\n",
      "     39                    -0.2692  0.9686        \u001b[35m0.0249\u001b[0m        0.1316        2.4258\n",
      "     40                    -0.2692  0.9684        \u001b[35m0.0246\u001b[0m        0.1323        2.8290\n",
      "     41                    -0.2690  0.9682        \u001b[35m0.0244\u001b[0m        0.1328        2.2342\n",
      "     42                    -0.2686  0.9682        \u001b[35m0.0241\u001b[0m        0.1331        1.9487\n",
      "     43                    \u001b[36m-0.2679\u001b[0m  0.9682        \u001b[35m0.0239\u001b[0m        0.1331        2.2625\n",
      "     44                    \u001b[36m-0.2670\u001b[0m  0.9683        \u001b[35m0.0236\u001b[0m        0.1328        2.6451\n",
      "     45                    \u001b[36m-0.2658\u001b[0m  0.9684        \u001b[35m0.0234\u001b[0m        0.1322        2.9342\n",
      "     46                    \u001b[36m-0.2644\u001b[0m  0.9686        \u001b[35m0.0232\u001b[0m        0.1313        2.5676\n",
      "     47                    \u001b[36m-0.2627\u001b[0m  0.9689        \u001b[35m0.0230\u001b[0m        0.1302        2.7004\n",
      "     48                    \u001b[36m-0.2609\u001b[0m  0.9692        \u001b[35m0.0228\u001b[0m        0.1288        1.7810\n",
      "     49                    \u001b[36m-0.2589\u001b[0m  0.9696        \u001b[35m0.0226\u001b[0m        0.1272        1.5042\n",
      "     50                    \u001b[36m-0.2566\u001b[0m  0.9700        \u001b[35m0.0225\u001b[0m        0.1254        1.7937\n",
      "     51                    \u001b[36m-0.2543\u001b[0m  0.9705        \u001b[35m0.0223\u001b[0m        0.1234        1.6360\n",
      "     52                    \u001b[36m-0.2518\u001b[0m  \u001b[32m0.9710\u001b[0m        \u001b[35m0.0221\u001b[0m        \u001b[31m0.1213\u001b[0m     +  1.4992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     53                    \u001b[36m-0.2492\u001b[0m  \u001b[32m0.9715\u001b[0m        \u001b[35m0.0220\u001b[0m        \u001b[31m0.1191\u001b[0m     +  2.7870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     54                    \u001b[36m-0.2465\u001b[0m  \u001b[32m0.9721\u001b[0m        \u001b[35m0.0219\u001b[0m        \u001b[31m0.1168\u001b[0m     +  2.5675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     55                    \u001b[36m-0.2437\u001b[0m  \u001b[32m0.9727\u001b[0m        \u001b[35m0.0217\u001b[0m        \u001b[31m0.1144\u001b[0m     +  2.3699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     56                    \u001b[36m-0.2409\u001b[0m  \u001b[32m0.9732\u001b[0m        \u001b[35m0.0216\u001b[0m        \u001b[31m0.1119\u001b[0m     +  2.5760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     57                    \u001b[36m-0.2381\u001b[0m  \u001b[32m0.9738\u001b[0m        \u001b[35m0.0215\u001b[0m        \u001b[31m0.1095\u001b[0m     +  2.4523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     58                    \u001b[36m-0.2352\u001b[0m  \u001b[32m0.9744\u001b[0m        \u001b[35m0.0213\u001b[0m        \u001b[31m0.1070\u001b[0m     +  3.8906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     59                    \u001b[36m-0.2323\u001b[0m  \u001b[32m0.9750\u001b[0m        \u001b[35m0.0212\u001b[0m        \u001b[31m0.1045\u001b[0m     +  2.3960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     60                    \u001b[36m-0.2294\u001b[0m  \u001b[32m0.9756\u001b[0m        \u001b[35m0.0211\u001b[0m        \u001b[31m0.1020\u001b[0m     +  2.3148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     61                    \u001b[36m-0.2265\u001b[0m  \u001b[32m0.9762\u001b[0m        \u001b[35m0.0210\u001b[0m        \u001b[31m0.0996\u001b[0m     +  3.6068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     62                    \u001b[36m-0.2237\u001b[0m  \u001b[32m0.9768\u001b[0m        \u001b[35m0.0209\u001b[0m        \u001b[31m0.0972\u001b[0m     +  4.0844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     63                    \u001b[36m-0.2211\u001b[0m  \u001b[32m0.9773\u001b[0m        \u001b[35m0.0208\u001b[0m        \u001b[31m0.0949\u001b[0m     +  1.8286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     64                    \u001b[36m-0.2185\u001b[0m  \u001b[32m0.9779\u001b[0m        \u001b[35m0.0208\u001b[0m        \u001b[31m0.0926\u001b[0m     +  2.5674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     65                    \u001b[36m-0.2160\u001b[0m  \u001b[32m0.9784\u001b[0m        \u001b[35m0.0207\u001b[0m        \u001b[31m0.0905\u001b[0m     +  2.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     66                    \u001b[36m-0.2136\u001b[0m  \u001b[32m0.9789\u001b[0m        \u001b[35m0.0206\u001b[0m        \u001b[31m0.0884\u001b[0m     +  2.0443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     67                    \u001b[36m-0.2112\u001b[0m  \u001b[32m0.9794\u001b[0m        \u001b[35m0.0205\u001b[0m        \u001b[31m0.0864\u001b[0m     +  2.1887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     68                    \u001b[36m-0.2091\u001b[0m  \u001b[32m0.9798\u001b[0m        \u001b[35m0.0204\u001b[0m        \u001b[31m0.0845\u001b[0m     +  2.2804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     69                    \u001b[36m-0.2070\u001b[0m  \u001b[32m0.9802\u001b[0m        \u001b[35m0.0204\u001b[0m        \u001b[31m0.0827\u001b[0m     +  1.8109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     70                    \u001b[36m-0.2051\u001b[0m  \u001b[32m0.9806\u001b[0m        \u001b[35m0.0203\u001b[0m        \u001b[31m0.0810\u001b[0m     +  2.8029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     71                    \u001b[36m-0.2034\u001b[0m  \u001b[32m0.9810\u001b[0m        \u001b[35m0.0202\u001b[0m        \u001b[31m0.0795\u001b[0m     +  2.9775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     72                    \u001b[36m-0.2019\u001b[0m  \u001b[32m0.9813\u001b[0m        \u001b[35m0.0202\u001b[0m        \u001b[31m0.0781\u001b[0m     +  2.9720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     73                    \u001b[36m-0.2006\u001b[0m  \u001b[32m0.9816\u001b[0m        \u001b[35m0.0201\u001b[0m        \u001b[31m0.0768\u001b[0m     +  2.6089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     74                    \u001b[36m-0.1995\u001b[0m  \u001b[32m0.9819\u001b[0m        \u001b[35m0.0200\u001b[0m        \u001b[31m0.0756\u001b[0m     +  2.0376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     75                    \u001b[36m-0.1987\u001b[0m  \u001b[32m0.9822\u001b[0m        \u001b[35m0.0200\u001b[0m        \u001b[31m0.0746\u001b[0m     +  1.9593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     76                    \u001b[36m-0.1982\u001b[0m  \u001b[32m0.9824\u001b[0m        \u001b[35m0.0199\u001b[0m        \u001b[31m0.0737\u001b[0m     +  2.2858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     77                    \u001b[36m-0.1978\u001b[0m  \u001b[32m0.9826\u001b[0m        \u001b[35m0.0199\u001b[0m        \u001b[31m0.0730\u001b[0m     +  2.8177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     78                    \u001b[36m-0.1976\u001b[0m  \u001b[32m0.9827\u001b[0m        \u001b[35m0.0198\u001b[0m        \u001b[31m0.0723\u001b[0m     +  2.6754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     79                    -0.1977  \u001b[32m0.9828\u001b[0m        \u001b[35m0.0198\u001b[0m        \u001b[31m0.0719\u001b[0m     +  2.0431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     80                    -0.1982  \u001b[32m0.9829\u001b[0m        \u001b[35m0.0197\u001b[0m        \u001b[31m0.0715\u001b[0m     +  4.8397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     81                    -0.1988  \u001b[32m0.9830\u001b[0m        \u001b[35m0.0197\u001b[0m        \u001b[31m0.0713\u001b[0m     +  2.9353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     82                    -0.1997  \u001b[32m0.9830\u001b[0m        \u001b[35m0.0196\u001b[0m        \u001b[31m0.0712\u001b[0m     +  2.2220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     83                    -0.2007  0.9830        \u001b[35m0.0196\u001b[0m        0.0713        1.5332\n",
      "     84                    -0.2019  0.9829        \u001b[35m0.0195\u001b[0m        0.0714        1.4971\n",
      "     85                    -0.2033  0.9829        \u001b[35m0.0195\u001b[0m        0.0717        1.5380\n",
      "     86                    -0.2048  0.9828        \u001b[35m0.0195\u001b[0m        0.0720        1.5252\n",
      "     87                    -0.2063  0.9827        \u001b[35m0.0194\u001b[0m        0.0725        1.8962\n",
      "     88                    -0.2079  0.9826        \u001b[35m0.0194\u001b[0m        0.0730        2.1390\n",
      "     89                    -0.2095  0.9824        \u001b[35m0.0194\u001b[0m        0.0736        1.6523\n",
      "     90                    -0.2111  0.9823        \u001b[35m0.0193\u001b[0m        0.0742        1.5016\n",
      "     91                    -0.2128  0.9821        \u001b[35m0.0193\u001b[0m        0.0749        2.0295\n",
      "     92                    -0.2145  0.9819        \u001b[35m0.0193\u001b[0m        0.0756        2.1326\n",
      "     93                    -0.2161  0.9817        \u001b[35m0.0193\u001b[0m        0.0764        2.8507\n",
      "     94                    -0.2177  0.9816        \u001b[35m0.0192\u001b[0m        0.0771        2.1553\n",
      "     95                    -0.2193  0.9814        \u001b[35m0.0192\u001b[0m        0.0779        1.9997\n",
      "     96                    -0.2208  0.9812        \u001b[35m0.0192\u001b[0m        0.0786        2.7483\n",
      "     97                    -0.2222  0.9810        \u001b[35m0.0192\u001b[0m        0.0793        2.5325\n",
      "     98                    -0.2235  0.9809        \u001b[35m0.0192\u001b[0m        0.0801        1.7585\n",
      "     99                    -0.2247  0.9807        \u001b[35m0.0192\u001b[0m        0.0807        1.7418\n",
      "    100                    -0.2260  0.9806        \u001b[35m0.0192\u001b[0m        0.0814        1.7974\n",
      "    101                    -0.2271  0.9804        \u001b[35m0.0192\u001b[0m        0.0819        2.0514\n",
      "    102                    -0.2281  0.9803        0.0192        0.0825        2.6823\n",
      "    103                    -0.2290  0.9802        0.0192        0.0830        2.7205\n",
      "    104                    -0.2299  0.9801        0.0192        0.0834        1.6227\n",
      "    105                    -0.2306  0.9800        0.0192        0.0837        1.5052\n",
      "    106                    -0.2312  0.9799        0.0192        0.0840        1.4459\n",
      "    107                    -0.2317  0.9799        0.0192        0.0843        1.7561\n",
      "    108                    -0.2321  0.9798        0.0192        0.0845        1.5515\n",
      "    109                    -0.2324  0.9798        0.0193        0.0846        1.4764\n",
      "    110                    -0.2326  0.9798        0.0193        0.0847        1.4820\n",
      "    111                    -0.2327  0.9798        0.0193        0.0847        1.4442\n",
      "    112                    -0.2327  0.9798        0.0193        0.0847        1.8356\n",
      "    113                    -0.2326  0.9798        0.0193        0.0846        2.7186\n",
      "    114                    -0.2324  0.9798        0.0194        0.0844        2.2992\n",
      "    115                    -0.2321  0.9799        0.0194        0.0843        1.8586\n",
      "    116                    -0.2318  0.9799        0.0194        0.0840        1.8356\n",
      "    117                    -0.2314  0.9800        0.0194        0.0838        2.0577\n",
      "    118                    -0.2309  0.9800        0.0194        0.0835        2.8310\n",
      "    119                    -0.2304  0.9801        0.0194        0.0832        2.5341\n",
      "    120                    -0.2298  0.9802        0.0194        0.0829        2.1441\n",
      "    121                    -0.2291  0.9803        0.0194        0.0825        2.5696\n",
      "    122                    -0.2285  0.9804        0.0194        0.0821        2.8314\n",
      "    123                    -0.2278  0.9805        0.0194        0.0817        2.6997\n",
      "    124                    -0.2270  0.9806        0.0194        0.0813        1.4480\n",
      "    125                    -0.2262  0.9807        0.0193        0.0809        2.3648\n",
      "    126                    -0.2254  0.9808        0.0192        0.0805        1.7946\n",
      "    127                    -0.2246  0.9809        \u001b[35m0.0192\u001b[0m        0.0801        2.7672\n",
      "    128                    -0.2237  0.9809        \u001b[35m0.0191\u001b[0m        0.0797        1.8229\n",
      "    129                    -0.2229  0.9810        \u001b[35m0.0189\u001b[0m        0.0793        1.6062\n",
      "    130                    -0.2220  0.9811        \u001b[35m0.0188\u001b[0m        0.0789        2.4434\n",
      "    131                    -0.2211  0.9812        \u001b[35m0.0186\u001b[0m        0.0786        1.8976\n",
      "    132                    -0.2203  0.9813        \u001b[35m0.0185\u001b[0m        0.0782        1.6445\n",
      "    133                    -0.2195  0.9814        \u001b[35m0.0183\u001b[0m        0.0778        1.6187\n",
      "    134                    -0.2187  0.9815        \u001b[35m0.0181\u001b[0m        0.0775        1.5707\n",
      "    135                    -0.2179  0.9816        \u001b[35m0.0178\u001b[0m        0.0771        1.4715\n",
      "    136                    -0.2170  0.9817        \u001b[35m0.0176\u001b[0m        0.0768        1.8998\n",
      "    137                    -0.2162  0.9817        \u001b[35m0.0174\u001b[0m        0.0764        2.4856\n",
      "    138                    -0.2153  0.9818        \u001b[35m0.0172\u001b[0m        0.0760        2.8027\n",
      "    139                    -0.2145  0.9819        \u001b[35m0.0169\u001b[0m        0.0756        2.7609\n",
      "    140                    -0.2137  0.9820        \u001b[35m0.0167\u001b[0m        0.0753        2.7744\n",
      "    141                    -0.2129  0.9821        \u001b[35m0.0165\u001b[0m        0.0749        2.2890\n",
      "    142                    -0.2123  0.9822        \u001b[35m0.0163\u001b[0m        0.0746        2.4122\n",
      "    143                    -0.2117  0.9822        \u001b[35m0.0161\u001b[0m        0.0744        2.0137\n",
      "    144                    -0.2113  0.9823        \u001b[35m0.0160\u001b[0m        0.0742        1.6759\n",
      "    145                    -0.2110  0.9823        \u001b[35m0.0158\u001b[0m        0.0741        1.5331\n",
      "    146                    -0.2109  0.9823        \u001b[35m0.0157\u001b[0m        0.0742        1.5456\n",
      "    147                    -0.2110  0.9822        \u001b[35m0.0155\u001b[0m        0.0743        1.4625\n",
      "    148                    -0.2114  0.9822        \u001b[35m0.0154\u001b[0m        0.0746        2.3275\n",
      "    149                    -0.2119  0.9821        \u001b[35m0.0153\u001b[0m        0.0750        2.5509\n",
      "    150                    -0.2128  0.9820        \u001b[35m0.0151\u001b[0m        0.0755        2.2106\n",
      "    151                    -0.2139  0.9818        \u001b[35m0.0150\u001b[0m        0.0761        2.8005\n",
      "    152                    -0.2152  0.9816        \u001b[35m0.0149\u001b[0m        0.0769        2.6450\n",
      "    153                    -0.2167  0.9814        \u001b[35m0.0148\u001b[0m        0.0777        2.5086\n",
      "    154                    -0.2186  0.9812        \u001b[35m0.0146\u001b[0m        0.0786        3.3999\n",
      "    155                    -0.2208  0.9810        \u001b[35m0.0145\u001b[0m        0.0796        2.0356\n",
      "    156                    -0.2238  0.9807        \u001b[35m0.0144\u001b[0m        0.0809        1.8018\n",
      "    157                    -0.2283  0.9802        \u001b[35m0.0143\u001b[0m        0.0827        2.3971\n",
      "    158                    -0.2348  0.9796        \u001b[35m0.0142\u001b[0m        0.0855        1.5744\n",
      "    159                    -0.2409  0.9789        \u001b[35m0.0141\u001b[0m        0.0882        1.9965\n",
      "    160                    -0.2432  0.9787        0.0141        0.0889        1.4526\n",
      "    161                    -0.2400  0.9793        0.0143        0.0865        1.5224\n",
      "    162                    -0.2353  0.9800        0.0146        0.0836        1.6591\n",
      "    163                    -0.2220  0.9821        0.0151        0.0748        2.1743\n",
      "    164                    -0.2013  \u001b[32m0.9846\u001b[0m        0.0158        \u001b[31m0.0645\u001b[0m     +  2.5524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    165                    -0.2294  0.9804        0.0161        0.0820        2.2108\n",
      "    166                    -0.2271  0.9805        0.0156        0.0815        4.7106\n",
      "    167                    -0.2298  0.9802        0.0145        0.0827        2.0196\n",
      "    168                    -0.2279  0.9808        0.0142        0.0804        2.1398\n",
      "    169                    -0.2254  0.9814        0.0143        0.0780        1.8515\n",
      "    170                    -0.2246  0.9816        0.0145        0.0770        2.4233\n",
      "    171                    -0.2234  0.9819        0.0146        0.0759        1.4915\n",
      "    172                    -0.2226  0.9820        0.0147        0.0751        1.5031\n",
      "    173                    -0.2198  0.9824        0.0147        0.0736        1.7996\n",
      "    174                    -0.2194  0.9827        0.0148        0.0726        1.4817\n",
      "    175                    -0.2028  0.9844        0.0147        0.0653        1.4795\n",
      "    176                    -0.2169  0.9823        \u001b[35m0.0140\u001b[0m        0.0742        2.6030\n",
      "    177                    -0.2210  0.9820        \u001b[35m0.0134\u001b[0m        0.0751        2.0801\n",
      "    178                    -0.2207  0.9824        \u001b[35m0.0130\u001b[0m        0.0737        2.2023\n",
      "    179                    -0.2160  0.9832        \u001b[35m0.0128\u001b[0m        0.0702        1.8872\n",
      "    180                    -0.2132  0.9836        0.0130        0.0684        1.7852\n",
      "    181                    -0.2104  0.9840        0.0132        0.0670        1.9078\n",
      "    182                    -0.2113  0.9839        0.0134        0.0673        1.9087\n",
      "    183                    -0.2051  \u001b[32m0.9847\u001b[0m        0.0134        \u001b[31m0.0642\u001b[0m     +  1.6322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    184                    -0.2124  0.9839        0.0134        0.0674        2.8790\n",
      "    185                    \u001b[36m-0.1970\u001b[0m  \u001b[32m0.9855\u001b[0m        0.0130        \u001b[31m0.0607\u001b[0m     +  3.1058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    186                    -0.2077  0.9842        \u001b[35m0.0122\u001b[0m        0.0661        2.3996\n",
      "    187                    -0.2157  0.9835        \u001b[35m0.0118\u001b[0m        0.0692        2.8043\n",
      "    188                    -0.2122  0.9840        \u001b[35m0.0116\u001b[0m        0.0667        2.3323\n",
      "    189                    -0.2040  0.9851        0.0117        0.0621        2.5148\n",
      "    190                    -0.2017  0.9854        0.0119        0.0612        2.4084\n",
      "    191                    -0.2008  0.9855        0.0121        0.0607        2.7661\n",
      "    192                    -0.2023  0.9853        0.0123        0.0615        2.1119\n",
      "    193                    -0.2005  \u001b[32m0.9855\u001b[0m        0.0122        \u001b[31m0.0606\u001b[0m     +  1.5033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    194                    -0.2039  0.9852        0.0120        0.0618        1.9804\n",
      "    195                    \u001b[36m-0.1927\u001b[0m  \u001b[32m0.9865\u001b[0m        0.0119        \u001b[31m0.0564\u001b[0m     +  1.5449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    196                    -0.2098  0.9843        \u001b[35m0.0115\u001b[0m        0.0655        1.5781\n",
      "    197                    -0.2101  0.9845        \u001b[35m0.0110\u001b[0m        0.0648        1.9747\n",
      "    198                    -0.1980  0.9860        \u001b[35m0.0107\u001b[0m        0.0586        1.5188\n",
      "    199                    -0.2004  0.9857        0.0109        0.0599        1.4962\n",
      "    200                    -0.1992  0.9859        0.0108        0.0591        1.5061\n",
      "    201                    -0.2036  0.9854        0.0108        0.0611        1.5510\n",
      "    202                    -0.1968  0.9862        \u001b[35m0.0107\u001b[0m        0.0578        1.5967\n",
      "    203                    -0.2064  0.9851        \u001b[35m0.0106\u001b[0m        0.0625        2.9500\n",
      "    204                    -0.1990  0.9859        \u001b[35m0.0104\u001b[0m        0.0589        2.7889\n",
      "    205                    -0.2043  0.9853        \u001b[35m0.0102\u001b[0m        0.0613        1.9555\n",
      "    206                    -0.2077  0.9850        0.0103        0.0628        1.7464\n",
      "    207                    -0.2017  0.9857        \u001b[35m0.0100\u001b[0m        0.0599        3.6125\n",
      "    208                    -0.2041  0.9855        0.0103        0.0607        3.7062\n",
      "    209                    -0.1976  0.9862        \u001b[35m0.0100\u001b[0m        0.0579        3.6491\n",
      "    210                    -0.2089  0.9848        \u001b[35m0.0099\u001b[0m        0.0635        1.6239\n",
      "    211                    -0.2220  0.9832        \u001b[35m0.0096\u001b[0m        0.0703        1.5548\n",
      "    212                    -0.2070  0.9851        \u001b[35m0.0094\u001b[0m        0.0623        2.7979\n",
      "    213                    -0.1952  0.9865        0.0100        0.0566        2.8443\n",
      "    214                    -0.2010  0.9856        0.0105        0.0603        2.7316\n",
      "    215                    -0.2019  0.9858        0.0110        0.0594        1.9672\n",
      "    216                    -0.2063  0.9848        0.0102        0.0637        2.7786\n",
      "    217                    -0.2395  0.9804        0.0106        0.0820        1.8138\n",
      "    218                    -0.2322  0.9814        \u001b[35m0.0091\u001b[0m        0.0778        6.3141\n",
      "    219                    -0.2160  0.9840        \u001b[35m0.0086\u001b[0m        0.0669        1.9552\n",
      "    220                    -0.1991  0.9860        0.0089        0.0587        1.8059\n",
      "    221                    -0.2030  0.9854        0.0102        0.0610        1.8385\n",
      "    222                    -0.2140  0.9842        0.0108        0.0660        1.5497\n",
      "    223                    -0.2042  0.9855        0.0097        0.0609        1.4602\n",
      "    224                    -0.2182  0.9836        0.0096        0.0686        1.5053\n",
      "    225                    -0.2113  0.9846        0.0095        0.0643        1.4519\n",
      "    226                    -0.1938  \u001b[32m0.9866\u001b[0m        0.0093        \u001b[31m0.0562\u001b[0m     +  1.6189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    227                    -0.2101  0.9846        0.0096        0.0646        1.5138\n",
      "    228                    -0.2056  0.9854        0.0095        0.0611        1.4431\n",
      "    229                    -0.2007  0.9859        0.0088        0.0591        1.5150\n",
      "    230                    -0.1981  0.9861        0.0088        0.0581        1.5799\n",
      "    231                    -0.1956  0.9863        0.0089        0.0571        1.4608\n",
      "    232                    \u001b[36m-0.1899\u001b[0m  \u001b[32m0.9869\u001b[0m        0.0091        \u001b[31m0.0546\u001b[0m     +  1.7954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    233                    \u001b[36m-0.1797\u001b[0m  \u001b[32m0.9880\u001b[0m        0.0092        \u001b[31m0.0502\u001b[0m     +  2.0562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    234                    -0.2175  0.9832        0.0106        0.0702        2.2707\n",
      "    235                    -0.1931  0.9866        0.0100        0.0562        3.5976\n",
      "    236                    \u001b[36m-0.1730\u001b[0m  \u001b[32m0.9886\u001b[0m        0.0093        \u001b[31m0.0476\u001b[0m     +  2.4477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    237                    -0.1956  0.9857        0.0121        0.0597        2.2144\n",
      "    238                    -0.2694  0.9731        0.0102        0.1126        1.7888\n",
      "    239                    -0.2531  0.9777        \u001b[35m0.0075\u001b[0m        0.0934        1.7553\n",
      "    240                    -0.2186  0.9833        \u001b[35m0.0074\u001b[0m        0.0699        2.3606\n",
      "    241                    -0.2073  0.9848        0.0075        0.0635        2.7710\n",
      "    242                    -0.1931  0.9865        0.0076        0.0564        2.2952\n",
      "    243                    -0.1965  0.9861        0.0087        0.0582        2.6888\n",
      "    244                    -0.2280  0.9819        0.0089        0.0755        1.8682\n",
      "    245                    -0.2272  0.9823        0.0075        0.0741        1.7731\n",
      "    246                    -0.2109  0.9845        \u001b[35m0.0072\u001b[0m        0.0650        1.8359\n",
      "    247                    -0.2018  0.9855        0.0075        0.0608        1.5694\n",
      "    248                    -0.2039  0.9852        0.0081        0.0618        1.4619\n",
      "    249                    -0.2184  0.9835        0.0084        0.0690        1.8583\n",
      "    250                    -0.2157  0.9839        0.0077        0.0674        1.9504\n",
      "    251                    -0.2146  0.9840        0.0075        0.0670        1.7619\n",
      "    252                    -0.2067  0.9851        0.0077        0.0625        2.2729\n",
      "    253                    -0.2121  0.9843        0.0078        0.0659        1.8370\n",
      "    254                    -0.2162  0.9839        0.0079        0.0675        1.8389\n",
      "    255                    -0.2162  0.9838        0.0077        0.0677        1.6758\n",
      "    256                    -0.2114  0.9844        0.0078        0.0652        1.4796\n",
      "    257                    -0.2149  0.9840        0.0079        0.0670        1.4652\n",
      "    258                    -0.2068  0.9851        0.0080        0.0622        1.8174\n",
      "    259                    -0.2127  0.9843        0.0078        0.0659        2.4065\n",
      "    260                    -0.2152  0.9841        0.0078        0.0666        1.8223\n",
      "    261                    -0.2129  0.9843        0.0078        0.0658        1.8905\n",
      "    262                    -0.2082  0.9848        0.0079        0.0635        2.0827\n",
      "    263                    -0.2129  0.9843        0.0080        0.0658        4.5610\n",
      "    264                    -0.2055  0.9853        0.0080        0.0614        2.5574\n",
      "    265                    -0.2088  0.9848        0.0078        0.0636        2.8466\n",
      "    266                    -0.2069  0.9850        0.0079        0.0627        2.0710\n",
      "    267                    -0.2100  0.9847        0.0080        0.0641        2.1523\n",
      "    268                    -0.2095  0.9847        0.0079        0.0640        2.8299\n",
      "    269                    -0.2124  0.9844        0.0079        0.0654        1.7444\n",
      "    270                    -0.2035  0.9855        0.0078        0.0609        1.7694\n",
      "    271                    -0.2100  0.9846        0.0079        0.0643        1.6251\n",
      "    272                    -0.2085  0.9848        0.0078        0.0634        2.0303\n",
      "    273                    -0.2092  0.9847        0.0077        0.0639        2.6792\n",
      "    274                    -0.2027  0.9855        0.0076        0.0608        2.4815\n",
      "    275                    -0.2102  0.9846        0.0078        0.0646        3.0123\n",
      "    276                    -0.2098  0.9846        0.0076        0.0644        1.8036\n",
      "    277                    -0.2113  0.9844        0.0075        0.0652        1.7347\n",
      "    278                    -0.2046  0.9852        0.0074        0.0621        1.7598\n",
      "    279                    -0.2125  0.9842        0.0076        0.0661        1.8240\n",
      "    280                    -0.2140  0.9840        0.0074        0.0670        1.4649\n",
      "    281                    -0.2157  0.9837        0.0073        0.0680        1.5441\n",
      "    282                    -0.2099  0.9844        \u001b[35m0.0072\u001b[0m        0.0651        1.6383\n",
      "    283                    -0.2156  0.9837        0.0073        0.0681        2.0475\n",
      "    284                    -0.2148  0.9838        0.0072        0.0679        2.3689\n",
      "    285                    -0.2182  0.9833        0.0072        0.0697        2.3796\n",
      "    286                    -0.2123  0.9841        \u001b[35m0.0071\u001b[0m        0.0667        2.4098\n",
      "    287                    -0.2192  0.9832        0.0073        0.0704        2.2866\n",
      "    288                    -0.2160  0.9835        0.0072        0.0689        2.2433\n",
      "    289                    -0.2225  0.9827        0.0072        0.0724        1.9041\n",
      "    290                    -0.2138  0.9838        \u001b[35m0.0071\u001b[0m        0.0677        2.0669\n",
      "    291                    -0.2243  0.9824        0.0073        0.0736        4.5339\n",
      "    292                    -0.2220  0.9827        0.0071        0.0724        2.3950\n",
      "    293                    -0.2275  0.9820        \u001b[35m0.0070\u001b[0m        0.0753        2.5405\n",
      "    294                    -0.2163  0.9835        \u001b[35m0.0070\u001b[0m        0.0691        2.7112\n",
      "    295                    -0.2261  0.9822        0.0071        0.0746        2.4044\n",
      "    296                    -0.2210  0.9829        0.0070        0.0716        2.0893\n",
      "    297                    -0.2250  0.9824        0.0070        0.0738        3.8296\n",
      "    298                    -0.2161  0.9835        0.0070        0.0692        2.7994\n",
      "    299                    -0.2274  0.9820        0.0072        0.0753        2.1894\n",
      "    300                    -0.2199  0.9830        0.0071        0.0713        2.0066\n",
      "    301                    -0.2297  0.9816        0.0072        0.0769        1.9711\n",
      "    302                    -0.2221  0.9827        0.0071        0.0725        1.6424\n",
      "    303                    -0.2318  0.9813        0.0072        0.0783        2.7568\n",
      "    304                    -0.2313  0.9813        0.0070        0.0784        2.3834\n",
      "    305                    -0.2391  0.9803        \u001b[35m0.0068\u001b[0m        0.0824        2.6388\n",
      "    306                    -0.2239  0.9825        \u001b[35m0.0068\u001b[0m        0.0733        2.2450\n",
      "    307                    -0.2336  0.9810        0.0068        0.0794        2.5110\n",
      "    308                    -0.2325  0.9812        0.0068        0.0786        8.0178\n",
      "    309                    -0.2288  0.9818        0.0068        0.0760        2.9000\n",
      "    310                    -0.2174  0.9830        0.0070        0.0710        2.4973\n",
      "    311                    -0.2426  0.9795        0.0073        0.0856        5.0294\n",
      "    312                    -0.2375  0.9802        0.0071        0.0829        2.2880\n",
      "    313                    -0.2418  0.9794        0.0070        0.0863        3.5647\n",
      "    314                    -0.2396  0.9800        0.0068        0.0838        2.6983\n",
      "    315                    -0.2423  0.9794        0.0070        0.0862        2.1725\n",
      "    316                    -0.2480  0.9780        0.0069        0.0919        2.1407\n",
      "    317                    -0.2588  0.9766        \u001b[35m0.0065\u001b[0m        0.0978        2.7854\n",
      "    318                    -0.2470  0.9787        \u001b[35m0.0063\u001b[0m        0.0891        1.9886\n",
      "    319                    -0.2405  0.9793        0.0065        0.0866        2.3515\n",
      "    320                    -0.2504  0.9780        0.0064        0.0922        1.8542\n",
      "    321                    -0.2386  0.9799        0.0065        0.0843        2.3604\n",
      "    322                    -0.2419  0.9789        0.0069        0.0881        2.3871\n",
      "    323                    -0.2459  0.9780        0.0069        0.0921        4.0865\n",
      "    324                    -0.2620  0.9759        0.0069        0.1009        2.2596\n",
      "    325                    -0.2363  0.9794        0.0068        0.0862        1.8005\n",
      "    326                    -0.2735  0.9730        0.0071        0.1132        1.8639\n",
      "    327                    -0.2792  0.9724        0.0065        0.1157        2.6032\n",
      "    328                    -0.2604  0.9750        \u001b[35m0.0063\u001b[0m        0.1044        2.5426\n",
      "    329                    -0.2693  0.9733        0.0065        0.1118        2.9486\n",
      "    330                    -0.2677  0.9744        \u001b[35m0.0060\u001b[0m        0.1073        2.3475\n",
      "    331                    -0.2436  0.9781        0.0061        0.0918        2.0924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    332                    -0.2544  0.9760        0.0065        0.1004        2.7716\n",
      "    333                    -0.2470  0.9769        0.0066        0.0967        1.9965\n",
      "    334                    -0.2629  0.9749        0.0068        0.1052        3.3481\n",
      "    335                    -0.2387  0.9781        0.0069        0.0918        2.0092\n",
      "    336                    -0.2730  0.9719        0.0075        0.1174        1.8359\n",
      "    337                    -0.2718  0.9723        0.0065        0.1160        2.2220\n",
      "    338                    -0.2773  0.9717        0.0061        0.1183        2.3157\n",
      "    339                    -0.2650  0.9743        0.0060        0.1076        1.9866\n",
      "    340                    -0.2674  0.9747        \u001b[35m0.0059\u001b[0m        0.1060        1.9309\n",
      "    341                    -0.2487  0.9778        0.0059        0.0927        2.4410\n",
      "    342                    -0.2549  0.9769        0.0061        0.0964        5.7992\n",
      "    343                    -0.2458  0.9781        0.0063        0.0917        2.3121\n",
      "    344                    -0.2592  0.9760        0.0067        0.1004        2.7713\n",
      "    345                    -0.2492  0.9767        0.0069        0.0977        2.2043\n",
      "    346                    -0.2672  0.9745        0.0070        0.1066        2.6078\n",
      "    347                    -0.2527  0.9770        0.0070        0.0961        3.3969\n",
      "    348                    -0.2697  0.9731        0.0072        0.1126        2.9619\n",
      "    349                    -0.2848  0.9707        0.0064        0.1228        1.8495\n",
      "    350                    -0.2807  0.9723        0.0059        0.1158        1.8289\n",
      "    351                    -0.2589  0.9760        0.0061        0.1002        1.8523\n",
      "    352                    -0.2724  0.9742        0.0060        0.1079        2.2340\n",
      "    353                    -0.2592  0.9766        0.0059        0.0977        5.6359\n",
      "    354                    -0.2475  0.9778        0.0061        0.0929        3.3760\n",
      "    355                    -0.2589  0.9756        0.0064        0.1020        2.9737\n",
      "    356                    -0.2711  0.9738        0.0065        0.1098        4.9527\n",
      "    357                    -0.2649  0.9740        0.0066        0.1087        2.0273\n",
      "    358                    -0.2812  0.9708        0.0065        0.1223        1.7971\n",
      "    359                    -0.2914  0.9694        0.0061        0.1281        2.5429\n",
      "    360                    -0.2684  0.9729        0.0060        0.1134        2.9039\n",
      "    361                    -0.2800  0.9706        0.0062        0.1230        2.8916\n",
      "    362                    -0.2839  0.9707        \u001b[35m0.0058\u001b[0m        0.1227        1.9340\n",
      "    363                    -0.2725  0.9722        0.0059        0.1164        2.4777\n",
      "    364                    -0.2711  0.9716        0.0062        0.1187        2.4191\n",
      "    365                    -0.2838  0.9698        0.0061        0.1263        4.4082\n",
      "    366                    -0.2773  0.9710        0.0061        0.1212        2.4838\n",
      "    367                    -0.2783  0.9701        0.0063        0.1251        2.9368\n",
      "    368                    -0.2846  0.9694        0.0061        0.1282        2.5141\n",
      "    369                    -0.2880  0.9695        0.0059        0.1278        2.5112\n",
      "    370                    -0.2714  0.9719        0.0060        0.1175        2.8401\n",
      "    371                    -0.2870  0.9694        0.0061        0.1280        2.5125\n",
      "    372                    -0.2847  0.9703        0.0059        0.1243        6.1759\n",
      "    373                    -0.2724  0.9720        0.0060        0.1170        3.0538\n",
      "    374                    -0.2802  0.9704        0.0062        0.1238        1.8891\n",
      "    375                    -0.2816  0.9709        0.0060        0.1216        3.8665\n",
      "    376                    -0.2794  0.9709        0.0063        0.1218        2.5352\n",
      "    377                    -0.2809  0.9705        0.0063        0.1236        2.2653\n",
      "    378                    -0.2925  0.9686        0.0062        0.1314        2.6122\n",
      "    379                    -0.2876  0.9697        0.0059        0.1266        2.5434\n",
      "    380                    -0.2795  0.9712        0.0060        0.1206        2.1345\n",
      "    381                    -0.2933  0.9695        \u001b[35m0.0058\u001b[0m        0.1278        1.8886\n",
      "    382                    -0.2750  0.9727        0.0058        0.1144        5.5892\n",
      "    383                    -0.2723  0.9727        0.0059        0.1144        2.2452\n",
      "    384                    -0.2783  0.9719        0.0059        0.1175        2.9436\n",
      "    385                    -0.2789  0.9719        0.0060        0.1176        2.2784\n",
      "    386                    -0.2756  0.9718        0.0064        0.1178        3.9404\n",
      "    387                    -0.2851  0.9702        0.0063        0.1248        2.2940\n",
      "    388                    -0.2960  0.9682        0.0062        0.1329        4.2279\n",
      "    389                    -0.2815  0.9708        0.0061        0.1223        2.3261\n",
      "    390                    -0.2879  0.9696        0.0061        0.1271        7.2324\n",
      "    391                    -0.3007  0.9681        \u001b[35m0.0057\u001b[0m        0.1335        3.6086\n",
      "    392                    -0.2698  0.9728        0.0058        0.1139        1.9315\n",
      "    393                    -0.2844  0.9698        0.0061        0.1264        1.9836\n",
      "    394                    -0.2880  0.9696        \u001b[35m0.0057\u001b[0m        0.1270        1.7878\n",
      "    395                    -0.2766  0.9706        0.0058        0.1230        1.8112\n",
      "    396                    -0.2802  0.9692        0.0063        0.1288        1.5457\n",
      "    397                    -0.2827  0.9692        0.0061        0.1290        1.8674\n",
      "    398                    -0.2961  0.9667        0.0061        0.1393        2.7700\n",
      "    399                    -0.2822  0.9691        0.0060        0.1294        1.9714\n",
      "    400                    -0.2916  0.9676        0.0059        0.1357        2.9278\n",
      "    401                    -0.2970  0.9677        \u001b[35m0.0055\u001b[0m        0.1352        3.5308\n",
      "    402                    -0.2737  0.9718        \u001b[35m0.0055\u001b[0m        0.1182        2.7216\n",
      "    403                    -0.2807  0.9707        0.0057        0.1225        2.8721\n",
      "    404                    -0.2763  0.9720        \u001b[35m0.0054\u001b[0m        0.1171        2.7077\n",
      "    405                    -0.2750  0.9717        0.0056        0.1186        2.7239\n",
      "    406                    -0.2667  0.9724        0.0062        0.1154        2.2631\n",
      "    407                    -0.2884  0.9682        0.0064        0.1331        1.5969\n",
      "    408                    -0.2919  0.9678        0.0064        0.1348        1.8467\n",
      "    409                    -0.2834  0.9690        0.0061        0.1299        1.7422\n",
      "    410                    -0.2969  0.9669        0.0059        0.1386        3.7918\n",
      "    411                    -0.2996  0.9675        0.0055        0.1360        1.7512\n",
      "    412                    -0.2759  0.9717        0.0056        0.1184        2.3700\n",
      "    413                    -0.2835  0.9709        0.0056        0.1217        2.2667\n",
      "    414                    -0.2846  0.9714        \u001b[35m0.0053\u001b[0m        0.1197        2.2526\n",
      "    415                    -0.2615  0.9743        0.0055        0.1074        1.4698\n",
      "    416                    -0.2697  0.9722        0.0059        0.1165        1.5625\n",
      "    417                    -0.2754  0.9713        0.0061        0.1201        1.5254\n",
      "    418                    -0.2853  0.9679        0.0066        0.1344        2.4022\n",
      "    419                    -0.2870  0.9673        0.0066        0.1370        2.5549\n",
      "    420                    -0.3106  0.9625        0.0061        0.1571        2.3793\n",
      "    421                    -0.3045  0.9653        0.0054        0.1453        2.3311\n",
      "    422                    -0.2786  0.9703        0.0055        0.1244        1.9535\n",
      "    423                    -0.2825  0.9708        0.0056        0.1221        3.0694\n",
      "    424                    -0.2792  0.9721        \u001b[35m0.0052\u001b[0m        0.1168        2.2082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
       "  module_=SequenceDoubleAtt(\n",
       "    (lstm1): LSTM(4, 256, batch_first=True)\n",
       "    (lstm2): LSTM(256, 512, batch_first=True)\n",
       "    (lin): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (lin_out): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (tanh): Tanh()\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lstm_att.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = datasets['^GSPC'][\"target\"]\n",
    "ext_dataset = datasets['^GSPC'][\"features\"]['ext']\n",
    "ext_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "# sa = load('ext_encoder_better')\n",
    "ext_train, ext_val = ext_scaler.fit_transform(ext_dataset.iloc[train_dates].to_numpy(np.float32)), ext_scaler.transform(ext_dataset.iloc[val_dates].to_numpy(np.float32))\n",
    "y_train, y_val = y_scaler.fit_transform(y.iloc[train_dates].to_numpy(np.float32)[...,None]), y_scaler.transform(y.iloc[val_dates].to_numpy(np.float32)[...,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_encoder = load('ext_encoder_better')\n",
    "ext_sa_train, ext_sa_val = encode(ext_train, ext_encoder), encode(ext_val, ext_encoder)\n",
    "ext_sa = np.concatenate((ext_sa_train, ext_sa_val))\n",
    "y_data = np.concatenate((y_train, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = days_group(ext_sa, n_days=n_days)\n",
    "y = y_data[n_days:]\n",
    "l1 = len(np.split(x, [(len(ext_sa)*3)//4])[0])\n",
    "l2 = len(np.split(x, [(len(ext_sa)*3)//4])[1])\n",
    "half = PredefinedSplit(np.concatenate((np.ones(l1)*-1,np.ones(l2))))\n",
    "half_split =  CVSplit(cv=half, stratified=False, random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modello del paper che usa attention mechanism\n",
    "#TODO testa se funziona e trova epoche\n",
    "n_days = 5\n",
    "\n",
    "batch_size = 20\n",
    "\n",
    "lstm_sa_att = NeuralNetRegressor(\n",
    "    module=SequenceDoubleAtt,\n",
    "    optimizer=optim.Adam,\n",
    "    batch_size=batch_size,\n",
    "    max_epochs=5000, # trovato 280\n",
    "    train_split=half_split,\n",
    "    callbacks=[\n",
    "        callbacks.EpochScoring('neg_mean_absolute_error', lower_is_better=False),\n",
    "        callbacks.EpochScoring('r2', lower_is_better=False),\n",
    "        callbacks.Checkpoint(monitor='valid_loss_best', f_pickle='lstm_sa_best')        \n",
    "    ],\n",
    "    \n",
    "    module__nb_features=ext_sa_train.shape[1],\n",
    "    module__hidden_size=256,\n",
    "#     module__nb_layers= 5,\n",
    "    optimizer__lr=0.0001,\n",
    "#     optimizer__weight_decay=0,\n",
    "#     optimizer__momentum=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    neg_mean_absolute_error       r2    train_loss    valid_loss    cp     dur\n",
      "-------  -------------------------  -------  ------------  ------------  ----  ------\n",
      "      1                    \u001b[36m-1.6688\u001b[0m  \u001b[32m-0.0260\u001b[0m        \u001b[35m2.4180\u001b[0m        \u001b[31m3.6369\u001b[0m     +  1.7605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2                    \u001b[36m-1.6681\u001b[0m  \u001b[32m-0.0136\u001b[0m        \u001b[35m2.3650\u001b[0m        \u001b[31m3.5932\u001b[0m     +  4.3878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3                    -1.6691  \u001b[32m-0.0085\u001b[0m        2.3658        \u001b[31m3.5751\u001b[0m     +  1.9298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4                    -1.6844  \u001b[32m0.0009\u001b[0m        \u001b[35m2.2221\u001b[0m        \u001b[31m3.5419\u001b[0m     +  1.9328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5                    -1.7041  -0.0046        \u001b[35m2.1991\u001b[0m        3.5611        6.2987\n",
      "      6                    -1.6997  -0.0020        2.3462        3.5522        8.8208\n",
      "      7                    -1.7113  -0.0103        \u001b[35m2.1271\u001b[0m        3.5815        2.9361\n",
      "      8                    -1.7168  -0.0160        \u001b[35m2.1063\u001b[0m        3.6018        3.0138\n",
      "      9                    -1.7200  -0.0198        \u001b[35m2.0855\u001b[0m        3.6152        3.0639\n",
      "     10                    -1.7220  -0.0225        \u001b[35m2.0695\u001b[0m        3.6246        3.1023\n",
      "     11                    -1.7235  -0.0244        \u001b[35m2.0577\u001b[0m        3.6316        3.0253\n",
      "     12                    -1.7245  -0.0259        \u001b[35m2.0485\u001b[0m        3.6369        2.0844\n",
      "     13                    -1.7253  -0.0271        \u001b[35m2.0413\u001b[0m        3.6412        3.2779\n",
      "     14                    -1.7260  -0.0281        \u001b[35m2.0353\u001b[0m        3.6446        1.6490\n",
      "     15                    -1.7265  -0.0289        \u001b[35m2.0303\u001b[0m        3.6474        1.5899\n",
      "     16                    -1.7270  -0.0296        \u001b[35m2.0260\u001b[0m        3.6498        1.7960\n",
      "     17                    -1.7274  -0.0302        \u001b[35m2.0222\u001b[0m        3.6518        2.0999\n",
      "     18                    -1.7277  -0.0307        \u001b[35m2.0189\u001b[0m        3.6536        1.7411\n",
      "     19                    -1.7280  -0.0311        \u001b[35m2.0160\u001b[0m        3.6552        1.7442\n",
      "     20                    -1.7282  -0.0315        \u001b[35m2.0133\u001b[0m        3.6566        2.8281\n",
      "     21                    -1.7284  -0.0319        \u001b[35m2.0109\u001b[0m        3.6579        3.1406\n",
      "     22                    -1.7286  -0.0322        \u001b[35m2.0088\u001b[0m        3.6590        2.1574\n",
      "     23                    -1.7288  -0.0325        \u001b[35m2.0068\u001b[0m        3.6600        2.2725\n",
      "     24                    -1.7290  -0.0327        \u001b[35m2.0049\u001b[0m        3.6610        1.9712\n",
      "     25                    -1.7291  -0.0330        \u001b[35m2.0032\u001b[0m        3.6619        1.9965\n",
      "     26                    -1.7293  -0.0332        \u001b[35m2.0016\u001b[0m        3.6627        1.8929\n",
      "     27                    -1.7294  -0.0334        \u001b[35m2.0001\u001b[0m        3.6634        5.6851\n",
      "     28                    -1.7295  -0.0336        \u001b[35m1.9987\u001b[0m        3.6641        2.9041\n",
      "     29                    -1.7296  -0.0338        \u001b[35m1.9974\u001b[0m        3.6648        2.6360\n",
      "     30                    -1.7297  -0.0340        \u001b[35m1.9961\u001b[0m        3.6654        2.0526\n",
      "     31                    -1.7298  -0.0342        \u001b[35m1.9950\u001b[0m        3.6660        1.6014\n",
      "     32                    -1.7299  -0.0343        \u001b[35m1.9939\u001b[0m        3.6665        1.7945\n",
      "     33                    -1.7300  -0.0345        \u001b[35m1.9928\u001b[0m        3.6671        1.6320\n",
      "     34                    -1.7301  -0.0346        \u001b[35m1.9917\u001b[0m        3.6676        1.6168\n",
      "     35                    -1.7302  -0.0347        \u001b[35m1.9908\u001b[0m        3.6681        3.0211\n",
      "     36                    -1.7302  -0.0349        \u001b[35m1.9898\u001b[0m        3.6685        2.1421\n",
      "     37                    -1.7303  -0.0350        \u001b[35m1.9890\u001b[0m        3.6690        2.9758\n",
      "     38                    -1.7304  -0.0351        \u001b[35m1.9881\u001b[0m        3.6693        3.0583\n",
      "     39                    -1.7304  -0.0352        \u001b[35m1.9873\u001b[0m        3.6698        2.8841\n",
      "     40                    -1.7305  -0.0353        \u001b[35m1.9865\u001b[0m        3.6701        3.0096\n",
      "     41                    -1.7306  -0.0354        \u001b[35m1.9857\u001b[0m        3.6705        1.9825\n",
      "     61                    -1.7312  -0.0368        \u001b[35m1.9732\u001b[0m        3.6752        2.0824\n",
      "     62                    -1.7312  -0.0368        \u001b[35m1.9726\u001b[0m        3.6753        1.5516\n",
      "     63                    -1.7312  -0.0368        \u001b[35m1.9721\u001b[0m        3.6755        1.7587\n",
      "     64                    -1.7312  -0.0369        \u001b[35m1.9716\u001b[0m        3.6756        3.0450\n",
      "     65                    -1.7312  -0.0369        \u001b[35m1.9710\u001b[0m        3.6757        1.9360\n",
      "     66                    -1.7312  -0.0369        \u001b[35m1.9705\u001b[0m        3.6758        3.3994\n",
      "     67                    -1.7312  -0.0369        \u001b[35m1.9700\u001b[0m        3.6759        2.0393\n",
      "     68                    -1.7312  -0.0370        \u001b[35m1.9694\u001b[0m        3.6760        2.9344\n",
      "     69                    -1.7311  -0.0370        \u001b[35m1.9689\u001b[0m        3.6761        2.7483\n",
      "     70                    -1.7311  -0.0370        \u001b[35m1.9683\u001b[0m        3.6761        2.8767\n",
      "     71                    -1.7311  -0.0370        \u001b[35m1.9677\u001b[0m        3.6761        2.1468\n",
      "     72                    -1.7311  -0.0370        \u001b[35m1.9672\u001b[0m        3.6761        2.0841\n",
      "     73                    -1.7310  -0.0370        \u001b[35m1.9665\u001b[0m        3.6761        2.0903\n",
      "     74                    -1.7310  -0.0370        \u001b[35m1.9659\u001b[0m        3.6760        2.9624\n",
      "     75                    -1.7309  -0.0370        \u001b[35m1.9653\u001b[0m        3.6760        3.0785\n",
      "     76                    -1.7308  -0.0369        \u001b[35m1.9646\u001b[0m        3.6759        3.1020\n",
      "     77                    -1.7308  -0.0369        \u001b[35m1.9639\u001b[0m        3.6758        2.8527\n",
      "     78                    -1.7306  -0.0369        \u001b[35m1.9632\u001b[0m        3.6756        3.1209\n",
      "     79                    -1.7305  -0.0368        \u001b[35m1.9624\u001b[0m        3.6755        3.1057\n",
      "     80                    -1.7304  -0.0368        \u001b[35m1.9616\u001b[0m        3.6752        2.3511\n",
      "     81                    -1.7302  -0.0367        \u001b[35m1.9606\u001b[0m        3.6751        2.4826\n",
      "     82                    -1.7300  -0.0366        \u001b[35m1.9597\u001b[0m        3.6747        2.9951\n",
      "     83                    -1.7298  -0.0365        \u001b[35m1.9585\u001b[0m        3.6745        3.4406\n",
      "     84                    -1.7296  -0.0364        \u001b[35m1.9572\u001b[0m        3.6740        2.6567\n",
      "     85                    -1.7293  -0.0364        \u001b[35m1.9556\u001b[0m        3.6738        2.6136\n",
      "     86                    -1.7289  -0.0362        \u001b[35m1.9538\u001b[0m        3.6733        3.3061\n",
      "     87                    -1.7285  -0.0363        \u001b[35m1.9512\u001b[0m        3.6734        2.4318\n",
      "     88                    -1.7279  -0.0363        \u001b[35m1.9483\u001b[0m        3.6736        2.1250\n",
      "     89                    -1.7275  -0.0367        \u001b[35m1.9439\u001b[0m        3.6750        1.9362\n",
      "     90                    -1.7270  -0.0374        \u001b[35m1.9381\u001b[0m        3.6774        1.9203\n",
      "     91                    -1.7268  -0.0391        \u001b[35m1.9288\u001b[0m        3.6835        2.1880\n",
      "     92                    -1.7271  -0.0425        \u001b[35m1.9140\u001b[0m        3.6955        5.5320\n",
      "     93                    -1.7286  -0.0491        \u001b[35m1.8898\u001b[0m        3.7191        2.9200\n",
      "     94                    -1.7314  -0.0601        \u001b[35m1.8573\u001b[0m        3.7580        1.9870\n",
      "     95                    -1.7372  -0.0787        \u001b[35m1.8136\u001b[0m        3.8239        2.6726\n",
      "     96                    -1.7450  -0.1041        \u001b[35m1.7601\u001b[0m        3.9141        2.7151\n",
      "     97                    -1.7560  -0.1415        \u001b[35m1.6935\u001b[0m        4.0464        3.0959\n",
      "     98                    -1.7676  -0.1865        \u001b[35m1.6410\u001b[0m        4.2060        2.5393\n",
      "     99                    -1.7732  -0.2198        \u001b[35m1.6219\u001b[0m        4.3241        2.9161\n",
      "    100                    -1.8136  -0.3176        \u001b[35m1.4020\u001b[0m        4.6707        2.6155\n",
      "    101                    -1.8954  -0.4920        \u001b[35m1.1411\u001b[0m        5.2891        3.2284\n",
      "    102                    -1.9970  -0.7071        \u001b[35m0.9097\u001b[0m        6.0517        2.5810\n",
      "    103                    -2.1037  -0.8942        \u001b[35m0.7673\u001b[0m        6.7147        3.0562\n",
      "    104                    -2.1961  -1.0467        \u001b[35m0.6425\u001b[0m        7.2555        1.7312\n",
      "    105                    -2.1599  -0.9714        \u001b[35m0.5727\u001b[0m        6.9885        2.7289\n",
      "    106                    -2.3762  -1.3228        0.5820        8.2340        2.0177\n",
      "    107                    -2.1019  -0.8557        \u001b[35m0.5028\u001b[0m        6.5783        1.9301\n",
      "    108                    -2.0509  -0.7516        0.5627        6.2093        1.6077\n",
      "    109                    -2.0176  -0.6826        0.5126        5.9647        1.5903\n",
      "    110                    -1.9828  -0.6126        \u001b[35m0.4746\u001b[0m        5.7165        1.5851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    111                    -1.9362  -0.5254        \u001b[35m0.4420\u001b[0m        5.4075        2.6091\n",
      "    112                    -1.8911  -0.4448        \u001b[35m0.4093\u001b[0m        5.1217        2.1236\n",
      "    113                    -1.8576  -0.3850        \u001b[35m0.3767\u001b[0m        4.9096        3.0635\n",
      "    114                    -1.8328  -0.3410        \u001b[35m0.3473\u001b[0m        4.7539        3.9407\n",
      "    115                    -1.8158  -0.3111        \u001b[35m0.3224\u001b[0m        4.6479        4.3636\n",
      "    116                    -1.8071  -0.2947        \u001b[35m0.3023\u001b[0m        4.5897        3.0634\n",
      "    117                    -1.8041  -0.2872        \u001b[35m0.2865\u001b[0m        4.5630        5.8181\n",
      "    118                    -1.8030  -0.2834        \u001b[35m0.2738\u001b[0m        4.5497        2.1681\n",
      "    119                    -1.8024  -0.2813        \u001b[35m0.2627\u001b[0m        4.5421        1.9686\n",
      "    120                    -1.8016  -0.2798        \u001b[35m0.2520\u001b[0m        4.5367        2.1598\n",
      "    121                    -1.8009  -0.2788        \u001b[35m0.2412\u001b[0m        4.5331        2.2230\n",
      "    122                    -1.7998  -0.2776        \u001b[35m0.2301\u001b[0m        4.5291        2.3618\n",
      "    123                    -1.7982  -0.2760        \u001b[35m0.2191\u001b[0m        4.5234        2.1103\n",
      "    124                    -1.7952  -0.2727        \u001b[35m0.2087\u001b[0m        4.5118        2.1243\n",
      "    125                    -1.7906  -0.2673        \u001b[35m0.1995\u001b[0m        4.4925        3.1135\n",
      "    126                    -1.7836  -0.2591        \u001b[35m0.1919\u001b[0m        4.4633        1.9401\n",
      "    127                    -1.7747  -0.2483        \u001b[35m0.1859\u001b[0m        4.4253        1.9623\n",
      "    128                    -1.7640  -0.2356        \u001b[35m0.1810\u001b[0m        4.3800        1.9452\n",
      "    129                    -1.7522  -0.2215        \u001b[35m0.1769\u001b[0m        4.3303        2.2298\n",
      "    130                    -1.7396  -0.2068        \u001b[35m0.1730\u001b[0m        4.2782        2.6726\n",
      "    131                    -1.7267  -0.1920        \u001b[35m0.1693\u001b[0m        4.2254        1.9057\n",
      "    132                    -1.7135  -0.1771        \u001b[35m0.1655\u001b[0m        4.1727        1.7389\n",
      "    133                    -1.7003  -0.1623        \u001b[35m0.1617\u001b[0m        4.1204        2.4797\n",
      "    134                    -1.6870  -0.1477        \u001b[35m0.1579\u001b[0m        4.0686        1.6302\n",
      "    135                    -1.6736  -0.1332        \u001b[35m0.1541\u001b[0m        4.0172        1.6024\n",
      "    136                    \u001b[36m-1.6600\u001b[0m  -0.1188        \u001b[35m0.1506\u001b[0m        3.9659        1.7402\n",
      "    137                    \u001b[36m-1.6463\u001b[0m  -0.1042        \u001b[35m0.1472\u001b[0m        3.9145        1.6412\n",
      "    138                    \u001b[36m-1.6324\u001b[0m  -0.0896        \u001b[35m0.1441\u001b[0m        3.8626        1.5912\n",
      "    139                    \u001b[36m-1.6182\u001b[0m  -0.0748        \u001b[35m0.1413\u001b[0m        3.8101        2.0096\n",
      "    140                    \u001b[36m-1.6037\u001b[0m  -0.0597        \u001b[35m0.1387\u001b[0m        3.7566        5.4204\n",
      "    141                    \u001b[36m-1.5889\u001b[0m  -0.0443        \u001b[35m0.1364\u001b[0m        3.7020        4.4901\n",
      "    142                    \u001b[36m-1.5737\u001b[0m  -0.0285        \u001b[35m0.1343\u001b[0m        3.6461        2.6063\n",
      "    143                    \u001b[36m-1.5581\u001b[0m  -0.0124        \u001b[35m0.1323\u001b[0m        3.5887        2.4424\n",
      "    144                    \u001b[36m-1.5420\u001b[0m  \u001b[32m0.0042\u001b[0m        \u001b[35m0.1304\u001b[0m        \u001b[31m3.5299\u001b[0m     +  1.6280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    145                    \u001b[36m-1.5254\u001b[0m  \u001b[32m0.0213\u001b[0m        \u001b[35m0.1287\u001b[0m        \u001b[31m3.4695\u001b[0m     +  2.9307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    146                    \u001b[36m-1.5083\u001b[0m  \u001b[32m0.0388\u001b[0m        \u001b[35m0.1270\u001b[0m        \u001b[31m3.4074\u001b[0m     +  2.6738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    147                    \u001b[36m-1.4907\u001b[0m  \u001b[32m0.0568\u001b[0m        \u001b[35m0.1254\u001b[0m        \u001b[31m3.3436\u001b[0m     +  3.5827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    148                    \u001b[36m-1.4725\u001b[0m  \u001b[32m0.0753\u001b[0m        \u001b[35m0.1239\u001b[0m        \u001b[31m3.2782\u001b[0m     +  3.3243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    149                    \u001b[36m-1.4539\u001b[0m  \u001b[32m0.0942\u001b[0m        \u001b[35m0.1224\u001b[0m        \u001b[31m3.2110\u001b[0m     +  3.2159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    150                    \u001b[36m-1.4348\u001b[0m  \u001b[32m0.1136\u001b[0m        \u001b[35m0.1210\u001b[0m        \u001b[31m3.1421\u001b[0m     +  8.8647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    151                    \u001b[36m-1.4153\u001b[0m  \u001b[32m0.1335\u001b[0m        \u001b[35m0.1196\u001b[0m        \u001b[31m3.0716\u001b[0m     +  4.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    152                    \u001b[36m-1.3953\u001b[0m  \u001b[32m0.1538\u001b[0m        \u001b[35m0.1183\u001b[0m        \u001b[31m2.9998\u001b[0m     +  2.5906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    153                    \u001b[36m-1.3753\u001b[0m  \u001b[32m0.1744\u001b[0m        \u001b[35m0.1170\u001b[0m        \u001b[31m2.9267\u001b[0m     +  5.6567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    154                    \u001b[36m-1.3551\u001b[0m  \u001b[32m0.1953\u001b[0m        \u001b[35m0.1157\u001b[0m        \u001b[31m2.8528\u001b[0m     +  2.3637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    155                    \u001b[36m-1.3346\u001b[0m  \u001b[32m0.2163\u001b[0m        \u001b[35m0.1144\u001b[0m        \u001b[31m2.7782\u001b[0m     +  4.2274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    156                    \u001b[36m-1.3140\u001b[0m  \u001b[32m0.2374\u001b[0m        \u001b[35m0.1132\u001b[0m        \u001b[31m2.7035\u001b[0m     +  5.5291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    157                    \u001b[36m-1.2935\u001b[0m  \u001b[32m0.2584\u001b[0m        \u001b[35m0.1120\u001b[0m        \u001b[31m2.6289\u001b[0m     +  2.3942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    158                    \u001b[36m-1.2729\u001b[0m  \u001b[32m0.2793\u001b[0m        \u001b[35m0.1108\u001b[0m        \u001b[31m2.5549\u001b[0m     +  2.1221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    159                    \u001b[36m-1.2527\u001b[0m  \u001b[32m0.2998\u001b[0m        \u001b[35m0.1097\u001b[0m        \u001b[31m2.4820\u001b[0m     +  1.5744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    160                    \u001b[36m-1.2331\u001b[0m  \u001b[32m0.3200\u001b[0m        \u001b[35m0.1086\u001b[0m        \u001b[31m2.4106\u001b[0m     +  3.0172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    161                    \u001b[36m-1.2139\u001b[0m  \u001b[32m0.3396\u001b[0m        \u001b[35m0.1075\u001b[0m        \u001b[31m2.3412\u001b[0m     +  2.3278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    162                    \u001b[36m-1.1953\u001b[0m  \u001b[32m0.3585\u001b[0m        \u001b[35m0.1065\u001b[0m        \u001b[31m2.2741\u001b[0m     +  7.3936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    163                    \u001b[36m-1.1774\u001b[0m  \u001b[32m0.3767\u001b[0m        \u001b[35m0.1055\u001b[0m        \u001b[31m2.2096\u001b[0m     +  3.5170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    164                    \u001b[36m-1.1602\u001b[0m  \u001b[32m0.3941\u001b[0m        \u001b[35m0.1045\u001b[0m        \u001b[31m2.1479\u001b[0m     +  5.8734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    165                    \u001b[36m-1.1435\u001b[0m  \u001b[32m0.4107\u001b[0m        \u001b[35m0.1036\u001b[0m        \u001b[31m2.0891\u001b[0m     +  3.6509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    166                    \u001b[36m-1.1275\u001b[0m  \u001b[32m0.4265\u001b[0m        \u001b[35m0.1027\u001b[0m        \u001b[31m2.0332\u001b[0m     +  2.4599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    167                    \u001b[36m-1.1121\u001b[0m  \u001b[32m0.4415\u001b[0m        \u001b[35m0.1019\u001b[0m        \u001b[31m1.9800\u001b[0m     +  4.2995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    168                    \u001b[36m-1.0975\u001b[0m  \u001b[32m0.4557\u001b[0m        \u001b[35m0.1011\u001b[0m        \u001b[31m1.9294\u001b[0m     +  2.1871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    169                    \u001b[36m-1.0837\u001b[0m  \u001b[32m0.4693\u001b[0m        \u001b[35m0.1003\u001b[0m        \u001b[31m1.8813\u001b[0m     +  4.7267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    170                    \u001b[36m-1.0704\u001b[0m  \u001b[32m0.4822\u001b[0m        \u001b[35m0.0996\u001b[0m        \u001b[31m1.8355\u001b[0m     +  2.1853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    171                    \u001b[36m-1.0578\u001b[0m  \u001b[32m0.4946\u001b[0m        \u001b[35m0.0989\u001b[0m        \u001b[31m1.7918\u001b[0m     +  2.2063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    172                    \u001b[36m-1.0456\u001b[0m  \u001b[32m0.5064\u001b[0m        \u001b[35m0.0983\u001b[0m        \u001b[31m1.7499\u001b[0m     +  1.9819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    173                    \u001b[36m-1.0341\u001b[0m  \u001b[32m0.5177\u001b[0m        \u001b[35m0.0977\u001b[0m        \u001b[31m1.7098\u001b[0m     +  2.9338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    174                    \u001b[36m-1.0233\u001b[0m  \u001b[32m0.5286\u001b[0m        \u001b[35m0.0971\u001b[0m        \u001b[31m1.6712\u001b[0m     +  2.2288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    175                    \u001b[36m-1.0128\u001b[0m  \u001b[32m0.5391\u001b[0m        \u001b[35m0.0966\u001b[0m        \u001b[31m1.6340\u001b[0m     +  2.6458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    176                    \u001b[36m-1.0026\u001b[0m  \u001b[32m0.5492\u001b[0m        \u001b[35m0.0961\u001b[0m        \u001b[31m1.5979\u001b[0m     +  2.5601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    177                    \u001b[36m-0.9928\u001b[0m  \u001b[32m0.5591\u001b[0m        \u001b[35m0.0956\u001b[0m        \u001b[31m1.5630\u001b[0m     +  2.8733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    178                    \u001b[36m-0.9835\u001b[0m  \u001b[32m0.5686\u001b[0m        \u001b[35m0.0951\u001b[0m        \u001b[31m1.5291\u001b[0m     +  2.6862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    179                    \u001b[36m-0.9746\u001b[0m  \u001b[32m0.5779\u001b[0m        \u001b[35m0.0946\u001b[0m        \u001b[31m1.4962\u001b[0m     +  3.1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    180                    \u001b[36m-0.9658\u001b[0m  \u001b[32m0.5870\u001b[0m        \u001b[35m0.0942\u001b[0m        \u001b[31m1.4640\u001b[0m     +  3.2044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    181                    \u001b[36m-0.9571\u001b[0m  \u001b[32m0.5958\u001b[0m        \u001b[35m0.0937\u001b[0m        \u001b[31m1.4328\u001b[0m     +  4.0747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    182                    \u001b[36m-0.9483\u001b[0m  \u001b[32m0.6045\u001b[0m        \u001b[35m0.0932\u001b[0m        \u001b[31m1.4022\u001b[0m     +  2.2707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    183                    \u001b[36m-0.9399\u001b[0m  \u001b[32m0.6127\u001b[0m        \u001b[35m0.0928\u001b[0m        \u001b[31m1.3728\u001b[0m     +  3.1062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    184                    \u001b[36m-0.9312\u001b[0m  \u001b[32m0.6212\u001b[0m        \u001b[35m0.0923\u001b[0m        \u001b[31m1.3428\u001b[0m     +  3.1798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    185                    \u001b[36m-0.9232\u001b[0m  \u001b[32m0.6288\u001b[0m        \u001b[35m0.0918\u001b[0m        \u001b[31m1.3160\u001b[0m     +  2.4848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    186                    \u001b[36m-0.9141\u001b[0m  \u001b[32m0.6373\u001b[0m        \u001b[35m0.0912\u001b[0m        \u001b[31m1.2858\u001b[0m     +  2.7197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    187                    \u001b[36m-0.9062\u001b[0m  \u001b[32m0.6446\u001b[0m        \u001b[35m0.0909\u001b[0m        \u001b[31m1.2600\u001b[0m     +  2.6737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    188                    \u001b[36m-0.8975\u001b[0m  \u001b[32m0.6523\u001b[0m        \u001b[35m0.0903\u001b[0m        \u001b[31m1.2325\u001b[0m     +  2.6982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    189                    \u001b[36m-0.8892\u001b[0m  \u001b[32m0.6597\u001b[0m        \u001b[35m0.0898\u001b[0m        \u001b[31m1.2065\u001b[0m     +  2.4512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    190                    \u001b[36m-0.8807\u001b[0m  \u001b[32m0.6670\u001b[0m        \u001b[35m0.0892\u001b[0m        \u001b[31m1.1804\u001b[0m     +  2.7914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    191                    \u001b[36m-0.8723\u001b[0m  \u001b[32m0.6742\u001b[0m        \u001b[35m0.0887\u001b[0m        \u001b[31m1.1549\u001b[0m     +  2.7679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    192                    \u001b[36m-0.8640\u001b[0m  \u001b[32m0.6813\u001b[0m        \u001b[35m0.0881\u001b[0m        \u001b[31m1.1297\u001b[0m     +  2.2747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    193                    \u001b[36m-0.8557\u001b[0m  \u001b[32m0.6883\u001b[0m        \u001b[35m0.0876\u001b[0m        \u001b[31m1.1049\u001b[0m     +  2.7187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    194                    \u001b[36m-0.8473\u001b[0m  \u001b[32m0.6952\u001b[0m        \u001b[35m0.0870\u001b[0m        \u001b[31m1.0805\u001b[0m     +  2.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    195                    \u001b[36m-0.8389\u001b[0m  \u001b[32m0.7020\u001b[0m        \u001b[35m0.0864\u001b[0m        \u001b[31m1.0564\u001b[0m     +  3.1572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    196                    \u001b[36m-0.8304\u001b[0m  \u001b[32m0.7087\u001b[0m        \u001b[35m0.0858\u001b[0m        \u001b[31m1.0328\u001b[0m     +  2.4148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    197                    \u001b[36m-0.8219\u001b[0m  \u001b[32m0.7152\u001b[0m        \u001b[35m0.0852\u001b[0m        \u001b[31m1.0095\u001b[0m     +  3.6135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    198                    \u001b[36m-0.8135\u001b[0m  \u001b[32m0.7217\u001b[0m        \u001b[35m0.0846\u001b[0m        \u001b[31m0.9867\u001b[0m     +  8.5043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    199                    \u001b[36m-0.8051\u001b[0m  \u001b[32m0.7280\u001b[0m        \u001b[35m0.0840\u001b[0m        \u001b[31m0.9644\u001b[0m     +  2.8227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    200                    \u001b[36m-0.7969\u001b[0m  \u001b[32m0.7341\u001b[0m        \u001b[35m0.0834\u001b[0m        \u001b[31m0.9425\u001b[0m     +  2.5481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    201                    \u001b[36m-0.7887\u001b[0m  \u001b[32m0.7402\u001b[0m        \u001b[35m0.0827\u001b[0m        \u001b[31m0.9210\u001b[0m     +  2.8621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    202                    \u001b[36m-0.7805\u001b[0m  \u001b[32m0.7461\u001b[0m        \u001b[35m0.0821\u001b[0m        \u001b[31m0.9002\u001b[0m     +  2.7375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    203                    \u001b[36m-0.7724\u001b[0m  \u001b[32m0.7518\u001b[0m        \u001b[35m0.0814\u001b[0m        \u001b[31m0.8798\u001b[0m     +  2.4144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    204                    \u001b[36m-0.7643\u001b[0m  \u001b[32m0.7574\u001b[0m        \u001b[35m0.0807\u001b[0m        \u001b[31m0.8600\u001b[0m     +  2.8296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    205                    \u001b[36m-0.7562\u001b[0m  \u001b[32m0.7628\u001b[0m        \u001b[35m0.0800\u001b[0m        \u001b[31m0.8408\u001b[0m     +  3.0915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    206                    \u001b[36m-0.7483\u001b[0m  \u001b[32m0.7681\u001b[0m        \u001b[35m0.0793\u001b[0m        \u001b[31m0.8222\u001b[0m     +  2.2342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    207                    \u001b[36m-0.7405\u001b[0m  \u001b[32m0.7731\u001b[0m        \u001b[35m0.0786\u001b[0m        \u001b[31m0.8042\u001b[0m     +  3.0819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    208                    \u001b[36m-0.7328\u001b[0m  \u001b[32m0.7780\u001b[0m        \u001b[35m0.0779\u001b[0m        \u001b[31m0.7869\u001b[0m     +  3.7126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    209                    \u001b[36m-0.7253\u001b[0m  \u001b[32m0.7827\u001b[0m        \u001b[35m0.0772\u001b[0m        \u001b[31m0.7702\u001b[0m     +  2.3991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    210                    \u001b[36m-0.7180\u001b[0m  \u001b[32m0.7872\u001b[0m        \u001b[35m0.0764\u001b[0m        \u001b[31m0.7542\u001b[0m     +  4.6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    211                    \u001b[36m-0.7110\u001b[0m  \u001b[32m0.7916\u001b[0m        \u001b[35m0.0757\u001b[0m        \u001b[31m0.7389\u001b[0m     +  4.1303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    212                    \u001b[36m-0.7041\u001b[0m  \u001b[32m0.7957\u001b[0m        \u001b[35m0.0749\u001b[0m        \u001b[31m0.7243\u001b[0m     +  3.0447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    213                    \u001b[36m-0.6977\u001b[0m  \u001b[32m0.7996\u001b[0m        \u001b[35m0.0742\u001b[0m        \u001b[31m0.7103\u001b[0m     +  4.0447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    214                    \u001b[36m-0.6915\u001b[0m  \u001b[32m0.8034\u001b[0m        \u001b[35m0.0734\u001b[0m        \u001b[31m0.6970\u001b[0m     +  2.6332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    215                    \u001b[36m-0.6855\u001b[0m  \u001b[32m0.8069\u001b[0m        \u001b[35m0.0726\u001b[0m        \u001b[31m0.6845\u001b[0m     +  2.8879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    216                    \u001b[36m-0.6798\u001b[0m  \u001b[32m0.8103\u001b[0m        \u001b[35m0.0718\u001b[0m        \u001b[31m0.6725\u001b[0m     +  3.6780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    217                    \u001b[36m-0.6743\u001b[0m  \u001b[32m0.8135\u001b[0m        \u001b[35m0.0710\u001b[0m        \u001b[31m0.6612\u001b[0m     +  2.0819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    218                    \u001b[36m-0.6689\u001b[0m  \u001b[32m0.8165\u001b[0m        \u001b[35m0.0702\u001b[0m        \u001b[31m0.6504\u001b[0m     +  2.5970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    219                    \u001b[36m-0.6639\u001b[0m  \u001b[32m0.8193\u001b[0m        \u001b[35m0.0694\u001b[0m        \u001b[31m0.6405\u001b[0m     +  2.9635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    220                    \u001b[36m-0.6590\u001b[0m  \u001b[32m0.8221\u001b[0m        \u001b[35m0.0686\u001b[0m        \u001b[31m0.6308\u001b[0m     +  2.7122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    221                    \u001b[36m-0.6544\u001b[0m  \u001b[32m0.8245\u001b[0m        \u001b[35m0.0678\u001b[0m        \u001b[31m0.6221\u001b[0m     +  2.2285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    222                    \u001b[36m-0.6499\u001b[0m  \u001b[32m0.8270\u001b[0m        \u001b[35m0.0669\u001b[0m        \u001b[31m0.6134\u001b[0m     +  2.5493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    223                    \u001b[36m-0.6458\u001b[0m  \u001b[32m0.8291\u001b[0m        \u001b[35m0.0662\u001b[0m        \u001b[31m0.6059\u001b[0m     +  2.3156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    224                    \u001b[36m-0.6416\u001b[0m  \u001b[32m0.8313\u001b[0m        \u001b[35m0.0653\u001b[0m        \u001b[31m0.5981\u001b[0m     +  4.8136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    225                    \u001b[36m-0.6379\u001b[0m  \u001b[32m0.8331\u001b[0m        \u001b[35m0.0645\u001b[0m        \u001b[31m0.5915\u001b[0m     +  2.9760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    226                    \u001b[36m-0.6340\u001b[0m  \u001b[32m0.8350\u001b[0m        \u001b[35m0.0637\u001b[0m        \u001b[31m0.5848\u001b[0m     +  3.9020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    227                    \u001b[36m-0.6306\u001b[0m  \u001b[32m0.8367\u001b[0m        \u001b[35m0.0629\u001b[0m        \u001b[31m0.5789\u001b[0m     +  2.5149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    228                    \u001b[36m-0.6272\u001b[0m  \u001b[32m0.8383\u001b[0m        \u001b[35m0.0620\u001b[0m        \u001b[31m0.5731\u001b[0m     +  3.1034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    229                    \u001b[36m-0.6241\u001b[0m  \u001b[32m0.8398\u001b[0m        \u001b[35m0.0612\u001b[0m        \u001b[31m0.5680\u001b[0m     +  4.6184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    230                    \u001b[36m-0.6210\u001b[0m  \u001b[32m0.8412\u001b[0m        \u001b[35m0.0604\u001b[0m        \u001b[31m0.5630\u001b[0m     +  2.2606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    231                    \u001b[36m-0.6182\u001b[0m  \u001b[32m0.8424\u001b[0m        \u001b[35m0.0596\u001b[0m        \u001b[31m0.5585\u001b[0m     +  1.7561\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    232                    \u001b[36m-0.6155\u001b[0m  \u001b[32m0.8436\u001b[0m        \u001b[35m0.0588\u001b[0m        \u001b[31m0.5543\u001b[0m     +  2.8050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    233                    \u001b[36m-0.6130\u001b[0m  \u001b[32m0.8447\u001b[0m        \u001b[35m0.0580\u001b[0m        \u001b[31m0.5504\u001b[0m     +  2.7346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    234                    \u001b[36m-0.6106\u001b[0m  \u001b[32m0.8458\u001b[0m        \u001b[35m0.0572\u001b[0m        \u001b[31m0.5468\u001b[0m     +  8.7208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    235                    \u001b[36m-0.6084\u001b[0m  \u001b[32m0.8467\u001b[0m        \u001b[35m0.0564\u001b[0m        \u001b[31m0.5435\u001b[0m     +  4.6408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    236                    \u001b[36m-0.6063\u001b[0m  \u001b[32m0.8475\u001b[0m        \u001b[35m0.0556\u001b[0m        \u001b[31m0.5405\u001b[0m     +  2.5927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    237                    \u001b[36m-0.6044\u001b[0m  \u001b[32m0.8483\u001b[0m        \u001b[35m0.0548\u001b[0m        \u001b[31m0.5378\u001b[0m     +  2.1118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    238                    \u001b[36m-0.6026\u001b[0m  \u001b[32m0.8490\u001b[0m        \u001b[35m0.0541\u001b[0m        \u001b[31m0.5353\u001b[0m     +  2.6754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    239                    \u001b[36m-0.6010\u001b[0m  \u001b[32m0.8496\u001b[0m        \u001b[35m0.0533\u001b[0m        \u001b[31m0.5331\u001b[0m     +  3.0020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    240                    \u001b[36m-0.5995\u001b[0m  \u001b[32m0.8502\u001b[0m        \u001b[35m0.0526\u001b[0m        \u001b[31m0.5312\u001b[0m     +  4.1734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    241                    \u001b[36m-0.5982\u001b[0m  \u001b[32m0.8506\u001b[0m        \u001b[35m0.0519\u001b[0m        \u001b[31m0.5295\u001b[0m     +  3.0348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    242                    \u001b[36m-0.5971\u001b[0m  \u001b[32m0.8510\u001b[0m        \u001b[35m0.0511\u001b[0m        \u001b[31m0.5281\u001b[0m     +  2.6312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    243                    \u001b[36m-0.5960\u001b[0m  \u001b[32m0.8514\u001b[0m        \u001b[35m0.0504\u001b[0m        \u001b[31m0.5269\u001b[0m     +  3.0408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    244                    \u001b[36m-0.5952\u001b[0m  \u001b[32m0.8516\u001b[0m        \u001b[35m0.0497\u001b[0m        \u001b[31m0.5260\u001b[0m     +  2.0105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    245                    \u001b[36m-0.5944\u001b[0m  \u001b[32m0.8518\u001b[0m        \u001b[35m0.0491\u001b[0m        \u001b[31m0.5253\u001b[0m     +  2.8620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    246                    \u001b[36m-0.5939\u001b[0m  \u001b[32m0.8520\u001b[0m        \u001b[35m0.0484\u001b[0m        \u001b[31m0.5248\u001b[0m     +  2.3040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    247                    \u001b[36m-0.5934\u001b[0m  \u001b[32m0.8520\u001b[0m        \u001b[35m0.0478\u001b[0m        \u001b[31m0.5246\u001b[0m     +  2.6463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    248                    \u001b[36m-0.5931\u001b[0m  0.8520        \u001b[35m0.0471\u001b[0m        0.5246        2.6172\n",
      "    249                    \u001b[36m-0.5930\u001b[0m  0.8519        \u001b[35m0.0465\u001b[0m        0.5249        2.6995\n",
      "    250                    \u001b[36m-0.5929\u001b[0m  0.8518        \u001b[35m0.0459\u001b[0m        0.5253        2.6020\n",
      "    251                    -0.5930  0.8516        \u001b[35m0.0454\u001b[0m        0.5260        2.6231\n",
      "    252                    -0.5932  0.8514        \u001b[35m0.0448\u001b[0m        0.5268        2.7238\n",
      "    253                    -0.5935  0.8511        \u001b[35m0.0442\u001b[0m        0.5279        2.2650\n",
      "    254                    -0.5938  0.8508        \u001b[35m0.0437\u001b[0m        0.5289        2.2327\n",
      "    255                    -0.5944  0.8504        \u001b[35m0.0432\u001b[0m        0.5304        2.1889\n",
      "    256                    -0.5948  0.8500        \u001b[35m0.0426\u001b[0m        0.5317        2.6439\n",
      "    257                    -0.5955  0.8495        \u001b[35m0.0422\u001b[0m        0.5334        2.8881\n",
      "    258                    -0.5962  0.8491        \u001b[35m0.0417\u001b[0m        0.5350        2.2513\n",
      "    259                    -0.5969  0.8486        \u001b[35m0.0412\u001b[0m        0.5368        1.9639\n",
      "    260                    -0.5977  0.8481        \u001b[35m0.0407\u001b[0m        0.5386        1.9870\n",
      "    261                    -0.5986  0.8475        \u001b[35m0.0403\u001b[0m        0.5406        2.2175\n",
      "    262                    -0.5994  0.8470        \u001b[35m0.0398\u001b[0m        0.5425        2.0397\n",
      "    263                    -0.6004  0.8464        \u001b[35m0.0394\u001b[0m        0.5446        1.5768\n",
      "    264                    -0.6014  0.8458        \u001b[35m0.0389\u001b[0m        0.5467        1.6309\n",
      "    265                    -0.6024  0.8452        \u001b[35m0.0385\u001b[0m        0.5489        1.6099\n",
      "    266                    -0.6035  0.8445        \u001b[35m0.0381\u001b[0m        0.5512        1.6286\n",
      "    267                    -0.6046  0.8438        \u001b[35m0.0377\u001b[0m        0.5536        2.1442\n",
      "    268                    -0.6058  0.8431        \u001b[35m0.0373\u001b[0m        0.5561        4.0041\n",
      "    269                    -0.6071  0.8424        \u001b[35m0.0370\u001b[0m        0.5587        1.9880\n",
      "    270                    -0.6085  0.8416        \u001b[35m0.0366\u001b[0m        0.5614        1.6116\n",
      "    271                    -0.6099  0.8408        \u001b[35m0.0363\u001b[0m        0.5643        3.4050\n",
      "    272                    -0.6115  0.8400        \u001b[35m0.0359\u001b[0m        0.5673        2.8505\n",
      "    273                    -0.6132  0.8391        \u001b[35m0.0357\u001b[0m        0.5705        2.6424\n",
      "    274                    -0.6150  0.8381        \u001b[35m0.0354\u001b[0m        0.5739        2.9109\n",
      "    275                    -0.6168  0.8371        \u001b[35m0.0351\u001b[0m        0.5774        2.4367\n",
      "    276                    -0.6188  0.8361        \u001b[35m0.0349\u001b[0m        0.5811        3.1703\n",
      "    277                    -0.6209  0.8350        \u001b[35m0.0347\u001b[0m        0.5849        3.4500\n",
      "    278                    -0.6231  0.8339        \u001b[35m0.0346\u001b[0m        0.5890        3.4000\n",
      "    279                    -0.6254  0.8327        \u001b[35m0.0344\u001b[0m        0.5931        3.5656\n",
      "    280                    -0.6277  0.8315        \u001b[35m0.0344\u001b[0m        0.5974        2.6080\n",
      "    281                    -0.6302  0.8302        \u001b[35m0.0343\u001b[0m        0.6018        2.4009\n",
      "    282                    -0.6327  0.8290        \u001b[35m0.0343\u001b[0m        0.6063        2.2636\n",
      "    283                    -0.6353  0.8277        0.0343        0.6108        2.5393\n",
      "    284                    -0.6378  0.8265        0.0344        0.6152        2.1046\n",
      "    285                    -0.6404  0.8252        0.0345        0.6195        2.8813\n",
      "    286                    -0.6428  0.8241        0.0347        0.6237        3.7623\n",
      "    287                    -0.6452  0.8230        0.0349        0.6275        2.6813\n",
      "    288                    -0.6474  0.8220        0.0351        0.6310        3.2104\n",
      "    289                    -0.6493  0.8212        0.0353        0.6339        2.8642\n",
      "    290                    -0.6509  0.8205        0.0356        0.6362        2.7911\n",
      "    291                    -0.6521  0.8201        0.0358        0.6378        4.8806\n",
      "    292                    -0.6529  0.8199        0.0361        0.6384        3.1653\n",
      "    293                    -0.6531  0.8200        0.0363        0.6380        2.3708\n",
      "    294                    -0.6527  0.8204        0.0364        0.6365        2.8292\n",
      "    295                    -0.6517  0.8212        0.0365        0.6338        2.7290\n",
      "    296                    -0.6499  0.8223        0.0365        0.6299        2.8068\n",
      "    297                    -0.6473  0.8238        0.0364        0.6246        2.2078\n",
      "    298                    -0.6440  0.8257        0.0361        0.6180        2.3102\n",
      "    299                    -0.6397  0.8279        0.0357        0.6101        3.1496\n",
      "    300                    -0.6346  0.8305        0.0352        0.6008        4.8553\n",
      "    301                    -0.6286  0.8336        0.0344        0.5901        1.9647\n",
      "    302                    -0.6218  0.8370        \u001b[35m0.0335\u001b[0m        0.5779        1.9941\n",
      "    303                    -0.6138  0.8408        \u001b[35m0.0325\u001b[0m        0.5642        1.9456\n",
      "    304                    -0.6048  0.8452        \u001b[35m0.0313\u001b[0m        0.5489        1.7574\n",
      "    305                    -0.5947  0.8499        \u001b[35m0.0300\u001b[0m        0.5320        1.6392\n",
      "    306                    \u001b[36m-0.5834\u001b[0m  \u001b[32m0.8551\u001b[0m        \u001b[35m0.0288\u001b[0m        \u001b[31m0.5138\u001b[0m     +  1.7491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    307                    \u001b[36m-0.5714\u001b[0m  \u001b[32m0.8604\u001b[0m        \u001b[35m0.0276\u001b[0m        \u001b[31m0.4948\u001b[0m     +  1.9035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    308                    \u001b[36m-0.5592\u001b[0m  \u001b[32m0.8656\u001b[0m        \u001b[35m0.0266\u001b[0m        \u001b[31m0.4763\u001b[0m     +  3.7527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    309                    \u001b[36m-0.5480\u001b[0m  \u001b[32m0.8701\u001b[0m        \u001b[35m0.0259\u001b[0m        \u001b[31m0.4604\u001b[0m     +  3.3079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    310                    \u001b[36m-0.5394\u001b[0m  \u001b[32m0.8734\u001b[0m        \u001b[35m0.0256\u001b[0m        \u001b[31m0.4489\u001b[0m     +  3.6421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    311                    \u001b[36m-0.5344\u001b[0m  \u001b[32m0.8750\u001b[0m        \u001b[35m0.0255\u001b[0m        \u001b[31m0.4432\u001b[0m     +  2.6552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    312                    \u001b[36m-0.5331\u001b[0m  \u001b[32m0.8750\u001b[0m        0.0256        \u001b[31m0.4431\u001b[0m     +  2.1433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    313                    -0.5348  0.8737        0.0257        0.4479        2.6440\n",
      "    314                    -0.5389  0.8712        0.0258        0.4566        1.9430\n",
      "    315                    -0.5449  0.8677        0.0258        0.4689        1.5743\n",
      "    316                    -0.5525  0.8631        0.0256        0.4852        1.9964\n",
      "    317                    -0.5621  0.8570        \u001b[35m0.0255\u001b[0m        0.5071        2.0458\n",
      "    318                    -0.5760  0.8485        \u001b[35m0.0254\u001b[0m        0.5371        1.9788\n",
      "    319                    -0.5969  0.8362        0.0254        0.5806        2.3991\n",
      "    320                    -0.6284  0.8174        0.0258        0.6472        1.9810\n",
      "    321                    -0.6792  0.7862        0.0269        0.7578        2.7907\n",
      "    322                    -0.7659  0.7294        0.0296        0.9593        3.1143\n",
      "    323                    -0.9211  0.6192        0.0358        1.3498        3.2305\n",
      "    324                    -1.1121  0.4615        0.0492        1.9089        2.2007\n",
      "    325                    -1.2878  0.3003        0.0677        2.4805        2.6155\n",
      "    326                    -1.4167  0.1757        0.0714        2.9223        2.6659\n",
      "    327                    -1.4660  0.1391        0.0775        3.0519        2.6627\n",
      "    328                    -1.3709  0.2241        0.0896        2.7506        3.7952\n",
      "    329                    -1.1995  0.3644        0.1253        2.2532        2.5327\n",
      "    330                    -1.0603  0.4744        0.1551        1.8634        2.6610\n",
      "    331                    -0.9809  0.5501        0.1521        1.5947        2.6754\n",
      "    332                    -0.9301  0.6067        0.1305        1.3941        3.3865\n",
      "    333                    -0.8563  0.6805        0.0916        1.1326        2.3467\n",
      "    334                    -0.7509  0.7641        0.0598        0.8362        2.4786\n",
      "    335                    -0.6356  0.8338        0.0443        0.5892        2.5708\n",
      "    336                    -0.5590  0.8697        0.0429        0.4618        7.3742\n",
      "    337                    -0.5394  \u001b[32m0.8771\u001b[0m        0.0465        \u001b[31m0.4356\u001b[0m     +  2.7065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    338                    \u001b[36m-0.5313\u001b[0m  \u001b[32m0.8798\u001b[0m        0.0436        \u001b[31m0.4261\u001b[0m     +  6.8136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    339                    \u001b[36m-0.5246\u001b[0m  \u001b[32m0.8819\u001b[0m        0.0401        \u001b[31m0.4188\u001b[0m     +  3.0112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    340                    \u001b[36m-0.5195\u001b[0m  \u001b[32m0.8833\u001b[0m        0.0373        \u001b[31m0.4138\u001b[0m     +  2.9849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    341                    \u001b[36m-0.5151\u001b[0m  \u001b[32m0.8844\u001b[0m        0.0349        \u001b[31m0.4098\u001b[0m     +  2.6770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    342                    \u001b[36m-0.5111\u001b[0m  \u001b[32m0.8853\u001b[0m        0.0328        \u001b[31m0.4064\u001b[0m     +  2.8915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    343                    \u001b[36m-0.5075\u001b[0m  \u001b[32m0.8861\u001b[0m        0.0311        \u001b[31m0.4038\u001b[0m     +  3.0894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    344                    \u001b[36m-0.5050\u001b[0m  \u001b[32m0.8866\u001b[0m        0.0297        \u001b[31m0.4021\u001b[0m     +  2.3491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    345                    \u001b[36m-0.5036\u001b[0m  \u001b[32m0.8867\u001b[0m        0.0286        \u001b[31m0.4016\u001b[0m     +  2.9328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    346                    \u001b[36m-0.5032\u001b[0m  0.8865        0.0277        0.4023        2.3028\n",
      "    347                    -0.5037  0.8861        0.0269        0.4038        1.7697\n",
      "    348                    -0.5048  0.8854        0.0260        0.4062        1.7396\n",
      "    349                    -0.5064  0.8845        \u001b[35m0.0252\u001b[0m        0.4095        2.8681\n",
      "    350                    -0.5086  0.8832        \u001b[35m0.0244\u001b[0m        0.4139        2.9859\n",
      "    351                    -0.5116  0.8816        \u001b[35m0.0237\u001b[0m        0.4196        2.5455\n",
      "    352                    -0.5156  0.8795        \u001b[35m0.0230\u001b[0m        0.4270        2.5289\n",
      "    353                    -0.5207  0.8768        \u001b[35m0.0224\u001b[0m        0.4367        2.1662\n",
      "    354                    -0.5276  0.8732        \u001b[35m0.0220\u001b[0m        0.4493        2.1928\n",
      "    355                    -0.5365  0.8686        \u001b[35m0.0216\u001b[0m        0.4657        1.9032\n",
      "    356                    -0.5478  0.8627        \u001b[35m0.0215\u001b[0m        0.4869        2.2961\n",
      "    357                    -0.5626  0.8550        0.0215        0.5141        3.1008\n",
      "    358                    -0.5807  0.8452        0.0218        0.5486        2.6097\n",
      "    359                    -0.6025  0.8331        0.0223        0.5915        2.8018\n",
      "    360                    -0.6281  0.8183        0.0229        0.6440        2.2547\n",
      "    361                    -0.6578  0.8005        0.0237        0.7072        1.9908\n",
      "    362                    -0.6916  0.7795        0.0247        0.7818        2.7251\n",
      "    363                    -0.7286  0.7553        0.0256        0.8674        3.0093\n",
      "    364                    -0.7654  0.7298        0.0262        0.9580        2.7320\n",
      "    365                    -0.7960  0.7074        0.0263        1.0372        2.5059\n",
      "    366                    -0.8118  0.6955        0.0254        1.0796        3.3677\n",
      "    367                    -0.8032  0.7021        0.0237        1.0561        2.4812\n",
      "    368                    -0.7651  0.7319        0.0216        0.9503        2.7595\n",
      "    369                    -0.7053  0.7775        \u001b[35m0.0193\u001b[0m        0.7887        2.6473\n",
      "    370                    -0.6433  0.8203        \u001b[35m0.0179\u001b[0m        0.6370        3.1609\n",
      "    371                    -0.6000  0.8471        0.0180        0.5420        2.7174\n",
      "    372                    -0.5752  0.8593        0.0189        0.4988        4.6551\n",
      "    373                    -0.5667  0.8624        0.0198        0.4879        4.2414\n",
      "    374                    -0.5700  0.8600        0.0205        0.4963        3.1185\n",
      "    375                    -0.5840  0.8527        0.0212        0.5222        2.4840\n",
      "    376                    -0.6080  0.8398        0.0224        0.5680        2.9185\n",
      "    377                    -0.6405  0.8194        0.0250        0.6404        2.7514\n",
      "    378                    -0.6948  0.7812        0.0308        0.7756        4.0084\n",
      "    379                    -0.8013  0.7152        0.0323        1.0098        4.6886\n",
      "    380                    -0.8030  0.7078        0.0263        1.0357        2.7242\n",
      "    381                    -0.7394  0.7503        \u001b[35m0.0170\u001b[0m        0.8850        3.1331\n",
      "    382                    -0.6422  0.8122        \u001b[35m0.0147\u001b[0m        0.6656        2.9764\n",
      "    383                    -0.5716  0.8555        0.0171        0.5124        2.5315\n",
      "    384                    -0.5509  0.8680        0.0216        0.4678        2.8462\n",
      "    385                    -0.5529  0.8675        0.0246        0.4699        3.0425\n",
      "    386                    -0.5597  0.8643        0.0252        0.4809        2.0602\n",
      "    387                    -0.5746  0.8579        0.0257        0.5038        2.4948\n",
      "    388                    -0.6008  0.8463        0.0267        0.5450        2.9913\n",
      "    389                    -0.6379  0.8283        0.0292        0.6086        5.5900\n",
      "    390                    -0.6780  0.8050        0.0349        0.6914        2.5309\n",
      "    391                    -0.7079  0.7801        0.0446        0.7795        2.5906\n",
      "    392                    -0.7253  0.7616        0.0519        0.8451        4.8413\n",
      "    393                    -0.7275  0.7598        0.0469        0.8516        2.5349\n",
      "    394                    -0.7086  0.7765        0.0359        0.7922        2.0256\n",
      "    395                    -0.6780  0.7996        0.0281        0.7104        2.5483\n",
      "    396                    -0.6456  0.8196        0.0241        0.6393        2.7014\n",
      "    397                    -0.6180  0.8344        0.0222        0.5870        2.1558\n",
      "    398                    -0.5973  0.8443        0.0213        0.5519        2.0000\n",
      "    399                    -0.5822  0.8509        0.0206        0.5287        2.1537\n",
      "    400                    -0.5701  0.8555        0.0197        0.5122        3.0093\n",
      "    401                    -0.5596  0.8590        0.0188        0.5000        2.7831\n",
      "    402                    -0.5519  0.8608        0.0179        0.4935        2.1124\n",
      "    403                    -0.5542  0.8590        0.0174        0.4999        2.2217\n",
      "    404                    -0.5717  0.8502        0.0177        0.5312        2.0772\n",
      "    405                    -0.6044  0.8331        0.0190        0.5916        2.0522\n",
      "    406                    -0.6431  0.8115        0.0215        0.6683        2.6112\n",
      "    407                    -0.6807  0.7888        0.0244        0.7487        2.5059\n",
      "    408                    -0.7147  0.7672        0.0274        0.8252        2.5613\n",
      "    409                    -0.7443  0.7474        0.0305        0.8953        2.4596\n",
      "    410                    -0.7712  0.7287        0.0326        0.9618        3.1205\n",
      "    411                    -0.7943  0.7119        0.0326        1.0214        4.0860\n",
      "    412                    -0.8068  0.7023        0.0300        1.0555        2.4039\n",
      "    413                    -0.7992  0.7072        0.0262        1.0379        2.7003\n",
      "    414                    -0.7658  0.7311        0.0224        0.9531        2.0118\n",
      "    415                    -0.7101  0.7704        0.0189        0.8140        1.6246\n",
      "    416                    -0.6484  0.8122        0.0165        0.6656        1.6722\n",
      "    417                    -0.6015  0.8431        0.0160        0.5560        1.6022\n",
      "    418                    -0.5723  0.8584        0.0175        0.5020        1.6005\n",
      "    419                    -0.5626  0.8619        0.0202        0.4895        1.8063\n",
      "    420                    -0.5656  0.8598        0.0228        0.4969        1.5967\n",
      "    421                    -0.5802  0.8534        0.0251        0.5195        1.8651\n",
      "    422                    -0.6043  0.8429        0.0273        0.5570        1.6220\n",
      "    423                    -0.6344  0.8288        0.0296        0.6069        1.6067\n",
      "    424                    -0.6606  0.8134        0.0336        0.6615        2.7165\n",
      "    425                    -0.6736  0.7999        0.0373        0.7095        2.7022\n",
      "    426                    -0.6804  0.7891        0.0361        0.7478        2.9648\n",
      "    427                    -0.6781  0.7900        0.0275        0.7443        2.4891\n",
      "    428                    -0.6407  0.8126        0.0180        0.6645        2.3558\n",
      "    429                    -0.5903  0.8405        \u001b[35m0.0139\u001b[0m        0.5653        2.6928\n",
      "    430                    -0.5566  0.8583        0.0151        0.5024        2.8304\n",
      "    431                    -0.5436  0.8651        0.0174        0.4782        4.6721\n",
      "    432                    -0.5412  0.8669        0.0182        0.4718        2.8135\n",
      "    433                    -0.5431  0.8666        0.0180        0.4728        2.5630\n",
      "    434                    -0.5464  0.8653        0.0174        0.4773        2.5164\n",
      "    435                    -0.5500  0.8635        0.0166        0.4840        2.9724\n",
      "    436                    -0.5539  0.8608        0.0156        0.4935        2.3849\n",
      "    437                    -0.5615  0.8559        0.0147        0.5108        2.3663\n",
      "    438                    -0.5883  0.8419        0.0145        0.5606        1.9686\n",
      "    439                    -0.6914  0.7904        0.0164        0.7429        2.0409\n",
      "    440                    -0.8536  0.6950        0.0218        1.0813        1.9148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    441                    -1.0029  0.5866        0.0336        1.4656        3.0728\n",
      "    442                    -1.0646  0.5302        0.0503        1.6654        3.0357\n",
      "    443                    -1.0338  0.5424        0.0616        1.6223        3.5262\n",
      "    444                    -0.9419  0.6041        0.0461        1.4035        2.2696\n",
      "    445                    -0.7789  0.7207        0.0235        0.9900        2.9210\n",
      "    446                    -0.6543  0.8125        0.0169        0.6645        2.6691\n",
      "    447                    -0.6028  0.8430        0.0219        0.5567        2.8409\n",
      "    448                    -0.5950  0.8462        0.0260        0.5452        2.7441\n",
      "    449                    -0.6051  0.8406        0.0281        0.5652        2.8435\n",
      "    450                    -0.6153  0.8341        0.0285        0.5883        4.3698\n",
      "    451                    -0.6175  0.8289        0.0280        0.6064        3.1674\n",
      "    452                    -0.6175  0.8263        0.0249        0.6158        3.6739\n",
      "    453                    -0.5937  0.8392        0.0197        0.5699        4.0606\n",
      "    454                    -0.5711  0.8517        0.0180        0.5255        4.4126\n",
      "    455                    -0.5649  0.8556        0.0192        0.5121        2.7611\n",
      "    456                    -0.5668  0.8552        0.0202        0.5132        2.5110\n",
      "    457                    -0.5710  0.8536        0.0205        0.5191        2.4377\n",
      "    458                    -0.5756  0.8514        0.0204        0.5267        3.6137\n",
      "    459                    -0.5798  0.8491        0.0200        0.5351        2.6606\n",
      "    460                    -0.5830  0.8466        0.0194        0.5438        2.0115\n",
      "    461                    -0.5849  0.8441        0.0184        0.5527        2.3403\n",
      "    462                    -0.5889  0.8407        0.0173        0.5646        2.3009\n",
      "    463                    -0.6018  0.8345        0.0160        0.5866        2.3696\n",
      "    464                    -0.6126  0.8303        0.0148        0.6015        5.4236\n",
      "    465                    -0.6001  0.8369        0.0153        0.5780        5.6505\n",
      "    466                    -0.5743  0.8497        0.0176        0.5329        2.5424\n",
      "    467                    -0.5438  0.8645        0.0199        0.4803        2.7641\n",
      "    468                    -0.5255  0.8741        0.0218        0.4462        3.1749\n",
      "    469                    -0.5285  0.8742        0.0230        0.4458        7.9660\n",
      "    470                    -0.5364  0.8709        0.0231        0.4578        2.7969\n",
      "    471                    -0.5414  0.8681        0.0221        0.4675        2.6924\n",
      "    472                    -0.5486  0.8646        0.0214        0.4799        2.5128\n",
      "    473                    -0.5619  0.8587        0.0215        0.5009        2.2255\n",
      "    474                    -0.5830  0.8491        0.0222        0.5348        3.7880\n",
      "    475                    -0.6116  0.8352        0.0241        0.5841        2.8278\n",
      "    476                    -0.6431  0.8179        0.0280        0.6456        3.4945\n",
      "    477                    -0.6672  0.8001        0.0341        0.7087        2.1113\n",
      "    478                    -0.6944  0.7776        0.0399        0.7883        3.2042\n",
      "    479                    -0.7873  0.7270        0.0335        0.9678        8.2530\n",
      "    480                    -0.7676  0.7366        0.0226        0.9338        2.7874\n",
      "    481                    -0.7354  0.7584        0.0160        0.8564        2.7177\n",
      "    482                    -0.7063  0.7752        0.0221        0.7968        2.8065\n",
      "    483                    -0.6816  0.7881        0.0281        0.7512        2.5474\n",
      "    484                    -0.6520  0.8043        0.0292        0.6938        3.0949\n",
      "    485                    -0.6162  0.8250        0.0266        0.6202        2.8323\n",
      "    486                    -0.5787  0.8467        0.0227        0.5436        2.7178\n",
      "    487                    -0.5488  0.8630        0.0198        0.4856        1.8326\n",
      "    488                    -0.5341  0.8705        0.0185        0.4589        1.6150\n",
      "    489                    -0.5323  0.8710        0.0179        0.4573        4.9023\n",
      "    490                    -0.5366  0.8683        0.0172        0.4669        2.2107\n",
      "    491                    -0.5435  0.8642        0.0163        0.4815        4.7608\n",
      "    492                    -0.5565  0.8575        0.0156        0.5053        2.4701\n",
      "    493                    -0.6030  0.8353        0.0166        0.5838        1.7339\n",
      "    494                    -0.7503  0.7648        0.0181        0.8336        2.1980\n",
      "    495                    -0.8152  0.7222        0.0266        0.9848        2.0234\n",
      "    496                    -0.8194  0.7120        0.0334        1.0209        5.3894\n",
      "    497                    -0.7636  0.7383        0.0374        0.9277        3.6398\n",
      "    498                    -0.6936  0.7788        0.0330        0.7843        2.9597\n",
      "    499                    -0.6231  0.8234        0.0222        0.6261        2.6038\n",
      "    500                    -0.5705  0.8550        0.0171        0.5140        2.5898\n",
      "    501                    -0.5453  0.8664        0.0182        0.4736        2.5751\n",
      "    502                    -0.5445  0.8653        0.0210        0.4775        3.0815\n",
      "    503                    -0.5553  0.8602        0.0219        0.4954        2.4089\n",
      "    504                    -0.5694  0.8536        0.0219        0.5189        2.5390\n",
      "    505                    -0.5833  0.8464        0.0217        0.5446        1.9877\n",
      "    506                    -0.5926  0.8385        0.0218        0.5725        1.7232\n",
      "    507                    -0.6357  0.8173        0.0211        0.6477        1.6585\n",
      "    508                    -0.6566  0.8083        0.0177        0.6797        1.6668\n",
      "    509                    -0.6150  0.8294        0.0157        0.6047        2.1284\n",
      "    510                    -0.5641  0.8544        0.0172        0.5161        3.0642\n",
      "    511                    -0.5284  0.8713        0.0186        0.4563        3.2236\n",
      "    512                    -0.5245  0.8735        0.0194        0.4484        2.3416\n",
      "    513                    -0.5368  0.8680        0.0209        0.4680        2.1543\n",
      "    514                    -0.5481  0.8626        0.0217        0.4871        2.6532\n",
      "    515                    -0.5624  0.8565        0.0225        0.5087        2.6651\n",
      "    516                    -0.5828  0.8479        0.0237        0.5393        3.0873\n",
      "    517                    -0.6084  0.8361        0.0253        0.5811        3.7451\n",
      "    518                    -0.6337  0.8226        0.0281        0.6290        2.6614\n",
      "    519                    -0.6494  0.8106        0.0318        0.6713        5.2215\n",
      "    520                    -0.6574  0.7997        0.0344        0.7100        2.2143\n",
      "    521                    -0.7031  0.7765        0.0299        0.7924        2.9618\n",
      "    522                    -0.6723  0.7941        0.0195        0.7301        3.1065\n",
      "    523                    -0.6149  0.8264        \u001b[35m0.0132\u001b[0m        0.6156        3.7611\n",
      "    524                    -0.5671  0.8513        0.0163        0.5273        3.0115\n",
      "    525                    -0.5370  0.8662        0.0191        0.4743        2.1605\n",
      "    526                    -0.5255  0.8725        0.0193        0.4518        3.8439\n",
      "    527                    -0.5314  0.8705        0.0189        0.4590        2.4256\n",
      "    528                    -0.5430  0.8653        0.0185        0.4776        4.1283\n",
      "    529                    -0.5542  0.8598        0.0182        0.4969        4.9048\n",
      "    530                    -0.5661  0.8540        0.0182        0.5176        2.7041\n",
      "    531                    -0.5790  0.8475        0.0185        0.5405        3.7537\n",
      "    532                    -0.5920  0.8404        0.0189        0.5657        2.6821\n",
      "    533                    -0.6030  0.8329        0.0193        0.5925        2.5692\n",
      "    534                    -0.6140  0.8244        0.0198        0.6225        2.7372\n",
      "    535                    -0.6497  0.8058        0.0216        0.6884        2.8031\n",
      "    536                    -0.8157  0.7245        0.0206        0.9766        2.6341\n",
      "    537                    -0.8617  0.6922        0.0287        1.0912        1.7520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    538                    -0.8925  0.6685        0.0349        1.1752        1.6820\n",
      "    539                    -0.8456  0.6911        0.0437        1.0951        3.5848\n",
      "    540                    -0.7657  0.7344        0.0422        0.9414        8.1838\n",
      "    541                    -0.6778  0.7881        0.0289        0.7513        2.6563\n",
      "    542                    -0.5952  0.8379        0.0183        0.5746        2.7858\n",
      "    543                    -0.5474  0.8646        0.0166        0.4799        3.9686\n",
      "    544                    -0.5395  0.8678        0.0197        0.4685        2.5278\n",
      "    545                    -0.5517  0.8621        0.0228        0.4887        2.0284\n",
      "    546                    -0.5653  0.8557        0.0234        0.5114        1.9260\n",
      "    547                    -0.5765  0.8498        0.0230        0.5323        2.0647\n",
      "    548                    -0.5819  0.8451        0.0222        0.5491        2.7173\n",
      "    549                    -0.5830  0.8411        0.0211        0.5633        2.3701\n",
      "    550                    -0.6050  0.8314        0.0189        0.5976        4.2571\n",
      "    551                    -0.5994  0.8358        0.0156        0.5822        2.7504\n",
      "    552                    -0.5642  0.8533        0.0149        0.5201        2.8067\n",
      "    553                    -0.5340  0.8671        0.0164        0.4711        3.8018\n",
      "    554                    -0.5190  0.8736        0.0170        0.4482        2.1998\n",
      "    555                    -0.5233  0.8722        0.0171        0.4532        2.2437\n",
      "    556                    -0.5352  0.8671        0.0172        0.4711        2.7667\n",
      "    557                    -0.5470  0.8616        0.0173        0.4907        2.6470\n",
      "    558                    -0.5596  0.8556        0.0177        0.5118        2.6431\n",
      "    559                    -0.5744  0.8486        0.0186        0.5366        2.5725\n",
      "    560                    -0.5920  0.8402        0.0197        0.5664        2.5683\n",
      "    561                    -0.6113  0.8306        0.0214        0.6006        4.8614\n",
      "    562                    -0.6299  0.8206        0.0234        0.6359        4.3455\n",
      "    563                    -0.6433  0.8120        0.0255        0.6663        3.1710\n",
      "    564                    -0.6484  0.8065        0.0269        0.6861        2.6451\n",
      "    565                    -0.6462  0.8047        0.0265        0.6924        5.0124\n",
      "    566                    -0.6720  0.7915        0.0246        0.7391        2.1176\n",
      "    567                    -0.7529  0.7539        0.0188        0.8723        2.7463\n",
      "    568                    -0.7526  0.7546        0.0214        0.8698        2.6276\n",
      "    569                    -0.7304  0.7627        0.0282        0.8412        2.6141\n",
      "    570                    -0.7116  0.7700        0.0328        0.8153        2.8855\n",
      "    571                    -0.6870  0.7828        0.0316        0.7701        2.8466\n",
      "    572                    -0.6535  0.8024        0.0265        0.7005        2.4757\n",
      "    573                    -0.6107  0.8272        0.0207        0.6126        1.9915\n",
      "    574                    -0.5674  0.8513        0.0169        0.5271        1.5680\n",
      "    575                    -0.5375  0.8667        0.0155        0.4725        2.1056\n",
      "    576                    -0.5299  0.8700        0.0158        0.4607        3.1276\n",
      "    577                    -0.5378  0.8663        0.0170        0.4739        2.9938\n",
      "    578                    -0.5485  0.8611        0.0179        0.4925        2.7728\n",
      "    579                    -0.5598  0.8556        0.0181        0.5120        2.4664\n",
      "    580                    -0.5704  0.8499        0.0179        0.5323        1.9202\n",
      "    581                    -0.5781  0.8444        0.0173        0.5514        2.4281\n",
      "    582                    -0.5822  0.8403        0.0165        0.5661        2.8506\n",
      "    583                    -0.6008  0.8308        0.0164        0.5999        2.3799\n",
      "    584                    -0.7127  0.7817        0.0173        0.7739        2.4085\n",
      "    585                    -0.7409  0.7633        0.0228        0.8392        2.2401\n",
      "    586                    -0.7552  0.7516        0.0269        0.8805        1.8525\n",
      "    587                    -0.7181  0.7668        0.0315        0.8266        2.2122\n",
      "    588                    -0.6678  0.7932        0.0303        0.7329        2.4639\n",
      "    589                    -0.6144  0.8251        0.0233        0.6200        2.5460\n",
      "    590                    -0.5674  0.8525        0.0166        0.5230        2.9912\n",
      "    591                    -0.5438  0.8644        0.0148        0.4808        2.5852\n",
      "    592                    -0.5444  0.8632        0.0162        0.4850        2.6645\n",
      "    593                    -0.5550  0.8576        0.0174        0.5047        2.5119\n",
      "    594                    -0.5671  0.8509        0.0173        0.5286        2.4742\n",
      "    595                    -0.5768  0.8447        0.0167        0.5507        2.5366\n",
      "    596                    -0.5832  0.8399        0.0161        0.5675        2.1641\n",
      "    597                    -0.5941  0.8333        0.0157        0.5908        2.2213\n",
      "    598                    -0.6669  0.8012        0.0168        0.7049        2.1514\n",
      "    599                    -0.7346  0.7680        0.0189        0.8223        2.2677\n",
      "    600                    -0.7319  0.7658        0.0233        0.8303        2.5486\n",
      "    601                    -0.6749  0.7911        0.0267        0.7405        3.4795\n",
      "    602                    -0.6133  0.8249        0.0261        0.6206        2.1718\n",
      "    603                    -0.5634  0.8547        0.0210        0.5152        4.7619\n",
      "    604                    -0.5458  0.8642        0.0176        0.4813        2.0004\n",
      "    605                    -0.5525  0.8600        0.0196        0.4961        1.9322\n",
      "    606                    -0.5648  0.8534        0.0201        0.5196        1.6928\n",
      "    607                    -0.5801  0.8455        0.0195        0.5478        2.2397\n",
      "    608                    -0.5950  0.8370        0.0201        0.5780        5.7987\n",
      "    609                    -0.6075  0.8302        0.0203        0.6020        6.2445\n",
      "    610                    -0.6153  0.8243        0.0202        0.6227        4.9301\n",
      "    611                    -0.6163  0.8199        0.0194        0.6383        6.6550\n",
      "    612                    -0.6420  0.8067        0.0189        0.6853        3.5901\n",
      "    613                    -0.6877  0.7881        0.0176        0.7511        5.4004\n",
      "    614                    -0.6541  0.8053        0.0175        0.6903        3.8888\n",
      "    615                    -0.6024  0.8314        0.0184        0.5976        3.9137\n",
      "    616                    -0.5533  0.8563        0.0195        0.5095        3.5218\n",
      "    617                    -0.5323  0.8682        0.0184        0.4672        2.6645\n",
      "    618                    -0.5451  0.8641        0.0183        0.4817        2.1976\n",
      "    619                    -0.5582  0.8574        0.0189        0.5056        2.3891\n",
      "    620                    -0.5712  0.8503        0.0185        0.5306        1.9661\n",
      "    621                    -0.5852  0.8422        0.0179        0.5592        1.9664\n",
      "    622                    -0.6010  0.8334        0.0186        0.5907        2.4353\n",
      "    623                    -0.6160  0.8247        0.0197        0.6214        2.3122\n",
      "    624                    -0.6279  0.8175        0.0206        0.6469        2.7961\n",
      "    625                    -0.6346  0.8122        0.0209        0.6657        1.9716\n",
      "    626                    -0.6348  0.8083        0.0204        0.6796        2.0020\n",
      "    627                    -0.6587  0.7957        0.0199        0.7243        2.9711\n",
      "    628                    -0.7137  0.7723        0.0183        0.8071        2.4751\n",
      "    629                    -0.6879  0.7857        0.0184        0.7597        4.1487\n",
      "    630                    -0.6434  0.8082        0.0202        0.6798        2.4648\n",
      "    631                    -0.5926  0.8351        0.0230        0.5844        4.6386\n",
      "    632                    -0.5483  0.8600        0.0215        0.4963        4.1743\n",
      "    633                    -0.5374  0.8681        0.0188        0.4674        3.1024\n",
      "    634                    -0.5516  0.8624        0.0183        0.4877        3.4247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    635                    -0.5586  0.8565        0.0179        0.5087        3.4610\n",
      "    636                    -0.5705  0.8504        0.0168        0.5303        2.5888\n",
      "    637                    -0.5819  0.8427        0.0177        0.5576        2.3067\n",
      "    638                    -0.5965  0.8362        0.0178        0.5805        2.1759\n",
      "    639                    -0.6074  0.8290        0.0186        0.6062        2.7534\n",
      "    640                    -0.6162  0.8244        0.0184        0.6225        2.9563\n",
      "    641                    -0.6205  0.8197        0.0182        0.6392        2.7366\n",
      "    642                    -0.6199  0.8160        0.0172        0.6521        4.8080\n",
      "    643                    -0.6488  0.8016        0.0169        0.7035        3.0938\n",
      "    644                    -0.7129  0.7740        0.0170        0.8012        2.0109\n",
      "    645                    -0.7059  0.7764        0.0200        0.7928        2.3253\n",
      "    646                    -0.6777  0.7885        0.0230        0.7496        2.1489\n",
      "    647                    -0.6365  0.8101        0.0258        0.6732        2.0402\n",
      "    648                    -0.5883  0.8375        0.0242        0.5760        2.4322\n",
      "    649                    -0.5535  0.8595        0.0195        0.4982        2.5924\n",
      "    650                    -0.5438  0.8652        0.0160        0.4780        2.6409\n",
      "    651                    -0.5483  0.8616        0.0151        0.4905        2.8954\n",
      "    652                    -0.5561  0.8561        0.0149        0.5101        2.7083\n",
      "    653                    -0.5682  0.8492        0.0152        0.5345        2.5018\n",
      "    654                    -0.5807  0.8417        0.0157        0.5611        2.7277\n",
      "    655                    -0.5932  0.8351        0.0160        0.5846        2.8413\n",
      "    656                    -0.6030  0.8290        0.0161        0.6062        2.8020\n",
      "    657                    -0.6101  0.8246        0.0159        0.6219        2.5917\n",
      "    658                    -0.6143  0.8205        0.0154        0.6364        3.0766\n",
      "    659                    -0.6206  0.8148        0.0149        0.6564        3.8391\n",
      "    660                    -0.6832  0.7859        0.0157        0.7591        2.7128\n",
      "    661                    -0.7788  0.7392        0.0192        0.9244        1.9995\n",
      "    662                    -0.8227  0.7087        0.0256        1.0326        2.3998\n",
      "    663                    -0.8223  0.7016        0.0340        1.0579        4.9876\n",
      "    664                    -0.7823  0.7205        0.0392        0.9907        2.9309\n",
      "    665                    -0.7211  0.7563        0.0331        0.8641        3.0315\n",
      "    666                    -0.6484  0.8032        0.0197        0.6975        2.6841\n",
      "    667                    -0.5897  0.8393        \u001b[35m0.0129\u001b[0m        0.5697        3.4888\n",
      "    668                    -0.5590  0.8557        \u001b[35m0.0118\u001b[0m        0.5115        2.7343\n",
      "    669                    -0.5559  0.8566        0.0128        0.5084        3.7551\n",
      "    670                    -0.5618  0.8520        0.0134        0.5248        4.2668\n",
      "    671                    -0.5670  0.8468        0.0130        0.5431        3.1132\n",
      "    672                    -0.5800  0.8384        0.0128        0.5729        2.8834\n",
      "    673                    -0.6058  0.8268        0.0128        0.6141        2.2074\n",
      "    674                    -0.6057  0.8276        0.0129        0.6110        2.2075\n",
      "    675                    -0.5918  0.8351        0.0125        0.5845        2.0525\n",
      "    676                    -0.5665  0.8477        0.0131        0.5398        2.9011\n",
      "    677                    -0.5417  0.8604        0.0134        0.4949        2.3267\n",
      "    678                    -0.5345  0.8651        0.0138        0.4781        2.8188\n",
      "    679                    -0.5370  0.8640        0.0143        0.4822        3.2207\n",
      "    680                    -0.5426  0.8610        0.0146        0.4929        3.1588\n",
      "    681                    -0.5523  0.8561        0.0146        0.5103        2.7649\n",
      "    682                    -0.5683  0.8483        0.0147        0.5379        3.1981\n",
      "    683                    -0.5903  0.8363        0.0149        0.5802        3.0328\n",
      "    684                    -0.6146  0.8202        0.0161        0.6375        3.0044\n",
      "    685                    -0.6410  0.8042        0.0194        0.6942        4.7168\n",
      "    686                    -0.6606  0.7919        0.0244        0.7376        2.6793\n",
      "    687                    -0.6710  0.7878        0.0294        0.7522        2.5757\n",
      "    688                    -0.6742  0.7880        0.0314        0.7514        2.3254\n",
      "    689                    -0.6716  0.7916        0.0290        0.7387        2.5822\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[initialized](\n",
       "  module_=SequenceDoubleAtt(\n",
       "    (lstm1): LSTM(12, 256, batch_first=True)\n",
       "    (lstm2): LSTM(256, 512, batch_first=True)\n",
       "    (lin): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (lin_out): Linear(in_features=512, out_features=1, bias=True)\n",
       "    (softmax): Softmax(dim=1)\n",
       "    (tanh): Tanh()\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_sa_att.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valutazione del training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_test = lstm_sa_att_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm_test.history[10].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm = lstm_test\n",
    "# valid_losses = lstm.history[:, 'valid_loss']\n",
    "# train_losses = lstm.history[:, 'train_loss']\n",
    "# plt.figure(figsize=(12,7))\n",
    "# plt.plot(valid_losses, label='valid_loss')\n",
    "# plt.plot(train_losses, label='train_loss')\n",
    "# # plt.xticks(np.arange(len(valid_losses)+1, step=50))\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test\n",
    "Dopo aver trovato i parametri migliori i modelli vengono testati facendo training sulla metà dei dati e predizioni sull'altra metà"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = ['lstm_sa_d', 'lstm_moro', 'lstm_sa_d_1000', 'lstm_att_d_600', 'lstm_sa_att_sa_1500']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('lstm_moro', 'open'), ('lstm_sa_d_1000','sa_ohlcv'), ('lstm_att_d_600','ohlcv'), ('lstm_sa_att_1500', 'ohlcv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_test = load(model[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[\"^GSPC\"][\"features\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "market = \"^DJI\"\n",
    "feature_set = model[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets[market][\"original\"][\"Open\"].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_dataset(datasets[market][\"features\"][feature_set])\n",
    "opn = datasets[market][\"original\"][\"Open\"]\n",
    "close = datasets[market][\"original\"][\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i primi 51 giorni non vengonon considerati poichè servono per il calcolo dell'inversa dello z_index\n",
    "Y_original = close.copy()\n",
    "Y_preds = Y_original.copy()\n",
    "\n",
    "for i in range(51, len(Y_val)): \n",
    "    pred = lstm_test.predict(X_val[i-10:i].to_numpy().astype(np.float32))[0] #in input vengono dati 10 gionri\n",
    "    #denormalization\n",
    "    previous_serie = Y_original[:i]\n",
    "    Y_preds[i] = z_score_inv(previous_serie[-50:], pred) #il z score viene calcolato su 50 gionri\n",
    "Y_preds = Y_preds[51:]\n",
    "Y_original = Y_original.reindex_like(Y_preds, copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "Y_preds.plot(label='close_pred')\n",
    "Y_original.plot(label='close_true')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_original.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn.metrics.mean_squared_error(Y_original, Y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = pd.Series(index=[\"MAPE\",\"RMSPE\", \"R2\", \"ROI\", \"ROI Ideal\", \"ROI vs ideal\"], dtype=np.float32)\n",
    "score[\"MAPE\"] = sklearn.metrics.mean_absolute_error(Y_original, Y_preds)/Y_preds.mean()\n",
    "score[\"RMSPE\"] = np.sqrt(sklearn.metrics.mean_squared_error(Y_original, Y_preds))/Y_original.mean()\n",
    "score[\"R2\"] = sklearn.metrics.r2_score(Y_original, Y_preds)\n",
    "score[\"ROI\"] = roi(Y_original,Y_preds, opn)\n",
    "score[\"ROI Ideal\"] = roi(Y_original,Y_original, opn)\n",
    "score[\"ROI vs ideal\"] = abs(roi(Y_original,Y_preds, opn) - roi(Y_original,Y_original, opn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame(index=score.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_transaction = False\n",
    "initial_capital = 10000\n",
    "capital = initial_capital\n",
    "cap_history = Y_preds.copy()\n",
    "holding=False\n",
    "opns = opn.reindex_like(Y_preds).copy()\n",
    "buy_history = opn.reindex_like(Y_preds).copy()\n",
    "sell_history = opn.reindex_like(Y_preds).copy()\n",
    "for date in Y_preds.index:\n",
    "    buy_history[date] = None\n",
    "    sell_history[date] = None\n",
    "    pred = Y_preds[date]\n",
    "    true = Y_original[date]\n",
    "    buy = Y_preds[date] - opns[date] > 0\n",
    "    delta_true = Y_original[date] - opns[date]\n",
    "    if(buy and not holding): # buy\n",
    "        if(show_transaction):\n",
    "            capital -= opns[date]\n",
    "        holding = True\n",
    "        buy_history[date] = capital\n",
    "    elif(not buy and holding): #sell\n",
    "        if(show_transaction):\n",
    "            capital += opns[date]\n",
    "        holding = False\n",
    "        sell_history[date] = capital\n",
    "    if(holding):\n",
    "        capital+= delta_true\n",
    "    cap_history[date] = capital\n",
    "if(holding and show_transaction):\n",
    "    capital += opns[-1]\n",
    "    holding = False\n",
    "abs_gain = capital - initial_capital\n",
    "perc_gain = (abs_gain) / initial_capital\n",
    "print(f\"guadagno assoluto: {abs_gain}\")\n",
    "print(f\"guadagno percentuale: {perc_gain}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.scatter(buy_history.index, buy_history, c='green', label='buy', linewidths=0.001)\n",
    "plt.scatter(sell_history.index, sell_history, c= 'red', label='sell', linewidths=0.001)\n",
    "cap_history.plot(label='capital')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "test_modelli.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
