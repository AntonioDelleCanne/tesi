{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/dsvm_server_admin/notebooks/fastai/tesi/DeepLearning_Financial\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4eb0ef627e0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mtss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeSeriesSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0mtrain_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0mtss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeSeriesSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_train_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0mtss_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCVSplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstratified\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd DeepLearning_Financial\n",
    "\n",
    "## EXTERNAL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import yfinance\n",
    "from pandas import Series\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, PredefinedSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random \n",
    "from sklearn.datasets import make_regression\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetRegressor\n",
    "from torch.nn.modules.loss import MSELoss\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from skorch.dataset import CVSplit\n",
    "from skorch import callbacks\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "import skorch\n",
    "import pywt\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "\n",
    "\n",
    "##INTERNAL\n",
    "from models import Autoencoder, waveletSmooth, SequenceDouble, SequenceDoubleAtt, SequenceAtt\n",
    "from utils import prepare_data_lstm, ExampleDataset, save_checkpoint, evaluate_lstm, backtest\n",
    "from testing import *\n",
    "from sa_tools import *\n",
    "from test_models import *\n",
    "\n",
    "\n",
    "\n",
    "#creazione dataset\n",
    "# indices = ['^DJI', '^GSPC', '^HSI', '^N225', 'ASHR','^NSEI']\n",
    "indices = ['^GSPC']\n",
    "feature_sets = ['open', 'ohlcv', 'ext']\n",
    "start_date=\"2000-01-01\"\n",
    "end_date=\"2018-12-31\"\n",
    "\n",
    "# datasets=get_datasets(indices = indices, feature_sets = feature_sets, end_date=end_date, start_date=start_date)\n",
    "# save(datasets, 'datasets')\n",
    "# datasets = load('^GSCP')\n",
    "# tss = TimeSeriesSplit(4)\n",
    "# tss_split = CVSplit(cv=tss, stratified=False, random_state=None)\n",
    "\n",
    "tss = TimeSeriesSplit(4)\n",
    "\n",
    "train_size = len(list(tss.split(X))[0][0])\n",
    "tss = TimeSeriesSplit(4, max_train_size=train_size)\n",
    "tss_split = CVSplit(cv=tss, stratified=False, random_state=None)\n",
    "\n",
    "for index in indices:\n",
    "    X=datasets[index][\"target\"]\n",
    "    \n",
    "    tss = TimeSeriesSplit(4)\n",
    "    train_size = len(list(tss.split(X))[0][0])\n",
    "    tss = TimeSeriesSplit(4, max_train_size=train_size)\n",
    "    tss_split = CVSplit(cv=tss, stratified=False, random_state=None)\n",
    "    \n",
    "    for i, (train, val) in enumerate(tss.split(X), start=1):\n",
    "        print(\"FOLD {}\".format(i))\n",
    "        train_dates = X.index[train]\n",
    "        val_dates = X.index[val]\n",
    "        print(\"Training set da {} a {}\".format(train_dates.min(), train_dates.max()))\n",
    "        print(\"Validation set da {} a {}\".format(val_dates.min(), val_dates.max()))\n",
    "        print(X.index.min())\n",
    "        print(X.index.max())\n",
    "    for i, (train_dates, val_dates) in enumerate(tss.split(datasets[index][\"target\"]), start=1):\n",
    "        for model_name in get_model_names():\n",
    "            \n",
    "            # get the model\n",
    "            save_name = \"{}_{}_{}_{}.pkl\".format(model_name, index, datasets[index][\"target\"].index[train_dates].min().date(), datasets[index][\"target\"].index[train_dates].max().date())\n",
    "            print(save_name)\n",
    "            model, feature_set = get_model(model_name,  save_name)\n",
    "                \n",
    "            #get the dataset\n",
    "            sa=None\n",
    "            ld=False\n",
    "            fset= feature_set\n",
    "            if(feature_set is 'ext_sa'):\n",
    "                fset='ext'\n",
    "                sa = \"sa_{}_{}_{}\".format(index, datasets[index][\"target\"][train_dates].index.min().date(), datasets[index][\"target\"][train_dates].index.max().date())\n",
    "                sa_fname = sa + \".pkl\"\n",
    "                if(os.path.isfile(sa_fname)):\n",
    "                    ld=True\n",
    "            x, y, x_train, x_val, x_scaler, y_train, y_val, y_scaler = get_dataset_train(datasets=datasets, feature_set=fset, train_dates=train_dates, val_dates=val_dates, index=index, sa=sa, ld=ld)\n",
    "            \n",
    "            \n",
    "            #prepare dataset for training\n",
    "            n_days=5\n",
    "\n",
    "            x_batch = days_group(x, n_days=n_days)\n",
    "            y_batch = y[n_days:]\n",
    "            serie = x_batch\n",
    "            train_dates_s = train_dates[n_days:].copy()\n",
    "            val_dates_s = val_dates.copy()\n",
    "\n",
    "            splitted = np.split(serie, [len(train_dates_s)])\n",
    "            l1, l2 = len(splitted[0]), len(splitted[1])\n",
    "            train_s = PredefinedSplit(np.concatenate((np.ones(l1)*-1,np.ones(l2))))\n",
    "            train_split =  CVSplit(cv=train_s, stratified=False, random_state=None)  \n",
    "            train_dates_s, val_dates_s = list(train_s.split(serie))[0]\n",
    "            \n",
    "            model.train_split = train_split\n",
    "            model.fit(x_batch,y_batch)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
