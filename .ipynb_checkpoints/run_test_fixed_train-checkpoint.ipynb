{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/home/dsvm_server_admin/notebooks/fastai/tesi/DeepLearning_Financial\n",
      "FOLD 1\n",
      "Training set da 2000-09-19 00:00:00 a 2004-05-18 00:00:00\n",
      "Validation set da 2004-05-19 00:00:00 a 2008-01-12 00:00:00\n",
      "2000-09-19 00:00:00\n",
      "2018-12-27 00:00:00\n",
      "FOLD 2\n",
      "Training set da 2004-05-15 00:00:00 a 2008-01-12 00:00:00\n",
      "Validation set da 2008-01-13 00:00:00 a 2011-09-07 00:00:00\n",
      "2000-09-19 00:00:00\n",
      "2018-12-27 00:00:00\n",
      "FOLD 3\n",
      "Training set da 2008-01-09 00:00:00 a 2011-09-07 00:00:00\n",
      "Validation set da 2011-09-08 00:00:00 a 2015-05-03 00:00:00\n",
      "2000-09-19 00:00:00\n",
      "2018-12-27 00:00:00\n",
      "FOLD 4\n",
      "Training set da 2011-09-04 00:00:00 a 2015-05-03 00:00:00\n",
      "Validation set da 2015-05-04 00:00:00 a 2018-12-27 00:00:00\n",
      "2000-09-19 00:00:00\n",
      "2018-12-27 00:00:00\n",
      "lstm_sa_^GSPC_2000-09-19_2004-05-18.pkl\n",
      "  epoch    neg_mean_absolute_error       r2    roi_train    train_loss    valid_loss    cp     dur\n",
      "-------  -------------------------  -------  -----------  ------------  ------------  ----  ------\n",
      "      1                    \u001b[36m-1.4055\u001b[0m  \u001b[32m-2.6201\u001b[0m     \u001b[35m-72.4272\u001b[0m        \u001b[31m1.0535\u001b[0m        \u001b[94m2.7234\u001b[0m     +  4.9436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/fastai/lib/python3.6/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2                    -1.4121  -2.6454     -72.4272        \u001b[31m1.0129\u001b[0m        2.7424        2.2737\n",
      "      3                    -1.4147  -2.6552     -72.4272        \u001b[31m1.0074\u001b[0m        2.7498        2.1194\n",
      "      4                    -1.4160  -2.6600     -72.4272        \u001b[31m1.0037\u001b[0m        2.7534        2.8389\n",
      "      5                    -1.4168  -2.6631     -72.4272        \u001b[31m1.0010\u001b[0m        2.7557        2.1804\n",
      "      6                    -1.4174  -2.6653     -72.4272        \u001b[31m0.9990\u001b[0m        2.7574        1.6225\n",
      "      7                    -1.4177  -2.6667     -72.5763        \u001b[31m0.9976\u001b[0m        2.7584        2.0779\n",
      "      8                    -1.4181  -2.6681     -72.5763        \u001b[31m0.9964\u001b[0m        2.7595        1.9373\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd DeepLearning_Financial\n",
    "\n",
    "## EXTERNAL\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import yfinance\n",
    "from pandas import Series\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import TimeSeriesSplit, PredefinedSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import random \n",
    "from sklearn.datasets import make_regression\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from skorch import NeuralNetRegressor\n",
    "from torch.nn.modules.loss import MSELoss\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from skorch.dataset import CVSplit\n",
    "from skorch import callbacks\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import partial\n",
    "import skorch\n",
    "import pywt\n",
    "from sklearn import preprocessing\n",
    "import joblib\n",
    "\n",
    "\n",
    "##INTERNAL\n",
    "from models import Autoencoder, waveletSmooth, SequenceDouble, SequenceDoubleAtt, SequenceAtt\n",
    "from utils import prepare_data_lstm, ExampleDataset, save_checkpoint, evaluate_lstm, backtest\n",
    "from testing import *\n",
    "from sa_tools import *\n",
    "from test_models import *\n",
    "\n",
    "\n",
    "\n",
    "#creazione dataset\n",
    "# indices = ['^DJI', '^GSPC', '^HSI', '^N225', 'ASHR','^NSEI']\n",
    "indices = ['^GSPC']\n",
    "feature_sets = ['open', 'ohlcv', 'ext']\n",
    "start_date=\"2000-01-01\"\n",
    "end_date=\"2018-12-31\"\n",
    "\n",
    "# datasets=get_datasets(indices = indices, feature_sets = feature_sets, end_date=end_date, start_date=start_date)\n",
    "# save(datasets, 'datasets')\n",
    "datasets = load('^GSCP')\n",
    "# tss = TimeSeriesSplit(4)\n",
    "# tss_split = CVSplit(cv=tss, stratified=False, random_state=None)\n",
    "\n",
    "\n",
    "for index in indices:\n",
    "    X=datasets[index][\"target\"]\n",
    "    \n",
    "    tss = TimeSeriesSplit(4)\n",
    "    max_train_size = len(list(tss.split(X))[0][0])\n",
    "    tss = TimeSeriesSplit(4, max_train_size=max_train_size)\n",
    "    tss_split = CVSplit(cv=tss, stratified=False, random_state=None)\n",
    "    \n",
    "    for i, (train, val) in enumerate(tss.split(X), start=1):\n",
    "        print(\"FOLD {}\".format(i))\n",
    "        train_dates = X.index[train]\n",
    "        val_dates = X.index[val]\n",
    "        print(\"Training set da {} a {}\".format(train_dates.min(), train_dates.max()))\n",
    "        print(\"Validation set da {} a {}\".format(val_dates.min(), val_dates.max()))\n",
    "        print(X.index.min())\n",
    "        print(X.index.max())\n",
    "    for i, (train_dates, val_dates) in enumerate(tss.split(datasets[index][\"target\"]), start=1):\n",
    "        for model_name in get_model_names():\n",
    "            \n",
    "            # get the model\n",
    "            save_name = \"{}_{}_{}_{}.pkl\".format(model_name, index, datasets[index][\"target\"].index[train_dates].min().date(), datasets[index][\"target\"].index[train_dates].max().date())\n",
    "            print(save_name)\n",
    "            model, feature_set = get_model(model_name,  save_name)\n",
    "                \n",
    "            #get the dataset\n",
    "            sa=None\n",
    "            ld=False\n",
    "            fset= feature_set\n",
    "            if(feature_set is 'ext_sa'):\n",
    "                fset='ext'\n",
    "                sa = \"sa_{}_{}_{}\".format(index, datasets[index][\"target\"][train_dates].index.min().date(), datasets[index][\"target\"][train_dates].index.max().date())\n",
    "                sa_fname = sa + \".pkl\"\n",
    "                if(os.path.isfile(sa_fname)):\n",
    "                    ld=True\n",
    "            x, y, x_train, x_val, x_scaler, y_train, y_val, y_scaler = get_dataset_train(datasets=datasets, feature_set=fset, train_dates=train_dates, val_dates=val_dates, index=index, sa=sa, ld=ld)\n",
    "            \n",
    "            #prepare dataset for training\n",
    "            n_days=5\n",
    "\n",
    "            x_batch = days_group(x, n_days=n_days)\n",
    "            y_batch = y[n_days:]\n",
    "            serie = x_batch\n",
    "            train_dates_s = train_dates[n_days:].copy()\n",
    "            val_dates_s = val_dates.copy()\n",
    "\n",
    "#             opn_test = y_scaler.transform(datasets[index][\"original\"][\"Open\"].iloc[val_dates].to_numpy().reshape(-1,1))\n",
    "            opn_test = datasets[index][\"original\"][\"Open\"].iloc[val_dates].to_numpy()\n",
    "            roi_test = partial(roi_train,opn=opn_test)\n",
    "            \n",
    "            model, feature_set = get_model(model_name,  save_name, roi_test=roi_test)\n",
    "            \n",
    "            model.y_scaler = y_scaler\n",
    "            model.x_scaler = x_scaler\n",
    "            \n",
    "            splitted = np.split(serie, [len(train_dates_s)])\n",
    "            l1, l2 = len(splitted[0]), len(splitted[1])\n",
    "            train_s = PredefinedSplit(np.concatenate((np.ones(l1)*-1,np.ones(l2))))\n",
    "            train_split =  CVSplit(cv=train_s, stratified=False, random_state=None)  \n",
    "            train_dates_s, val_dates_s = list(train_s.split(serie))[0]\n",
    "            \n",
    "            model.train_split = train_split\n",
    "            model.fit(x_batch,y_batch)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
