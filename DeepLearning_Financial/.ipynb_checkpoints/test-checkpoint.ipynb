{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXTERNAL\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pickle\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import time\n",
    "import os\n",
    "import random \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "##INTERNAL\n",
    "from models import Autoencoder\n",
    "from models import Sequence\n",
    "from models import waveletSmooth\n",
    "\n",
    "from utils import prepare_data_lstm, ExampleDataset, save_checkpoint, evaluate_lstm, backtest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caricamento dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close Price</th>\n",
       "      <th>Open Price</th>\n",
       "      <th>High Price</th>\n",
       "      <th>Low Price</th>\n",
       "      <th>Volume</th>\n",
       "      <th>MACD</th>\n",
       "      <th>CCI</th>\n",
       "      <th>ATR</th>\n",
       "      <th>BOLL</th>\n",
       "      <th>EMA20</th>\n",
       "      <th>MA10</th>\n",
       "      <th>MTM6</th>\n",
       "      <th>MA5</th>\n",
       "      <th>MTM12</th>\n",
       "      <th>ROC</th>\n",
       "      <th>SMI</th>\n",
       "      <th>WVAD</th>\n",
       "      <th>US Dollar Index</th>\n",
       "      <th>Federal Fund Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2008-07-01</th>\n",
       "      <td>1284.91</td>\n",
       "      <td>1276.69</td>\n",
       "      <td>1285.31</td>\n",
       "      <td>1260.68</td>\n",
       "      <td>584629.0</td>\n",
       "      <td>-25.545595</td>\n",
       "      <td>-127.422348</td>\n",
       "      <td>24.63</td>\n",
       "      <td>1347.154231</td>\n",
       "      <td>1327.453482</td>\n",
       "      <td>1307.927</td>\n",
       "      <td>-33.09</td>\n",
       "      <td>1289.682</td>\n",
       "      <td>-75.12</td>\n",
       "      <td>-5.523408</td>\n",
       "      <td>-0.039542</td>\n",
       "      <td>-3.772245e+07</td>\n",
       "      <td>72.3400</td>\n",
       "      <td>2.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-07-02</th>\n",
       "      <td>1261.52</td>\n",
       "      <td>1285.82</td>\n",
       "      <td>1292.17</td>\n",
       "      <td>1261.51</td>\n",
       "      <td>527609.0</td>\n",
       "      <td>-27.571986</td>\n",
       "      <td>-115.864929</td>\n",
       "      <td>30.66</td>\n",
       "      <td>1342.391538</td>\n",
       "      <td>1321.174102</td>\n",
       "      <td>1300.298</td>\n",
       "      <td>-52.77</td>\n",
       "      <td>1277.592</td>\n",
       "      <td>-98.62</td>\n",
       "      <td>-7.250724</td>\n",
       "      <td>-0.060551</td>\n",
       "      <td>-5.204278e+07</td>\n",
       "      <td>71.9900</td>\n",
       "      <td>1.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-07-03</th>\n",
       "      <td>1262.90</td>\n",
       "      <td>1262.96</td>\n",
       "      <td>1271.48</td>\n",
       "      <td>1252.01</td>\n",
       "      <td>324759.0</td>\n",
       "      <td>-28.735319</td>\n",
       "      <td>-115.171527</td>\n",
       "      <td>19.47</td>\n",
       "      <td>1337.470769</td>\n",
       "      <td>1315.624188</td>\n",
       "      <td>1292.305</td>\n",
       "      <td>-59.07</td>\n",
       "      <td>1273.542</td>\n",
       "      <td>-88.03</td>\n",
       "      <td>-6.516252</td>\n",
       "      <td>-0.041755</td>\n",
       "      <td>-5.556148e+07</td>\n",
       "      <td>72.7300</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-07-07</th>\n",
       "      <td>1252.31</td>\n",
       "      <td>1262.90</td>\n",
       "      <td>1273.95</td>\n",
       "      <td>1240.68</td>\n",
       "      <td>526542.0</td>\n",
       "      <td>-30.164080</td>\n",
       "      <td>-111.302302</td>\n",
       "      <td>33.27</td>\n",
       "      <td>1331.857308</td>\n",
       "      <td>1309.594265</td>\n",
       "      <td>1285.743</td>\n",
       "      <td>-26.07</td>\n",
       "      <td>1268.328</td>\n",
       "      <td>-85.50</td>\n",
       "      <td>-6.740987</td>\n",
       "      <td>-0.049841</td>\n",
       "      <td>-5.352552e+07</td>\n",
       "      <td>72.7100</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008-07-08</th>\n",
       "      <td>1273.70</td>\n",
       "      <td>1251.84</td>\n",
       "      <td>1274.17</td>\n",
       "      <td>1242.84</td>\n",
       "      <td>603411.0</td>\n",
       "      <td>-29.233405</td>\n",
       "      <td>-82.732708</td>\n",
       "      <td>31.33</td>\n",
       "      <td>1326.985000</td>\n",
       "      <td>1306.175764</td>\n",
       "      <td>1281.313</td>\n",
       "      <td>-6.30</td>\n",
       "      <td>1267.068</td>\n",
       "      <td>-69.13</td>\n",
       "      <td>-3.356020</td>\n",
       "      <td>-0.030884</td>\n",
       "      <td>-3.909113e+07</td>\n",
       "      <td>72.9600</td>\n",
       "      <td>1.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-26</th>\n",
       "      <td>2146.10</td>\n",
       "      <td>2158.54</td>\n",
       "      <td>2158.54</td>\n",
       "      <td>2145.04</td>\n",
       "      <td>357811.0</td>\n",
       "      <td>-3.864936</td>\n",
       "      <td>-10.284991</td>\n",
       "      <td>19.65</td>\n",
       "      <td>2163.793077</td>\n",
       "      <td>2157.914061</td>\n",
       "      <td>2146.918</td>\n",
       "      <td>20.33</td>\n",
       "      <td>2158.170</td>\n",
       "      <td>-35.20</td>\n",
       "      <td>-1.846804</td>\n",
       "      <td>0.008891</td>\n",
       "      <td>-1.701630e+07</td>\n",
       "      <td>95.2986</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-27</th>\n",
       "      <td>2159.93</td>\n",
       "      <td>2146.04</td>\n",
       "      <td>2161.13</td>\n",
       "      <td>2141.55</td>\n",
       "      <td>366113.0</td>\n",
       "      <td>-3.200729</td>\n",
       "      <td>22.969783</td>\n",
       "      <td>19.58</td>\n",
       "      <td>2162.872308</td>\n",
       "      <td>2158.106055</td>\n",
       "      <td>2150.209</td>\n",
       "      <td>20.81</td>\n",
       "      <td>2162.204</td>\n",
       "      <td>32.12</td>\n",
       "      <td>-1.199821</td>\n",
       "      <td>0.015815</td>\n",
       "      <td>-1.258582e+07</td>\n",
       "      <td>95.4499</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-28</th>\n",
       "      <td>2171.37</td>\n",
       "      <td>2161.85</td>\n",
       "      <td>2172.40</td>\n",
       "      <td>2151.79</td>\n",
       "      <td>361908.0</td>\n",
       "      <td>-1.731270</td>\n",
       "      <td>100.879276</td>\n",
       "      <td>20.61</td>\n",
       "      <td>2162.438846</td>\n",
       "      <td>2159.369288</td>\n",
       "      <td>2154.769</td>\n",
       "      <td>31.61</td>\n",
       "      <td>2163.854</td>\n",
       "      <td>12.33</td>\n",
       "      <td>-0.455233</td>\n",
       "      <td>0.011104</td>\n",
       "      <td>-7.050632e+06</td>\n",
       "      <td>95.4275</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-29</th>\n",
       "      <td>2151.13</td>\n",
       "      <td>2168.90</td>\n",
       "      <td>2172.67</td>\n",
       "      <td>2145.20</td>\n",
       "      <td>424922.0</td>\n",
       "      <td>-2.174842</td>\n",
       "      <td>39.501463</td>\n",
       "      <td>27.47</td>\n",
       "      <td>2161.063077</td>\n",
       "      <td>2158.584594</td>\n",
       "      <td>2155.156</td>\n",
       "      <td>-11.99</td>\n",
       "      <td>2158.644</td>\n",
       "      <td>24.11</td>\n",
       "      <td>1.095963</td>\n",
       "      <td>0.005565</td>\n",
       "      <td>-1.063365e+07</td>\n",
       "      <td>95.5172</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-09-30</th>\n",
       "      <td>2168.27</td>\n",
       "      <td>2156.51</td>\n",
       "      <td>2175.30</td>\n",
       "      <td>2156.51</td>\n",
       "      <td>417334.0</td>\n",
       "      <td>-1.130292</td>\n",
       "      <td>86.866401</td>\n",
       "      <td>24.17</td>\n",
       "      <td>2160.787308</td>\n",
       "      <td>2159.507013</td>\n",
       "      <td>2158.067</td>\n",
       "      <td>-8.91</td>\n",
       "      <td>2159.360</td>\n",
       "      <td>42.50</td>\n",
       "      <td>0.427505</td>\n",
       "      <td>0.013444</td>\n",
       "      <td>-4.482216e+06</td>\n",
       "      <td>95.4426</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2079 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Close Price  Open Price  High Price  Low Price    Volume  \\\n",
       "Date                                                                   \n",
       "2008-07-01      1284.91     1276.69     1285.31    1260.68  584629.0   \n",
       "2008-07-02      1261.52     1285.82     1292.17    1261.51  527609.0   \n",
       "2008-07-03      1262.90     1262.96     1271.48    1252.01  324759.0   \n",
       "2008-07-07      1252.31     1262.90     1273.95    1240.68  526542.0   \n",
       "2008-07-08      1273.70     1251.84     1274.17    1242.84  603411.0   \n",
       "...                 ...         ...         ...        ...       ...   \n",
       "2016-09-26      2146.10     2158.54     2158.54    2145.04  357811.0   \n",
       "2016-09-27      2159.93     2146.04     2161.13    2141.55  366113.0   \n",
       "2016-09-28      2171.37     2161.85     2172.40    2151.79  361908.0   \n",
       "2016-09-29      2151.13     2168.90     2172.67    2145.20  424922.0   \n",
       "2016-09-30      2168.27     2156.51     2175.30    2156.51  417334.0   \n",
       "\n",
       "                 MACD         CCI    ATR         BOLL        EMA20      MA10  \\\n",
       "Date                                                                           \n",
       "2008-07-01 -25.545595 -127.422348  24.63  1347.154231  1327.453482  1307.927   \n",
       "2008-07-02 -27.571986 -115.864929  30.66  1342.391538  1321.174102  1300.298   \n",
       "2008-07-03 -28.735319 -115.171527  19.47  1337.470769  1315.624188  1292.305   \n",
       "2008-07-07 -30.164080 -111.302302  33.27  1331.857308  1309.594265  1285.743   \n",
       "2008-07-08 -29.233405  -82.732708  31.33  1326.985000  1306.175764  1281.313   \n",
       "...               ...         ...    ...          ...          ...       ...   \n",
       "2016-09-26  -3.864936  -10.284991  19.65  2163.793077  2157.914061  2146.918   \n",
       "2016-09-27  -3.200729   22.969783  19.58  2162.872308  2158.106055  2150.209   \n",
       "2016-09-28  -1.731270  100.879276  20.61  2162.438846  2159.369288  2154.769   \n",
       "2016-09-29  -2.174842   39.501463  27.47  2161.063077  2158.584594  2155.156   \n",
       "2016-09-30  -1.130292   86.866401  24.17  2160.787308  2159.507013  2158.067   \n",
       "\n",
       "             MTM6       MA5  MTM12       ROC       SMI          WVAD  \\\n",
       "Date                                                                   \n",
       "2008-07-01 -33.09  1289.682 -75.12 -5.523408 -0.039542 -3.772245e+07   \n",
       "2008-07-02 -52.77  1277.592 -98.62 -7.250724 -0.060551 -5.204278e+07   \n",
       "2008-07-03 -59.07  1273.542 -88.03 -6.516252 -0.041755 -5.556148e+07   \n",
       "2008-07-07 -26.07  1268.328 -85.50 -6.740987 -0.049841 -5.352552e+07   \n",
       "2008-07-08  -6.30  1267.068 -69.13 -3.356020 -0.030884 -3.909113e+07   \n",
       "...           ...       ...    ...       ...       ...           ...   \n",
       "2016-09-26  20.33  2158.170 -35.20 -1.846804  0.008891 -1.701630e+07   \n",
       "2016-09-27  20.81  2162.204  32.12 -1.199821  0.015815 -1.258582e+07   \n",
       "2016-09-28  31.61  2163.854  12.33 -0.455233  0.011104 -7.050632e+06   \n",
       "2016-09-29 -11.99  2158.644  24.11  1.095963  0.005565 -1.063365e+07   \n",
       "2016-09-30  -8.91  2159.360  42.50  0.427505  0.013444 -4.482216e+06   \n",
       "\n",
       "            US Dollar Index  Federal Fund Rate  \n",
       "Date                                            \n",
       "2008-07-01          72.3400               2.11  \n",
       "2008-07-02          71.9900               1.95  \n",
       "2008-07-03          72.7300               1.92  \n",
       "2008-07-07          72.7100               1.99  \n",
       "2008-07-08          72.9600               1.97  \n",
       "...                     ...                ...  \n",
       "2016-09-26          95.2986               0.40  \n",
       "2016-09-27          95.4499               0.40  \n",
       "2016-09-28          95.4275               0.40  \n",
       "2016-09-29          95.5172               0.40  \n",
       "2016-09-30          95.4426               0.29  \n",
       "\n",
       "[2079 rows x 19 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"./data/S&P500IndexData-Table1.csv\"\n",
    "data_master = pd.read_csv(path, sep=\";\")\n",
    "data_master[\"Date\"] = pd.to_datetime(data_master[\"Ntime\"].astype(str))\n",
    "data_master.drop(\"time\", inplace=True, axis=1)\n",
    "data_master.drop(\"Ntime\", inplace=True, axis=1)\n",
    "data_master.set_index(\"Date\", inplace=True)\n",
    "pd.set_option('display.max_columns', None)\n",
    "data_master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Scales the inputs such that they are in an appropriate range''' \n",
    "def get_scaled_features(feats_to_scale):\n",
    "    feats = feats_to_scale.copy()\n",
    "    feats[\"Close Price\"].loc[:] = feats[\"Close Price\"].loc[:]/1000\n",
    "    feats[\"Open Price\"].loc[:] = feats[\"Open Price\"].loc[:]/1000\n",
    "    feats[\"High Price\"].loc[:] = feats[\"High Price\"].loc[:]/1000\n",
    "    feats[\"Low Price\"].loc[:] = feats[\"Low Price\"].loc[:]/1000\n",
    "    feats[\"Volume\"].loc[:] = feats[\"Volume\"].loc[:]/1000000\n",
    "    feats[\"MACD\"].loc[:] = feats[\"MACD\"].loc[:]/10\n",
    "    feats[\"CCI\"].loc[:] = feats[\"CCI\"].loc[:]/100\n",
    "    feats[\"ATR\"].loc[:] = feats[\"ATR\"].loc[:]/100\n",
    "    feats[\"BOLL\"].loc[:] = feats[\"BOLL\"].loc[:]/1000\n",
    "    feats[\"EMA20\"].loc[:] = feats[\"EMA20\"].loc[:]/1000\n",
    "    feats[\"MA10\"].loc[:] = feats[\"MA10\"].loc[:]/1000\n",
    "    feats[\"MTM6\"].loc[:] = feats[\"MTM6\"].loc[:]/100\n",
    "    feats[\"MA5\"].loc[:] = feats[\"MA5\"].loc[:]/1000\n",
    "    feats[\"MTM12\"].loc[:] = feats[\"MTM12\"].loc[:]/100\n",
    "    feats[\"ROC\"].loc[:] = feats[\"ROC\"].loc[:]/10\n",
    "    feats[\"SMI\"].loc[:] = feats[\"SMI\"].loc[:] * 10\n",
    "    feats[\"WVAD\"].loc[:] = feats[\"WVAD\"].loc[:]/100000000\n",
    "    feats[\"US Dollar Index\"].loc[:] = feats[\"US Dollar Index\"].loc[:]/100\n",
    "    feats[\"Federal Fund Rate\"].loc[:] = feats[\"Federal Fund Rate\"].loc[:]\n",
    "    return feats\n",
    "\n",
    "def train_valid_test_split(data, step_size, out_type=np.float):\n",
    "    return data[:-2*step_size].to_numpy(dtype=out_type, copy=True), data[-2*step_size:-step_size].to_numpy(dtype=out_type, copy=True) ,data[-step_size:].to_numpy(dtype=out_type, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO metti parametro look in the future(boolean)\n",
    "def apply_wavelet_transform(data, consider_future=True):\n",
    "    res = data.copy()\n",
    "    if(len(res.shape) == 1):\n",
    "        res = res[...,np.newaxis]\n",
    "    if(consider_future):\n",
    "        for i in range(res.shape[1]):\n",
    "            res[:,i] = waveletSmooth(res[:,i].copy(), level=1)[-len(res):]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training parameters\n",
    "\n",
    "# 600 is a bit more than 2 years of data\n",
    "num_datapoints = 600\n",
    "# roll by approx. 60 days - 3 months of trading days\n",
    "step_size = int(0.1 * num_datapoints)\n",
    "# calculate number of iterations we can do over the entire data set\n",
    "num_iterations = int(np.ceil((len(data_master)-num_datapoints)/step_size))+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##init training loop\n",
    "n=0\n",
    "\n",
    "y_test_lst = []\n",
    "preds = []\n",
    "ct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SCALE THE DATA\n",
    "data = data_master.iloc[n*step_size:num_datapoints+n*step_size,:].copy()\n",
    "data.columns = [col.strip() for col in data.columns.tolist()]\n",
    "ct +=1\n",
    "feats = get_scaled_features(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATE AND SPLIT THE DATASET\n",
    "X = feats.copy().drop(\"Close Price\", axis=1)\n",
    "Y = feats[\"Close Price\"].copy()\n",
    "\n",
    "X_train, X_valid, X_test = train_valid_test_split(X, step_size)\n",
    "Y_train, Y_valid, Y_test = train_valid_test_split(Y, step_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DENOISING USING DWT\n",
    "#TODO vedi se con metodo originale risultati diversi, in caso vedi perche'(se effettivamente future data leaking)\n",
    "#TODO capisci bene wavelet per mettere spiegazione in relazione e per fare TODO sopra(guarda anche codice e doc wavelet)\n",
    "X_train_denoised = apply_wavelet_transform(X_train)\n",
    "X_valid_denoised = apply_wavelet_transform(np.append(X_train, X_valid, axis=0))\n",
    "X_test_denoised = apply_wavelet_transform(np.append(np.append(X_train, X_valid, axis=0), X_test, axis=0))\n",
    "Y_train_denoised = apply_wavelet_transform(Y_train)\n",
    "Y_valid_denoised = apply_wavelet_transform(np.append(Y_train, Y_valid, axis=0))\n",
    "Y_test_denoised = apply_wavelet_transform(np.append(np.append(Y_train, Y_valid, axis=0), Y_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO mostra i dati prima e dopo aver applicato la wavelet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ENCODE FEATURES USING STACKED AUTOENCODER\n",
    "#TODO capisci e fai refactoring\n",
    "num_hidden_1 = 10\n",
    "num_hidden_2 = 10\n",
    "num_hidden_3 = 10\n",
    "num_hidden_4 = 10\n",
    "\n",
    "n_epoch=100#20000\n",
    "\n",
    "# ---- train using training data\n",
    "\n",
    "# The n==0 statement is done because we only want to initialize the network once and then keep training\n",
    "# as we move through time \n",
    "\n",
    "if n == 0:\n",
    "    auto1 = Autoencoder(feats_norm_train.shape[1], num_hidden_1)\n",
    "auto1.fit(feats_norm_train, n_epoch=n_epoch)\n",
    "\n",
    "inputs = torch.autograd.Variable(torch.from_numpy(feats_norm_train.astype(np.float32)))\n",
    "\n",
    "if n == 0:\n",
    "    auto2 = Autoencoder(num_hidden_1, num_hidden_2)\n",
    "auto1_out = auto1.encoder(inputs).data.numpy()\n",
    "auto2.fit(auto1_out, n_epoch=n_epoch)\n",
    "\n",
    "if n == 0:\n",
    "    auto3 = Autoencoder(num_hidden_2, num_hidden_3)\n",
    "auto1_out = torch.autograd.Variable(torch.from_numpy(auto1_out.astype(np.float32)))\n",
    "auto2_out = auto2.encoder(auto1_out).data.numpy()\n",
    "auto3.fit(auto2_out, n_epoch=n_epoch)\n",
    "\n",
    "if n == 0:\n",
    "    auto4 = Autoencoder(num_hidden_3, num_hidden_4)\n",
    "auto2_out = torch.autograd.Variable(torch.from_numpy(auto2_out.astype(np.float32)))\n",
    "auto3_out = auto3.encoder(auto2_out).data.numpy()\n",
    "auto4.fit(auto3_out, n_epoch=n_epoch)\n",
    "\n",
    "\n",
    "# Change to evaluation mode, in this mode the network behaves differently, e.g. dropout is switched off and so on\n",
    "auto1.eval()        \n",
    "auto2.eval()\n",
    "auto3.eval()\n",
    "auto4.eval()\n",
    "\n",
    "X_train = feats_norm_train\n",
    "X_train = torch.autograd.Variable(torch.from_numpy(X_train.astype(np.float32)))\n",
    "train_encoded = auto4.encoder(auto3.encoder(auto2.encoder(auto1.encoder(X_train))))\n",
    "train_encoded = train_encoded.data.numpy()\n",
    "\n",
    "# ---- encode validation and test data using autoencoder trained only on training data \n",
    "X_validate = feats_norm_validate_WT   \n",
    "X_validate = torch.autograd.Variable(torch.from_numpy(X_validate.astype(np.float32)))\n",
    "validate_encoded = auto4.encoder(auto3.encoder(auto2.encoder(auto1.encoder(X_validate))))\n",
    "validate_encoded = validate_encoded.data.numpy()\n",
    "\n",
    "X_test = feats_norm_test_WT\n",
    "X_test = torch.autograd.Variable(torch.from_numpy(X_test.astype(np.float32)))\n",
    "test_encoded = auto4.encoder(auto3.encoder(auto2.encoder(auto1.encoder(X_test))))\n",
    "test_encoded = test_encoded.data.numpy()\n",
    "\n",
    "# switch back to training mode\n",
    "auto1.train()        \n",
    "auto2.train()\n",
    "auto3.train()\n",
    "auto4.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in train, test and validation set\n",
    "\n",
    "# y_test = data_close_new[-step_size:].to_numpy()\n",
    "# y_validate = data_close_new[-2*step_size:-step_size].to_numpy()\n",
    "# y_train = data_close_new[:-2*step_size].to_numpy()\n",
    "# feats_train = train.to_numpy().astype(np.float)\n",
    "# feats_validate = validate.to_numpy().astype(np.float)\n",
    "# feats_test = test.to_numpy().astype(np.float)\n",
    "# ---------------------------------------------------------------------------\n",
    "# ----------------------- STEP 2.0: NORMALIZE DATA --------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# REMOVED THE NORMALIZATION AND MANUALLY SCALED TO APPROPRIATE VALUES ABOVE\n",
    "\n",
    "# \"\"\"\n",
    "# scaler = StandardScaler().fit(feats_train)\n",
    "\n",
    "# feats_norm_train = scaler.transform(feats_train)\n",
    "# feats_norm_validate = scaler.transform(feats_validate)\n",
    "# feats_norm_test = scaler.transform(feats_test)\n",
    "# \"\"\"\n",
    "# \"\"\"\n",
    "# scaler = MinMaxScaler(feature_range=(0,1))\n",
    "# scaler.fit(feats_train)\n",
    "\n",
    "# feats_norm_train = scaler.transform(feats_train)\n",
    "# feats_norm_validate = scaler.transform(feats_validate)\n",
    "# feats_norm_test = scaler.transform(feats_test)\n",
    "# \"\"\"    \n",
    "# data_close = pd.Series(np.concatenate((y_train, y_validate, y_test)))\n",
    "\n",
    "# feats_norm_train = feats_train.copy()\n",
    "# feats_norm_validate = feats_validate.copy()\n",
    "# feats_norm_test = feats_test.copy()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ----------------------- STEP 2.1: DENOISE USING DWT -----------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# for i in range(feats_norm_train.shape[1]):\n",
    "#     feats_norm_train[:,i] = waveletSmooth(feats_norm_train[:,i], level=1)[-len(feats_norm_train):]\n",
    "\n",
    "# # for the validation we have to do the transform using training data + the current and past validation data\n",
    "# # i.e. we CAN'T USE all the validation data because we would then look into the future \n",
    "# temp = np.copy(feats_norm_train)\n",
    "# feats_norm_validate_WT = np.copy(feats_norm_validate)\n",
    "# for j in range(feats_norm_validate.shape[0]):\n",
    "#     #first concatenate train with the latest validation sample\n",
    "#     temp = np.append(temp, np.expand_dims(feats_norm_validate[j,:], axis=0), axis=0)\n",
    "#     for i in range(feats_norm_validate.shape[1]):\n",
    "#         feats_norm_validate_WT[j,i] = waveletSmooth(temp[:,i], level=1)[-1]\n",
    "\n",
    "# # for the test we have to do the transform using training data + validation data + current and past test data\n",
    "# # i.e. we CAN'T USE all the test data because we would then look into the future \n",
    "# temp_train = np.copy(feats_norm_train)\n",
    "# temp_val = np.copy(feats_norm_validate)\n",
    "# temp = np.concatenate((temp_train, temp_val))\n",
    "# feats_norm_test_WT = np.copy(feats_norm_test)\n",
    "# for j in range(feats_norm_test.shape[0]):\n",
    "#     #first concatenate train with the latest validation sample\n",
    "#     temp = np.append(temp, np.expand_dims(feats_norm_test[j,:], axis=0), axis=0)\n",
    "#     for i in range(feats_norm_test.shape[1]):\n",
    "#         feats_norm_test_WT[j,i] = waveletSmooth(temp[:,i], level=1)[-1]\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ------------- STEP 3: ENCODE FEATURES USING STACKED AUTOENCODER -----------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# num_hidden_1 = 10\n",
    "# num_hidden_2 = 10\n",
    "# num_hidden_3 = 10\n",
    "# num_hidden_4 = 10\n",
    "\n",
    "# n_epoch=100#20000\n",
    "\n",
    "# # ---- train using training data\n",
    "\n",
    "# # The n==0 statement is done because we only want to initialize the network once and then keep training\n",
    "# # as we move through time \n",
    "\n",
    "# if n == 0:\n",
    "#     auto1 = Autoencoder(feats_norm_train.shape[1], num_hidden_1)\n",
    "# auto1.fit(feats_norm_train, n_epoch=n_epoch)\n",
    "\n",
    "# inputs = torch.autograd.Variable(torch.from_numpy(feats_norm_train.astype(np.float32)))\n",
    "\n",
    "# if n == 0:\n",
    "#     auto2 = Autoencoder(num_hidden_1, num_hidden_2)\n",
    "# auto1_out = auto1.encoder(inputs).data.numpy()\n",
    "# auto2.fit(auto1_out, n_epoch=n_epoch)\n",
    "\n",
    "# if n == 0:\n",
    "#     auto3 = Autoencoder(num_hidden_2, num_hidden_3)\n",
    "# auto1_out = torch.autograd.Variable(torch.from_numpy(auto1_out.astype(np.float32)))\n",
    "# auto2_out = auto2.encoder(auto1_out).data.numpy()\n",
    "# auto3.fit(auto2_out, n_epoch=n_epoch)\n",
    "\n",
    "# if n == 0:\n",
    "#     auto4 = Autoencoder(num_hidden_3, num_hidden_4)\n",
    "# auto2_out = torch.autograd.Variable(torch.from_numpy(auto2_out.astype(np.float32)))\n",
    "# auto3_out = auto3.encoder(auto2_out).data.numpy()\n",
    "# auto4.fit(auto3_out, n_epoch=n_epoch)\n",
    "\n",
    "\n",
    "# # Change to evaluation mode, in this mode the network behaves differently, e.g. dropout is switched off and so on\n",
    "# auto1.eval()        \n",
    "# auto2.eval()\n",
    "# auto3.eval()\n",
    "# auto4.eval()\n",
    "\n",
    "# X_train = feats_norm_train\n",
    "# X_train = torch.autograd.Variable(torch.from_numpy(X_train.astype(np.float32)))\n",
    "# train_encoded = auto4.encoder(auto3.encoder(auto2.encoder(auto1.encoder(X_train))))\n",
    "# train_encoded = train_encoded.data.numpy()\n",
    "\n",
    "# # ---- encode validation and test data using autoencoder trained only on training data \n",
    "# X_validate = feats_norm_validate_WT   \n",
    "# X_validate = torch.autograd.Variable(torch.from_numpy(X_validate.astype(np.float32)))\n",
    "# validate_encoded = auto4.encoder(auto3.encoder(auto2.encoder(auto1.encoder(X_validate))))\n",
    "# validate_encoded = validate_encoded.data.numpy()\n",
    "\n",
    "# X_test = feats_norm_test_WT\n",
    "# X_test = torch.autograd.Variable(torch.from_numpy(X_test.astype(np.float32)))\n",
    "# test_encoded = auto4.encoder(auto3.encoder(auto2.encoder(auto1.encoder(X_test))))\n",
    "# test_encoded = test_encoded.data.numpy()\n",
    "\n",
    "# # switch back to training mode\n",
    "# auto1.train()        \n",
    "# auto2.train()\n",
    "# auto3.train()\n",
    "# auto4.train()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# -------------------- STEP 4: PREPARE TIME-SERIES --------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# split the entire training time-series into pieces, depending on the number\n",
    "# of time steps for the LSTM\n",
    "\n",
    "time_steps = 4\n",
    "\n",
    "args = (train_encoded, validate_encoded, test_encoded)\n",
    "\n",
    "x_concat = np.concatenate(args)\n",
    "\n",
    "validate_encoded_extra = np.concatenate((train_encoded[-time_steps:], validate_encoded))\n",
    "test_encoded_extra = np.concatenate((validate_encoded[-time_steps:], test_encoded))\n",
    "\n",
    "y_train_input = data_close[:-len(validate_encoded)-len(test_encoded)]\n",
    "y_val_input = data_close[-len(test_encoded)-len(validate_encoded)-1:-len(test_encoded)]\n",
    "y_test_input = data_close[-len(test_encoded)-1:]\n",
    "\n",
    "x, y = prepare_data_lstm(train_encoded, y_train_input, time_steps, log_return=True, train=True)\n",
    "x_v, y_v = prepare_data_lstm(validate_encoded_extra, y_val_input, time_steps, log_return=False, train=False)\n",
    "x_te, y_te = prepare_data_lstm(test_encoded_extra, y_test_input, time_steps, log_return=False, train=False)\n",
    "\n",
    "\n",
    "x_test = x_te\n",
    "x_validate = x_v\n",
    "x_train = x \n",
    "\n",
    "y_test = y_te \n",
    "y_validate = y_v \n",
    "y_train = y\n",
    "\n",
    "y_train = y_train.as_matrix()\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ------------- STEP 5: TIME-SERIES REGRESSION USING LSTM -------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "batchsize = 60\n",
    "\n",
    "trainloader = ExampleDataset(x_train, y_train, batchsize)\n",
    "valloader = ExampleDataset(x_validate, y_validate, 1)\n",
    "testloader = ExampleDataset(x_test, y_test, 1)\n",
    "\n",
    "# set ramdom seed to 0\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# build the model\n",
    "if n == 0:\n",
    "    seq = Sequence(num_hidden_4, hidden_size=100, nb_layers=3)\n",
    "\n",
    "resume = \"\"\n",
    "\n",
    "# if a path is given in resume, we resume from a checkpoint\n",
    "if os.path.isfile(resume):\n",
    "    print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "    checkpoint = torch.load(resume)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    seq.load_state_dict(checkpoint['state_dict'])\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "          .format(resume, checkpoint['epoch']))\n",
    "else:\n",
    "    print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "\n",
    "# get the number of model parameters\n",
    "print('Number of model parameters: {}'.format(\n",
    "    sum([p.data.nelement() for p in seq.parameters()])))\n",
    "\n",
    "# we use the mean squared error loss\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(params=seq.parameters(), lr=0.0005)\n",
    "\n",
    "start_epoch = 0 \n",
    "epochs = 1#5000\n",
    "\n",
    "global_loss_val = np.inf\n",
    "#begin to train\n",
    "global_profit_val = -np.inf\n",
    "\n",
    "for i in range(start_epoch, epochs):\n",
    "    seq.train()\n",
    "    loss_train = 0\n",
    "\n",
    "    # shuffle ONLY training set        \n",
    "    combined = list(zip(x_train, y_train))\n",
    "    random.shuffle(combined)\n",
    "    x_train=[]\n",
    "    y_train=[]\n",
    "    x_train[:], y_train[:] = zip(*combined)\n",
    "\n",
    "    # initialize trainloader with newly shuffled training data        \n",
    "    trainloader = ExampleDataset(x_train, y_train, batchsize)\n",
    "\n",
    "    pred_train = []\n",
    "    target_train = []\n",
    "    for j in range(len(trainloader)):\n",
    "        sample = trainloader[j]\n",
    "        sample_x = sample[\"x\"]\n",
    "\n",
    "        if len(sample_x) != 0:\n",
    "\n",
    "            sample_x = np.stack(sample_x)\n",
    "            input = Variable(torch.FloatTensor(sample_x), requires_grad=False)\n",
    "            input = torch.transpose(input, 0, 1)\n",
    "            target = Variable(torch.FloatTensor([x for x in sample[\"y\"]]), requires_grad=False)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = seq(input)\n",
    "            loss = criterion(out, target)\n",
    "\n",
    "            loss_train += float(loss.data.numpy())\n",
    "            pred_train.extend(out.data.numpy().flatten().tolist())\n",
    "            target_train.extend(target.data.numpy().flatten().tolist())\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "    if i % 100 == 0:\n",
    "\n",
    "        plt.plot(pred_train)\n",
    "        plt.plot(target_train)\n",
    "        plt.show()\n",
    "\n",
    "        loss_val, pred_val, target_val = evaluate_lstm(dataloader=valloader, model=seq, criterion=criterion)\n",
    "\n",
    "        plt.scatter(range(len(pred_val)), pred_val)\n",
    "        plt.scatter(range(len(pred_val)), target_val)\n",
    "        plt.show()\n",
    "\n",
    "        index, real = backtest(pred_val, y_validate)\n",
    "\n",
    "        print(index[-1])\n",
    "        # save according to profitability\n",
    "        if index[-1]>global_profit_val and i>200:\n",
    "            print(\"CURRENT BEST\")\n",
    "            global_profit_val = index[-1]\n",
    "            save_checkpoint({'epoch': i + 1, 'state_dict': seq.state_dict()}, is_best=True, filename='checkpoint_lstm.pth.tar')\n",
    "\n",
    "        save_checkpoint({'epoch': i + 1, 'state_dict': seq.state_dict()}, is_best=False, filename='checkpoint_lstm.pth.tar')\n",
    "\n",
    "        print(\"LOSS TRAIN: \" + str(float(loss_train)))        \n",
    "        print(\"LOSS VAL: \" + str(float(loss_val)))\n",
    "        print(i)\n",
    "\n",
    "# do the final test\n",
    "# first load the best checkpoint on the val set\n",
    "\n",
    "resume = \"./runs/checkpoint/model_best.pth.tar\"\n",
    "#resume = \"./runs/HF/checkpoint_lstm.pth.tar\"\n",
    "\n",
    "if os.path.isfile(resume):\n",
    "    print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "    checkpoint = torch.load(resume)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    seq.load_state_dict(checkpoint['state_dict'])\n",
    "    print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "          .format(resume, checkpoint['epoch']))\n",
    "else:\n",
    "    print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "\n",
    "seq.eval()\n",
    "\n",
    "loss_test, preds_test, target_test = evaluate_lstm(dataloader=testloader, model=seq, criterion=criterion)\n",
    "\n",
    "print(\"LOSS TEST: \" + str(float(loss_test)))\n",
    "\n",
    "temp2 = y_test.as_matrix().flatten().tolist()\n",
    "y_test_lst.extend(temp2)\n",
    "\n",
    "plt.plot(preds_test)\n",
    "plt.plot(y_test_lst)\n",
    "plt.scatter(range(len(preds_test)), preds_test)\n",
    "plt.scatter(range(len(y_test_lst)), y_test_lst)\n",
    "plt.savefig(\"test_preds.pdf\")\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# ------------------ STEP 6: BACKTEST (ARTICLE WAY) -------------------------\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "index, real = backtest(preds_test, pd.DataFrame(y_test_lst))\n",
    "\n",
    "plt.close()\n",
    "plt.plot(index, label=\"strat\")\n",
    "plt.plot(real, label=\"bm\")\n",
    "plt.legend()\n",
    "plt.savefig(\"performance_article_way.pdf\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##training loop\n",
    "\n",
    "y_test_lst = []\n",
    "preds = []\n",
    "ct = 0\n",
    "\n",
    "##TODO crea batch prima\n",
    "\n",
    "for n in range(num_iterations):\n",
    "#     print(n)\n",
    "    data = data_master.iloc[n*step_size:num_datapoints+n*step_size,:]\n",
    "    data.columns = [col.strip() for col in data.columns.tolist()]\n",
    "#     print(data.shape)\n",
    "    ct +=1\n",
    "\n",
    "    feats = get_scaled_features(data.iloc[:,2:])\n",
    "\n",
    "    \n",
    "    data_close = feats[\"Close Price\"].copy()\n",
    "    data_close_new = data_close\n",
    "\n",
    "    # Split in train, test and validation set\n",
    "\n",
    "    train, validate, test = get_train_valid_test(feats, step_size)\n",
    "\n",
    "    y_test = data_close_new[-step_size:].to_numpy()\n",
    "    y_validate = data_close_new[-2*step_size:-step_size].to_numpy()\n",
    "    y_train = data_close_new[:-2*step_size].to_numpy()\n",
    "    feats_train = train.to_numpy().astype(np.float)\n",
    "    feats_validate = validate.to_numpy().astype(np.float)\n",
    "    feats_test = test.to_numpy().astype(np.float)\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # ----------------------- STEP 2.0: NORMALIZE DATA --------------------------\n",
    "    # ---------------------------------------------------------------------------\n",
    "\n",
    "    # REMOVED THE NORMALIZATION AND MANUALLY SCALED TO APPROPRIATE VALUES ABOVE\n",
    "\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler().fit(feats_train)\n",
    "\n",
    "    feats_norm_train = scaler.transform(feats_train)\n",
    "    feats_norm_validate = scaler.transform(feats_validate)\n",
    "    feats_norm_test = scaler.transform(feats_test)\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler.fit(feats_train)\n",
    "\n",
    "    feats_norm_train = scaler.transform(feats_train)\n",
    "    feats_norm_validate = scaler.transform(feats_validate)\n",
    "    feats_norm_test = scaler.transform(feats_test)\n",
    "    \"\"\"    \n",
    "    data_close = pd.Series(np.concatenate((y_train, y_validate, y_test)))\n",
    "    \n",
    "    feats_norm_train = feats_train.copy()\n",
    "    feats_norm_validate = feats_validate.copy()\n",
    "    feats_norm_test = feats_test.copy()\n",
    "    \n",
    "    # ---------------------------------------------------------------------------\n",
    "    # ----------------------- STEP 2.1: DENOISE USING DWT -----------------------\n",
    "    # ---------------------------------------------------------------------------\n",
    "\n",
    "    for i in range(feats_norm_train.shape[1]):\n",
    "        feats_norm_train[:,i] = waveletSmooth(feats_norm_train[:,i], level=1)[-len(feats_norm_train):]\n",
    "\n",
    "    # for the validation we have to do the transform using training data + the current and past validation data\n",
    "    # i.e. we CAN'T USE all the validation data because we would then look into the future \n",
    "    temp = np.copy(feats_norm_train)\n",
    "    feats_norm_validate_WT = np.copy(feats_norm_validate)\n",
    "    for j in range(feats_norm_validate.shape[0]):\n",
    "        #first concatenate train with the latest validation sample\n",
    "        temp = np.append(temp, np.expand_dims(feats_norm_validate[j,:], axis=0), axis=0)\n",
    "        for i in range(feats_norm_validate.shape[1]):\n",
    "            feats_norm_validate_WT[j,i] = waveletSmooth(temp[:,i], level=1)[-1]\n",
    "\n",
    "    # for the test we have to do the transform using training data + validation data + current and past test data\n",
    "    # i.e. we CAN'T USE all the test data because we would then look into the future \n",
    "    temp_train = np.copy(feats_norm_train)\n",
    "    temp_val = np.copy(feats_norm_validate)\n",
    "    temp = np.concatenate((temp_train, temp_val))\n",
    "    feats_norm_test_WT = np.copy(feats_norm_test)\n",
    "    for j in range(feats_norm_test.shape[0]):\n",
    "        #first concatenate train with the latest validation sample\n",
    "        temp = np.append(temp, np.expand_dims(feats_norm_test[j,:], axis=0), axis=0)\n",
    "        for i in range(feats_norm_test.shape[1]):\n",
    "            feats_norm_test_WT[j,i] = waveletSmooth(temp[:,i], level=1)[-1]\n",
    "    \n",
    "    # ---------------------------------------------------------------------------\n",
    "    # ------------- STEP 3: ENCODE FEATURES USING STACKED AUTOENCODER -----------\n",
    "    # ---------------------------------------------------------------------------\n",
    "\n",
    "    num_hidden_1 = 10\n",
    "    num_hidden_2 = 10\n",
    "    num_hidden_3 = 10\n",
    "    num_hidden_4 = 10\n",
    "\n",
    "    n_epoch=100#20000\n",
    "\n",
    "    # ---- train using training data\n",
    "    \n",
    "    # The n==0 statement is done because we only want to initialize the network once and then keep training\n",
    "    # as we move through time \n",
    "\n",
    "    if n == 0:\n",
    "        auto1 = Autoencoder(feats_norm_train.shape[1], num_hidden_1)\n",
    "    auto1.fit(feats_norm_train, n_epoch=n_epoch)\n",
    "\n",
    "    inputs = torch.autograd.Variable(torch.from_numpy(feats_norm_train.astype(np.float32)))\n",
    "\n",
    "    if n == 0:\n",
    "        auto2 = Autoencoder(num_hidden_1, num_hidden_2)\n",
    "    auto1_out = auto1.encoder(inputs).data.numpy()\n",
    "    auto2.fit(auto1_out, n_epoch=n_epoch)\n",
    "\n",
    "    if n == 0:\n",
    "        auto3 = Autoencoder(num_hidden_2, num_hidden_3)\n",
    "    auto1_out = torch.autograd.Variable(torch.from_numpy(auto1_out.astype(np.float32)))\n",
    "    auto2_out = auto2.encoder(auto1_out).data.numpy()\n",
    "    auto3.fit(auto2_out, n_epoch=n_epoch)\n",
    "\n",
    "    if n == 0:\n",
    "        auto4 = Autoencoder(num_hidden_3, num_hidden_4)\n",
    "    auto2_out = torch.autograd.Variable(torch.from_numpy(auto2_out.astype(np.float32)))\n",
    "    auto3_out = auto3.encoder(auto2_out).data.numpy()\n",
    "    auto4.fit(auto3_out, n_epoch=n_epoch)\n",
    "    \n",
    "\n",
    "    # Change to evaluation mode, in this mode the network behaves differently, e.g. dropout is switched off and so on\n",
    "    auto1.eval()        \n",
    "    auto2.eval()\n",
    "    auto3.eval()\n",
    "    auto4.eval()\n",
    "    \n",
    "    X_train = feats_norm_train\n",
    "    X_train = torch.autograd.Variable(torch.from_numpy(X_train.astype(np.float32)))\n",
    "    train_encoded = auto4.encoder(auto3.encoder(auto2.encoder(auto1.encoder(X_train))))\n",
    "    train_encoded = train_encoded.data.numpy()\n",
    "\n",
    "    # ---- encode validation and test data using autoencoder trained only on training data \n",
    "    X_validate = feats_norm_validate_WT   \n",
    "    X_validate = torch.autograd.Variable(torch.from_numpy(X_validate.astype(np.float32)))\n",
    "    validate_encoded = auto4.encoder(auto3.encoder(auto2.encoder(auto1.encoder(X_validate))))\n",
    "    validate_encoded = validate_encoded.data.numpy()\n",
    "\n",
    "    X_test = feats_norm_test_WT\n",
    "    X_test = torch.autograd.Variable(torch.from_numpy(X_test.astype(np.float32)))\n",
    "    test_encoded = auto4.encoder(auto3.encoder(auto2.encoder(auto1.encoder(X_test))))\n",
    "    test_encoded = test_encoded.data.numpy()\n",
    "    \n",
    "    # switch back to training mode\n",
    "    auto1.train()        \n",
    "    auto2.train()\n",
    "    auto3.train()\n",
    "    auto4.train()\n",
    "\n",
    "    \n",
    "    # ---------------------------------------------------------------------------\n",
    "    # -------------------- STEP 4: PREPARE TIME-SERIES --------------------------\n",
    "    # ---------------------------------------------------------------------------\n",
    "\n",
    "    # split the entire training time-series into pieces, depending on the number\n",
    "    # of time steps for the LSTM\n",
    "\n",
    "    time_steps = 4\n",
    "\n",
    "    args = (train_encoded, validate_encoded, test_encoded)\n",
    "\n",
    "    x_concat = np.concatenate(args)\n",
    "\n",
    "    validate_encoded_extra = np.concatenate((train_encoded[-time_steps:], validate_encoded))\n",
    "    test_encoded_extra = np.concatenate((validate_encoded[-time_steps:], test_encoded))\n",
    "\n",
    "    y_train_input = data_close[:-len(validate_encoded)-len(test_encoded)]\n",
    "    y_val_input = data_close[-len(test_encoded)-len(validate_encoded)-1:-len(test_encoded)]\n",
    "    y_test_input = data_close[-len(test_encoded)-1:]\n",
    "\n",
    "    x, y = prepare_data_lstm(train_encoded, y_train_input, time_steps, log_return=True, train=True)\n",
    "    x_v, y_v = prepare_data_lstm(validate_encoded_extra, y_val_input, time_steps, log_return=False, train=False)\n",
    "    x_te, y_te = prepare_data_lstm(test_encoded_extra, y_test_input, time_steps, log_return=False, train=False)\n",
    "\n",
    "\n",
    "    x_test = x_te\n",
    "    x_validate = x_v\n",
    "    x_train = x \n",
    "\n",
    "    y_test = y_te \n",
    "    y_validate = y_v \n",
    "    y_train = y\n",
    "\n",
    "    y_train = y_train.as_matrix()\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # ------------- STEP 5: TIME-SERIES REGRESSION USING LSTM -------------------\n",
    "    # ---------------------------------------------------------------------------\n",
    "\n",
    "    batchsize = 60\n",
    "\n",
    "    trainloader = ExampleDataset(x_train, y_train, batchsize)\n",
    "    valloader = ExampleDataset(x_validate, y_validate, 1)\n",
    "    testloader = ExampleDataset(x_test, y_test, 1)\n",
    "\n",
    "    # set ramdom seed to 0\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    # build the model\n",
    "    if n == 0:\n",
    "        seq = Sequence(num_hidden_4, hidden_size=100, nb_layers=3)\n",
    "\n",
    "    resume = \"\"\n",
    "\n",
    "    # if a path is given in resume, we resume from a checkpoint\n",
    "    if os.path.isfile(resume):\n",
    "        print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "        checkpoint = torch.load(resume)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        seq.load_state_dict(checkpoint['state_dict'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(resume, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "\n",
    "    # get the number of model parameters\n",
    "    print('Number of model parameters: {}'.format(\n",
    "        sum([p.data.nelement() for p in seq.parameters()])))\n",
    "\n",
    "    # we use the mean squared error loss\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(params=seq.parameters(), lr=0.0005)\n",
    "\n",
    "    start_epoch = 0 \n",
    "    epochs = 1#5000\n",
    "\n",
    "    global_loss_val = np.inf\n",
    "    #begin to train\n",
    "    global_profit_val = -np.inf\n",
    "\n",
    "    for i in range(start_epoch, epochs):\n",
    "        seq.train()\n",
    "        loss_train = 0\n",
    "\n",
    "        # shuffle ONLY training set        \n",
    "        combined = list(zip(x_train, y_train))\n",
    "        random.shuffle(combined)\n",
    "        x_train=[]\n",
    "        y_train=[]\n",
    "        x_train[:], y_train[:] = zip(*combined)\n",
    "        \n",
    "        # initialize trainloader with newly shuffled training data        \n",
    "        trainloader = ExampleDataset(x_train, y_train, batchsize)\n",
    "\n",
    "        pred_train = []\n",
    "        target_train = []\n",
    "        for j in range(len(trainloader)):\n",
    "            sample = trainloader[j]\n",
    "            sample_x = sample[\"x\"]\n",
    "\n",
    "            if len(sample_x) != 0:\n",
    "\n",
    "                sample_x = np.stack(sample_x)\n",
    "                input = Variable(torch.FloatTensor(sample_x), requires_grad=False)\n",
    "                input = torch.transpose(input, 0, 1)\n",
    "                target = Variable(torch.FloatTensor([x for x in sample[\"y\"]]), requires_grad=False)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                out = seq(input)\n",
    "                loss = criterion(out, target)\n",
    "\n",
    "                loss_train += float(loss.data.numpy())\n",
    "                pred_train.extend(out.data.numpy().flatten().tolist())\n",
    "                target_train.extend(target.data.numpy().flatten().tolist())\n",
    "\n",
    "                loss.backward()\n",
    "\n",
    "                optimizer.step()\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "\n",
    "            plt.plot(pred_train)\n",
    "            plt.plot(target_train)\n",
    "            plt.show()\n",
    "            \n",
    "            loss_val, pred_val, target_val = evaluate_lstm(dataloader=valloader, model=seq, criterion=criterion)\n",
    "            \n",
    "            plt.scatter(range(len(pred_val)), pred_val)\n",
    "            plt.scatter(range(len(pred_val)), target_val)\n",
    "            plt.show()\n",
    "\n",
    "            index, real = backtest(pred_val, y_validate)\n",
    "\n",
    "            print(index[-1])\n",
    "            # save according to profitability\n",
    "            if index[-1]>global_profit_val and i>200:\n",
    "                print(\"CURRENT BEST\")\n",
    "                global_profit_val = index[-1]\n",
    "                save_checkpoint({'epoch': i + 1, 'state_dict': seq.state_dict()}, is_best=True, filename='checkpoint_lstm.pth.tar')\n",
    "\n",
    "            save_checkpoint({'epoch': i + 1, 'state_dict': seq.state_dict()}, is_best=False, filename='checkpoint_lstm.pth.tar')\n",
    "\n",
    "            print(\"LOSS TRAIN: \" + str(float(loss_train)))        \n",
    "            print(\"LOSS VAL: \" + str(float(loss_val)))\n",
    "            print(i)\n",
    "\n",
    "    # do the final test\n",
    "    # first load the best checkpoint on the val set\n",
    "\n",
    "    resume = \"./runs/checkpoint/model_best.pth.tar\"\n",
    "    #resume = \"./runs/HF/checkpoint_lstm.pth.tar\"\n",
    "\n",
    "    if os.path.isfile(resume):\n",
    "        print(\"=> loading checkpoint '{}'\".format(resume))\n",
    "        checkpoint = torch.load(resume)\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        seq.load_state_dict(checkpoint['state_dict'])\n",
    "        print(\"=> loaded checkpoint '{}' (epoch {})\"\n",
    "              .format(resume, checkpoint['epoch']))\n",
    "    else:\n",
    "        print(\"=> no checkpoint found at '{}'\".format(resume))\n",
    "\n",
    "    seq.eval()\n",
    "\n",
    "    loss_test, preds_test, target_test = evaluate_lstm(dataloader=testloader, model=seq, criterion=criterion)\n",
    "\n",
    "    print(\"LOSS TEST: \" + str(float(loss_test)))\n",
    "\n",
    "    temp2 = y_test.as_matrix().flatten().tolist()\n",
    "    y_test_lst.extend(temp2)\n",
    "        \n",
    "    plt.plot(preds_test)\n",
    "    plt.plot(y_test_lst)\n",
    "    plt.scatter(range(len(preds_test)), preds_test)\n",
    "    plt.scatter(range(len(y_test_lst)), y_test_lst)\n",
    "    plt.savefig(\"test_preds.pdf\")\n",
    "\n",
    "    # ---------------------------------------------------------------------------\n",
    "    # ------------------ STEP 6: BACKTEST (ARTICLE WAY) -------------------------\n",
    "    # ---------------------------------------------------------------------------\n",
    "\n",
    "    index, real = backtest(preds_test, pd.DataFrame(y_test_lst))\n",
    "\n",
    "    plt.close()\n",
    "    plt.plot(index, label=\"strat\")\n",
    "    plt.plot(real, label=\"bm\")\n",
    "    plt.legend()\n",
    "    plt.savefig(\"performance_article_way.pdf\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fastai)",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
